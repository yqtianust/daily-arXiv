{"id": "2506.03283", "pdf": "https://arxiv.org/pdf/2506.03283", "abs": "https://arxiv.org/abs/2506.03283", "authors": ["Viola Campos", "Ridwan Shariffdeen", "Adrian Ulges", "Yannic Noller"], "title": "Empirical Evaluation of Generalizable Automated Program Repair with Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Automated Program Repair (APR) proposes bug fixes to aid developers in\nmaintaining software. The state of the art in this domain focuses on using\nLLMs, leveraging their strong capabilities to comprehend specifications in\nnatural language and to generate program code. Recent works have shown that\nLLMs can be used to generate repairs. However, despite the APR community's\nresearch achievements and several industry deployments in the last decade, APR\nstill lacks the capabilities to generalize broadly. In this work, we present an\nintensive empirical evaluation of LLMs for generating patches. We evaluate a\ndiverse set of 13 recent models, including open ones (e.g., Llama 3.3, Qwen 2.5\nCoder, and DeepSeek R1 (dist.)) and closed ones (e.g., o3-mini, GPT-4o, Claude\n3.7 Sonnet, Gemini 2.0 Flash). In particular, we explore language-agnostic\nrepairs by utilizing benchmarks for Java (e.g., Defects4J), JavaScript (e.g.,\nBugsJS), Python (e.g., BugsInPy), and PHP (e.g., BugsPHP). Besides the\ngeneralization between different languages and levels of patch complexity, we\nalso investigate the effects of fault localization (FL) as a preprocessing step\nand compare the progress for open vs closed models. Our evaluation represents a\nsnapshot of the current repair capabilities of the latest LLMs. Key results\ninclude: (1) Different LLMs tend to perform best for different languages, which\nmakes it hard to develop cross-platform repair techniques with single LLMs. (2)\nThe combinations of models add value with respect to uniquely fixed bugs, so a\ncommittee of expert models should be considered. (3) Under realistic\nassumptions of imperfect FL, we observe significant drops in accuracy from the\nusual practice of using perfect FL. Our findings and insights will help both\nresearchers and practitioners develop reliable and generalizable APR techniques\nand evaluate them in realistic and fair environments.", "AI": {"tldr": "The paper evaluates 13 LLMs for automated program repair (APR) across multiple languages, highlighting challenges in generalization, the benefits of model combinations, and the impact of imperfect fault localization.", "motivation": "Despite advancements in APR using LLMs, broad generalization remains a challenge. This work aims to empirically assess the repair capabilities of diverse LLMs to guide future research and practical applications.", "method": "An intensive empirical evaluation of 13 LLMs (open and closed) was conducted using benchmarks for Java, JavaScript, Python, and PHP. The study explored language-agnostic repairs, patch complexity, and the role of fault localization.", "result": "Key findings: (1) LLMs perform variably across languages, complicating cross-platform repairs. (2) Combining models improves bug fixes. (3) Imperfect fault localization significantly reduces accuracy.", "conclusion": "The study provides insights for developing reliable APR techniques and emphasizes realistic evaluation environments, benefiting both researchers and practitioners."}}
{"id": "2506.03396", "pdf": "https://arxiv.org/pdf/2506.03396", "abs": "https://arxiv.org/abs/2506.03396", "authors": ["Jinhan Kim", "Nargiz Humbatova", "Gunel Jahangirova", "Shin Yoo", "Paolo Tonella"], "title": "Fault Localisation and Repair for DL Systems: An Empirical Study with LLMs", "categories": ["cs.SE"], "comment": "arXiv admin note: text overlap with arXiv:2301.11568", "summary": "Numerous Fault Localisation (FL) and repair techniques have been proposed to\naddress faults in Deep Learning (DL) models. However, their effectiveness in\npractical applications remains uncertain due to the reliance on pre-defined\nrules. This paper presents a comprehensive evaluation of state-of-the-art FL\nand repair techniques, examining their advantages and limitations. Moreover, we\nintroduce a novel approach that harnesses the power of Large Language Models\n(LLMs) in localising and repairing DL faults. Our evaluation, conducted on a\ncarefully designed benchmark, reveals the strengths and weaknesses of current\nFL and repair techniques. We emphasise the importance of enhanced accuracy and\nthe need for more rigorous assessment methods that employ multiple ground truth\npatches. Notably, LLMs exhibit remarkable performance in both FL and repair\ntasks. For instance, the GPT-4 model achieves 44% and 82% improvements in FL\nand repair tasks respectively, compared to the second-best tool, demonstrating\nthe potential of LLMs in this domain. Our study sheds light on the current\nstate of FL and repair techniques and suggests that LLMs could be a promising\navenue for future advancements.", "AI": {"tldr": "The paper evaluates Fault Localisation (FL) and repair techniques for Deep Learning (DL) models, highlighting their limitations and proposing a novel LLM-based approach. GPT-4 shows significant improvements over existing tools.", "motivation": "To assess the effectiveness of current FL and repair techniques for DL models and explore the potential of LLMs in improving accuracy and performance.", "method": "A comprehensive evaluation of state-of-the-art FL and repair techniques, alongside a novel LLM-based approach, tested on a designed benchmark.", "result": "LLMs, particularly GPT-4, outperform existing tools with 44% and 82% improvements in FL and repair tasks, respectively.", "conclusion": "LLMs like GPT-4 show promise for advancing FL and repair in DL models, but more rigorous assessment methods are needed."}}
{"id": "2506.03401", "pdf": "https://arxiv.org/pdf/2506.03401", "abs": "https://arxiv.org/abs/2506.03401", "authors": ["Xiwei Xu", "Hans Weytjens", "Dawen Zhang", "Qinghua Lu", "Ingo Weber", "Liming Zhu"], "title": "RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines", "categories": ["cs.SE"], "comment": null, "summary": "Recent studies show that 60% of LLM-based compound systems in enterprise\nenvironments leverage some form of retrieval-augmented generation (RAG), which\nenhances the relevance and accuracy of LLM (or other genAI) outputs by\nretrieving relevant information from external data sources. LLMOps involves the\npractices and techniques for managing the lifecycle and operations of LLM\ncompound systems in production environments. It supports enhancing LLM systems\nthrough continuous operations and feedback evaluation. RAGOps extends LLMOps by\nincorporating a strong focus on data management to address the continuous\nchanges in external data sources. This necessitates automated methods for\nevaluating and testing data operations, enhancing retrieval relevance and\ngeneration quality. In this paper, we (1) characterize the generic architecture\nof RAG applications based on the 4+1 model view for describing software\narchitectures, (2) outline the lifecycle of RAG systems, which integrates the\nmanagement lifecycles of both the LLM and the data, (3) define the key design\nconsiderations of RAGOps across different stages of the RAG lifecycle and\nquality trade-off analyses, (4) highlight the overarching research challenges\naround RAGOps, and (5) present two use cases of RAG applications and the\ncorresponding RAGOps considerations.", "AI": {"tldr": "The paper discusses RAGOps, an extension of LLMOps, focusing on data management for RAG systems. It outlines architecture, lifecycle, design considerations, challenges, and use cases.", "motivation": "To enhance LLM systems by addressing continuous changes in external data sources through RAGOps, improving retrieval relevance and generation quality.", "method": "Characterizes RAG architecture, outlines lifecycle, defines design considerations, highlights challenges, and presents use cases.", "result": "A framework for RAGOps, integrating LLM and data management lifecycles, with practical insights from use cases.", "conclusion": "RAGOps is crucial for managing RAG systems, with ongoing research challenges and practical implications."}}
{"id": "2506.03504", "pdf": "https://arxiv.org/pdf/2506.03504", "abs": "https://arxiv.org/abs/2506.03504", "authors": ["Zhuo Zhuo", "Xiangyu Zhang"], "title": "Beyond C/C++: Probabilistic and LLM Methods for Next-Generation Software Reverse Engineering", "categories": ["cs.SE"], "comment": null, "summary": "This proposal discusses the growing challenges in reverse engineering modern\nsoftware binaries, particularly those compiled from newer system programming\nlanguages such as Rust, Go, and Mojo. Traditional reverse engineering\ntechniques, developed with a focus on C and C++, fall short when applied to\nthese newer languages due to their reliance on outdated heuristics and failure\nto fully utilize the rich semantic information embedded in binary programs.\nThese challenges are exacerbated by the limitations of current data-driven\nmethods, which are susceptible to generating inaccurate results, commonly\nreferred to as hallucinations. To overcome these limitations, we propose a\nnovel approach that integrates probabilistic binary analysis with fine-tuned\nlarge language models (LLMs). Our method systematically models the\nuncertainties inherent in reverse engineering, enabling more accurate reasoning\nabout incomplete or ambiguous information. By incorporating LLMs, we extend the\nanalysis beyond traditional heuristics, allowing for more creative and\ncontext-aware inferences, particularly for binaries from diverse programming\nlanguages. This hybrid approach not only enhances the robustness and accuracy\nof reverse engineering efforts but also offers a scalable solution adaptable to\nthe rapidly evolving landscape of software development.", "AI": {"tldr": "A novel hybrid approach combining probabilistic binary analysis and fine-tuned LLMs addresses challenges in reverse engineering modern software binaries from languages like Rust, Go, and Mojo, improving accuracy and scalability.", "motivation": "Traditional reverse engineering techniques fail for newer languages due to outdated heuristics and lack of semantic utilization, while current data-driven methods suffer from inaccuracies (hallucinations).", "method": "Integrates probabilistic binary analysis with fine-tuned LLMs to model uncertainties and enable context-aware inferences.", "result": "Enhances robustness, accuracy, and scalability of reverse engineering for diverse programming languages.", "conclusion": "The hybrid approach offers a promising solution for modern software binaries, adapting to evolving development trends."}}
{"id": "2506.03382", "pdf": "https://arxiv.org/pdf/2506.03382", "abs": "https://arxiv.org/abs/2506.03382", "authors": ["Matteo Palazzo", "Luca Roversi"], "title": "Towards a Characterization of Two-way Bijections in a Reversible Computational Model", "categories": ["cs.LO", "cs.CC", "cs.PL", "F.3.2"], "comment": "8 pages, 3 figures, 5 listings. Author's copy of the version which\n  will appear in the Proceedings of the 17th International Conference, RC 2025,\n  Odense, Denmark, July 3-4, 2025", "summary": "We introduce an imperative, stack-based, and reversible computational model\nthat characterizes Two-way Bijections both implicitly, concerning their\ncomputational complexity, and with zero-garbage.", "AI": {"tldr": "A reversible, stack-based computational model for Two-way Bijections with zero-garbage and implicit computational complexity.", "motivation": "To characterize Two-way Bijections in a reversible and efficient manner, addressing computational complexity and garbage-free execution.", "method": "Introduces an imperative, stack-based, and reversible computational model.", "result": "The model successfully characterizes Two-way Bijections with zero-garbage and implicit computational complexity.", "conclusion": "The proposed model provides an efficient and reversible framework for Two-way Bijections, ensuring computational clarity and zero-garbage execution."}}
{"id": "2506.03507", "pdf": "https://arxiv.org/pdf/2506.03507", "abs": "https://arxiv.org/abs/2506.03507", "authors": ["Eric O'Donoghue", "Yvette Hastings", "Ernesto Ortiz", "A. Redempta Manzi Muneza"], "title": "Software Bill of Materials in Software Supply Chain Security A Systematic Literature Review", "categories": ["cs.SE", "cs.CR"], "comment": "16 pages, 4 figures, 5 tables", "summary": "Software Bill of Materials (SBOMs) are increasingly regarded as essential\ntools for securing software supply chains (SSCs), yet their real-world use and\nadoption barriers remain poorly understood. This systematic literature review\nsynthesizes evidence from 40 peer-reviewed studies to evaluate how SBOMs are\ncurrently used to bolster SSC security. We identify five primary application\nareas: vulnerability management, transparency, component assessment, risk\nassessment, and SSC integrity. Despite clear promise, adoption is hindered by\nsignificant barriers: generation tooling, data privacy, format/standardization,\nsharing/distribution, cost/overhead, vulnerability exploitability, maintenance,\nanalysis tooling, false positives, hidden packages, and tampering. To structure\nour analysis, we map these barriers to the ISO/IEC 25019:2023 Quality-in-Use\nmodel, revealing critical deficiencies in SBOM trustworthiness, usability, and\nsuitability for security tasks. We also highlight key gaps in the literature.\nThese include the absence of applying machine learning techniques to assess\nSBOMs and limited evaluation of SBOMs and SSCs using software quality assurance\ntechniques. Our findings provide actionable insights for researchers, tool\ndevelopers, and practitioners seeking to advance SBOM-driven SSC security and\nlay a foundation for future work at the intersection of SSC assurance,\nautomation, and empirical software engineering.", "AI": {"tldr": "This paper reviews 40 studies on SBOMs, identifying five key uses and 11 adoption barriers, mapped to ISO/IEC 25019:2023. It highlights gaps like lack of ML techniques and SQA evaluations, offering insights for future research.", "motivation": "To understand real-world SBOM adoption and barriers in securing software supply chains, as current knowledge is limited.", "method": "A systematic literature review of 40 peer-reviewed studies, analyzing SBOM applications and barriers, mapped to ISO/IEC 25019:2023.", "result": "Identified five SBOM uses (vulnerability management, transparency, etc.) and 11 barriers (tooling, privacy, etc.), revealing gaps in trustworthiness and usability.", "conclusion": "SBOMs show promise but face adoption challenges. Future work should explore ML and SQA techniques to enhance SBOM-driven security."}}
{"id": "2506.04019", "pdf": "https://arxiv.org/pdf/2506.04019", "abs": "https://arxiv.org/abs/2506.04019", "authors": ["Neeva Oza", "Ishaan Govil", "Parul Gupta", "Dinesh Khandelwal", "Dinesh Garg", "Parag Singla"], "title": "CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL", "68-02 (Primary) 68T50, 68T07, 68N19, 68N30 (Secondary)", "I.2.7; I.2.6; I.2.5; D.3.0; D.3.3; D.3.1; F.3.2; F.3.1; F.3.3;\n  D.2.3; D.2.5"], "comment": null, "summary": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code.", "AI": {"tldr": "The paper explores LLMs for code-equivalence checking, introduces CETBench for benchmarking, and finds simple transformations significantly reduce LLM performance. A fine-tuning approach is proposed to improve results.", "motivation": "To evaluate LLMs' capabilities in code-equivalence checking, which is crucial for tasks like code re-writing and translation.", "method": "Introduces CETBench, a dataset of program pairs with random transformations, and tests SOTA LLMs. Proposes fine-tuning to enhance performance.", "result": "Simple code transformations cause significant performance drops in LLMs. Fine-tuning improves accuracy on transformed pairs.", "conclusion": "LLMs lack deep semantic understanding of code, and fine-tuning can mitigate performance issues for code-equivalence checking."}}
{"id": "2506.03535", "pdf": "https://arxiv.org/pdf/2506.03535", "abs": "https://arxiv.org/abs/2506.03535", "authors": ["Qiming Zhu", "Jialun Cao", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Shing-Chi Cheung"], "title": "Across Programming Language Silos: A Study on Cross-Lingual Retrieval-augmented Code Generation", "categories": ["cs.SE"], "comment": null, "summary": "Current research on large language models (LLMs) with retrieval-augmented\ncode generation (RACG) mainly focuses on single-language settings, leaving\ncross-lingual effectiveness and security unexplored. Multi-lingual RACG systems\nare valuable for migrating code-bases across programming languages (PLs), yet\nface risks from error (e.g. adversarial data corruption) propagation in\ncross-lingual transfer. We construct a dataset spanning 13 PLs with nearly 14k\ninstances to explore utility and robustness of multi-lingual RACG systems. Our\ninvestigation reveals four key insights: (1) Effectiveness: multi-lingual RACG\nsignificantly enhances multi-lingual code LLMs generation; (2) Inequality: Java\ndemonstrate superior cross-lingual utility over Python in RACG; (3) Robustness:\nAdversarial attacks degrade performance significantly in mono-lingual RACG but\nshow mitigated impacts in cross-lingual scenarios; Counterintuitively,\nperturbed code may improve RACG in cross-lingual scenarios; (4) Specialization:\nDomain-specific code retrievers outperform significantly general text\nretrievers. These findings establish foundation for developing effective and\nsecure multi-lingual code assistants.", "AI": {"tldr": "The paper explores multi-lingual retrieval-augmented code generation (RACG) in LLMs, highlighting its effectiveness, robustness, and specialization across 13 programming languages.", "motivation": "Current research lacks focus on cross-lingual effectiveness and security in RACG, despite its value for code migration across languages.", "method": "A dataset of 13 PLs with 14k instances was constructed to analyze utility and robustness of multi-lingual RACG systems.", "result": "Key insights include enhanced multi-lingual code generation, Java's superiority over Python, mitigated adversarial impacts in cross-lingual scenarios, and domain-specific retrievers outperforming general ones.", "conclusion": "The findings provide a foundation for developing secure and effective multi-lingual code assistants."}}
{"id": "2506.03585", "pdf": "https://arxiv.org/pdf/2506.03585", "abs": "https://arxiv.org/abs/2506.03585", "authors": ["Inseok Yeo", "Duksan Ryu", "Jongmoon Baik"], "title": "Improving LLM-Based Fault Localization with External Memory and Project Context", "categories": ["cs.SE"], "comment": "12 Pages, 7 figures", "summary": "Fault localization, the process of identifying the software components\nresponsible for failures, is essential but often time-consuming. Recent\nadvances in Large Language Models (LLMs) have enabled fault localization\nwithout extensive defect datasets or model fine-tuning. However, existing\nLLM-based methods rely only on general LLM capabilities and lack integration of\nproject-specific knowledge, resulting in limited effectiveness, especially for\ncomplex software.\n  We introduce MemFL, a novel approach that enhances LLM-based fault\nlocalization by integrating project-specific knowledge via external memory.\nThis memory includes static summaries of the project and dynamic, iterative\ndebugging insights gathered from previous attempts. By leveraging external\nmemory, MemFL simplifies debugging into three streamlined steps, significantly\nimproving efficiency and accuracy. Iterative refinement through dynamic memory\nfurther enhances reasoning quality over time.\n  Evaluated on the Defects4J benchmark, MemFL using GPT-4o-mini localized 12.7%\nmore bugs than current LLM-based methods, achieving this improvement with just\n21% of the execution time (17.4 seconds per bug) and 33% of the API cost\n(0.0033 dollars per bug). On complex projects, MemFL's advantage increased to\n27.6%. Additionally, MemFL with GPT-4.1-mini outperformed existing methods by\n24.4%, requiring only 24.7 seconds and 0.0094 dollars per bug. MemFL thus\ndemonstrates significant improvements by effectively incorporating\nproject-specific knowledge into LLM-based fault localization, delivering high\naccuracy with reduced time and cost.", "AI": {"tldr": "MemFL enhances LLM-based fault localization by integrating project-specific knowledge via external memory, improving efficiency and accuracy.", "motivation": "Existing LLM-based fault localization lacks project-specific knowledge, limiting effectiveness, especially for complex software.", "method": "MemFL uses external memory (static project summaries and dynamic debugging insights) to streamline debugging into three steps.", "result": "MemFL localized 12.7% more bugs than current methods, with reduced execution time (17.4s/bug) and cost ($0.0033/bug). On complex projects, improvement rose to 27.6%.", "conclusion": "MemFL significantly improves fault localization by incorporating project-specific knowledge, achieving high accuracy with lower time and cost."}}
{"id": "2506.03691", "pdf": "https://arxiv.org/pdf/2506.03691", "abs": "https://arxiv.org/abs/2506.03691", "authors": ["Weiyuan Xu", "Juntao Luo", "Tao Huang", "Kaixin Sui", "Jie Geng", "Qijun Ma", "Isami Akasaka", "Xiaoxue Shi", "Jing Tang", "Peng Cai"], "title": "A Two-Staged LLM-Based Framework for CI/CD Failure Detection and Remediation with Industrial Validation", "categories": ["cs.SE"], "comment": "12 pages, 5 figures", "summary": "Continuous Integration and Continuous Deployment (CI/CD) pipelines are\npivotal to modern software engineering, yet diagnosing and resolving their\nfailures remains a complex and labor-intensive challenge. In this paper, we\npresent LogSage, the first end-to-end LLM-powered framework that performs root\ncause analysis and solution generation from failed CI/CD pipeline logs. During\nthe root cause analysis stage, LogSage employs a specialized log preprocessing\npipeline tailored for LLMs, which extracts critical error logs and eliminates\nnoise to enhance the precision of LLM-driven root cause analysis. In the\nsolution generation stage, LogSage leverages RAG to integrate historical\nresolution strategies and utilizes tool-calling to deliver actionable,\nautomated fixes. We evaluated the root cause analysis stage using a newly\ncurated open-source dataset, achieving 98\\% in precision and 12\\% improvement\nover naively designed LLM-based log analysis baselines, while attaining\nnear-perfect recall. The end-to-end system was rigorously validated in a\nlarge-scale industrial CI/CD environment of production quality, processing more\nthan 3,000 executions daily and accumulating more than 1.07 million executions\nin its first year of deployment, with end-to-end precision exceeding 88\\%.\nThese two forms of evaluation confirm that LogSage providing a scalable and\npractical solution to manage CI/CD pipeline failures in real-world DevOps\nworkflows.", "AI": {"tldr": "LogSage is an LLM-powered framework for diagnosing and fixing CI/CD pipeline failures, achieving high precision and recall in root cause analysis and solution generation.", "motivation": "CI/CD pipeline failures are complex and labor-intensive to diagnose and resolve, necessitating an automated solution.", "method": "LogSage uses a specialized log preprocessing pipeline for LLMs and RAG for solution generation, integrating historical data and tool-calling.", "result": "Achieved 98% precision in root cause analysis and 88% end-to-end precision in real-world deployment, processing over 3,000 executions daily.", "conclusion": "LogSage is a scalable and practical solution for managing CI/CD pipeline failures in DevOps workflows."}}
{"id": "2506.03801", "pdf": "https://arxiv.org/pdf/2506.03801", "abs": "https://arxiv.org/abs/2506.03801", "authors": ["Peter Pfeiffer", "Alexander Rombach", "Maxim Majlatow", "Nijat Mehdiyev"], "title": "From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation", "categories": ["cs.SE", "cs.LG", "cs.MA"], "comment": "Accepted to the Next Gen Data and Process Management: Large Language\n  Models and Beyond workshop at SIGMOD 2025", "summary": "Traditional Business Process Management (BPM) struggles with rigidity,\nopacity, and scalability in dynamic environments while emerging Large Language\nModels (LLMs) present transformative opportunities alongside risks. This paper\nexplores four real-world use cases that demonstrate how LLMs, augmented with\ntrustworthy process intelligence, redefine process modeling, prediction, and\nautomation. Grounded in early-stage research projects with industrial partners,\nthe work spans manufacturing, modeling, life-science, and design processes,\naddressing domain-specific challenges through human-AI collaboration. In\nmanufacturing, an LLM-driven framework integrates uncertainty-aware explainable\nMachine Learning (ML) with interactive dialogues, transforming opaque\npredictions into auditable workflows. For process modeling, conversational\ninterfaces democratize BPMN design. Pharmacovigilance agents automate drug\nsafety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable\ntextile design employs multi-agent systems to navigate regulatory and\nenvironmental trade-offs. We intend to examine tensions between transparency\nand efficiency, generalization and specialization, and human agency versus\nautomation. By mapping these trade-offs, we advocate for context-sensitive\nintegration prioritizing domain needs, stakeholder values, and iterative\nhuman-in-the-loop workflows over universal solutions. This work provides\nactionable insights for researchers and practitioners aiming to operationalize\nLLMs in critical BPM environments.", "AI": {"tldr": "LLMs enhance BPM by addressing rigidity and opacity through human-AI collaboration in manufacturing, modeling, life-science, and design, balancing transparency, efficiency, and human agency.", "motivation": "Traditional BPM lacks adaptability in dynamic environments; LLMs offer transformative potential but require trustworthy integration.", "method": "Four real-world use cases demonstrate LLM-augmented BPM, combining uncertainty-aware ML, conversational interfaces, knowledge graphs, and multi-agent systems.", "result": "LLMs improve process modeling, prediction, and automation while addressing domain-specific challenges through human-AI collaboration.", "conclusion": "Context-sensitive LLM integration, prioritizing domain needs and stakeholder values, is advocated over universal solutions for critical BPM."}}
{"id": "2506.03840", "pdf": "https://arxiv.org/pdf/2506.03840", "abs": "https://arxiv.org/abs/2506.03840", "authors": ["Pragya Verma", "Marcos Vinicius Cruz", "Grischa Liebel"], "title": "Differences between Neurodivergent and Neurotypical Software Engineers: Analyzing the 2022 Stack Overflow Survey", "categories": ["cs.SE"], "comment": null, "summary": "Neurodiversity describes variation in brain function among people, including\ncommon conditions such as Autism spectrum disorder (ASD), Attention deficit\nhyperactivity disorder (ADHD), and dyslexia. While Software Engineering (SE)\nliterature has started to explore the experiences of neurodivergent software\nengineers, there is a lack of research that compares their challenges to those\nof neurotypical software engineers. To address this gap, we analyze existing\ndata from the 2022 Stack Overflow Developer survey that collected data on\nneurodiversity. We quantitatively compare the answers of professional engineers\nwith ASD (n=374), ADHD (n=1305), and dyslexia (n=363) with neurotypical\nengineers. Our findings indicate that neurodivergent engineers face more\ndifficulties than neurotypical engineers. Specifically, engineers with ADHD\nreport that they face more interruptions caused by waiting for answers, and\nthat they less frequently interact with individuals outside their team. This\nstudy provides a baseline for future research comparing neurodivergent\nengineers with neurotypical ones. Several factors in the Stack Overflow survey\nand in our analysis are likely to lead to conservative estimates of the actual\neffects between neurodivergent and neurotypical engineers, e.g., the effects of\nthe COVID-19 pandemic and our focus on employed professionals.", "AI": {"tldr": "The paper compares challenges faced by neurodivergent (ASD, ADHD, dyslexia) and neurotypical software engineers using 2022 Stack Overflow data, finding neurodivergent engineers face more difficulties.", "motivation": "To address the lack of research comparing challenges of neurodivergent and neurotypical software engineers.", "method": "Quantitative analysis of 2022 Stack Overflow Developer survey data, comparing responses of neurodivergent (ASD, ADHD, dyslexia) and neurotypical engineers.", "result": "Neurodivergent engineers face more difficulties; ADHD engineers report more interruptions and less interaction outside their team.", "conclusion": "The study establishes a baseline for future research, noting conservative estimates due to survey limitations and pandemic effects."}}
{"id": "2506.03877", "pdf": "https://arxiv.org/pdf/2506.03877", "abs": "https://arxiv.org/abs/2506.03877", "authors": ["Christian Gang Liu", "Peter Bodorik", "Dawn Jutla"], "title": "Automated Mechanism to Support Trade Transactions in Smart Contracts with Upgrade and Repair", "categories": ["cs.SE"], "comment": null, "summary": "In our previous research, we addressed the problem of automated\ntransformation of models, represented using the business process model and\nnotation (BPMN) standard, into the methods of a smart contract. The\ntransformation supports BPMN models that contain complex multi-step activities\nthat are supported using our concept of multi-step nested trade transactions,\nwherein the transactional properties are enforced by a mechanism generated\nautomatically by the transformation process from a BPMN model to a smart\ncontract. In this paper, we present a methodology for repairing a smart\ncontract that cannot be completed due to events that were not anticipated by\nthe developer and thus prevent the completion of the smart contract. The repair\nprocess starts with the original BPMN model fragment causing the issue,\nproviding the modeler with the innermost transaction fragment containing the\nfailed activity. The modeler amends the BPMN pattern on the basis of successful\ncompletion of previous activities. If repairs exceed the inner transaction's\nscope, they are addressed using the parent transaction's BPMN model. The\namended BPMN model is then transformed into a new smart contract, ensuring\nconsistent data and logic transitions. We previously developed a tool, called\nTABS+, as a proof of concept (PoC) to transform BPMN models into smart\ncontracts for nested transactions. This paper describes the tool TABS+R,\ndeveloped by extending the TABS+ tool, to allow the repair of smart contracts.", "AI": {"tldr": "This paper introduces TABS+R, an extension of the TABS+ tool, for repairing smart contracts derived from BPMN models when unforeseen events prevent their completion.", "motivation": "The need arises from unanticipated events in smart contracts derived from BPMN models, which can halt execution. A repair methodology is proposed to address such issues.", "method": "The repair process involves identifying the problematic BPMN fragment, amending it based on prior successful activities, and transforming the updated model into a new smart contract.", "result": "TABS+R is developed as an extension of TABS+ to enable smart contract repair, ensuring data and logic consistency.", "conclusion": "The proposed methodology and tool (TABS+R) effectively address smart contract repair, enhancing reliability in nested transaction scenarios."}}
{"id": "2506.03903", "pdf": "https://arxiv.org/pdf/2506.03903", "abs": "https://arxiv.org/abs/2506.03903", "authors": ["Hugo Andrade", "Jo\u00e3o Bispo", "Filipe F. Correia"], "title": "Multi-Language Detection of Design Pattern Instances", "categories": ["cs.SE"], "comment": "Preprint accepted for publication in Journal of Software: Evolution\n  and Process, 2024", "summary": "Code comprehension is often supported by source code analysis tools which\nprovide more abstract views over software systems, such as those detecting\ndesign patterns. These tools encompass analysis of source code and ensuing\nextraction of relevant information. However, the analysis of the source code is\noften specific to the target programming language.\n  We propose DP-LARA, a multi-language pattern detection tool that uses the\nmulti-language capability of the LARA framework to support finding pattern\ninstances in a code base. LARA provides a virtual AST, which is common to\nmultiple OOP programming languages, and DP-LARA then performs code analysis of\ndetecting pattern instances on this abstract representation.\n  We evaluate the detection performance and consistency of DP-LARA with a few\nsoftware projects. Results show that a multi-language approach does not\ncompromise detection performance, and DP-LARA is consistent across the\nlanguages we tested it for (i.e., Java and C/C++). Moreover, by providing a\nvirtual AST as the abstract representation, we believe to have decreased the\neffort of extending the tool to new programming languages and maintaining\nexisting ones.", "AI": {"tldr": "DP-LARA is a multi-language pattern detection tool leveraging LARA's virtual AST for consistent design pattern detection across languages like Java and C/C++.", "motivation": "Existing tools for code comprehension are often language-specific, limiting their applicability. DP-LARA addresses this by using a multi-language approach.", "method": "DP-LARA utilizes LARA's virtual AST, a common abstract representation for multiple OOP languages, to detect design patterns.", "result": "DP-LARA maintains detection performance and consistency across languages (Java and C/C++), with reduced effort for extending to new languages.", "conclusion": "DP-LARA demonstrates the feasibility and benefits of a multi-language approach for design pattern detection, simplifying tool extension and maintenance."}}
{"id": "2506.03909", "pdf": "https://arxiv.org/pdf/2506.03909", "abs": "https://arxiv.org/abs/2506.03909", "authors": ["Lantian Li", "Zhihao Liu", "Zhongxing Yu"], "title": "Solsmith: Solidity Random Program Generator for Compiler Testing", "categories": ["cs.SE"], "comment": "11 pages, 12 figures", "summary": "Smart contracts are computer programs that run on blockchain platforms, with\nSolidity being the most widely used language for their development. As\nblockchain technology advances, smart contracts have become increasingly\nimportant across various fields. In order for smart contracts to operate\ncorrectly, the correctness of the compiler is particularly crucial. Although\nsome research efforts have been devoted to testing Solidity compilers, they\nprimarily focus on testing methods and do not address the core issue of\ngenerating test programs. To fill this gap, this paper designs and implements\nSolsmith, a test program generator specifically aimed at uncovering defects in\nSolidity compilers. It tests the compiler correctness by generating valid and\ndiverse Solidity programs. We have designed a series of unique program\ngeneration strategies tailored to Solidity, including enabling optimizations\nmore frequently, avoiding undefined behaviour, and mitigating behavioural\ndifferences caused by intermediate representations. To validate the\neffectiveness of Solsmith, we assess the effectiveness of the test programs\ngenerated by Solsmith using the approach of differential testing. The\npreliminary results show that Solsmith can generate the expected test programs\nand uncover four confirmed defects in Solidity compilers, demonstrating the\neffectiveness and potential of Solsmith.", "AI": {"tldr": "Solsmith is a test program generator for Solidity compilers, designed to uncover defects by generating diverse and valid Solidity programs. It successfully identified four confirmed defects.", "motivation": "Existing research on Solidity compiler testing lacks focus on generating test programs, leaving a gap in ensuring compiler correctness.", "method": "Solsmith employs unique program generation strategies for Solidity, such as frequent optimizations and avoiding undefined behavior, and uses differential testing for validation.", "result": "Solsmith generated effective test programs and uncovered four confirmed defects in Solidity compilers.", "conclusion": "Solsmith demonstrates effectiveness in testing Solidity compilers, highlighting its potential for improving compiler reliability."}}
{"id": "2506.03921", "pdf": "https://arxiv.org/pdf/2506.03921", "abs": "https://arxiv.org/abs/2506.03921", "authors": ["Xunzhu Tang", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Boosting Open-Source LLMs for Program Repair via Reasoning Transfer and LLM-Guided Reinforcement Learning", "categories": ["cs.SE"], "comment": null, "summary": "Several closed-source LLMs have consistently outperformed open-source\nalternatives in program repair tasks, primarily due to their superior reasoning\ncapabilities and extensive pre-training. This paper introduces Repairity, a\nnovel three-stage methodology that significantly narrows this performance gap\nthrough reasoning extraction and reinforcement learning. Our approach: (1)\nsystematically filters high-quality reasoning traces from closed-source models\nusing correctness verification, (2) transfers this reasoning knowledge to\nopen-source models via supervised fine-tuning, and (3) develops reinforcement\nlearning with LLM-based feedback to further optimize performance. Empirical\nevaluation across multiple program repair benchmarks demonstrates that\nRepairity improves the performance of Qwen2.5-Coder-32B-Instruct, a base open\nsource LLM, by 8.68\\% on average, reducing the capability gap with\nClaude-Sonnet3.7, a state-of-the-art closed-source model, from 10.05% to 1.35%.\nAblation studies confirm that both reasoning extraction and LLM-guided\nreinforcement learning contribute significantly to these improvements. Our\nmethodology generalizes effectively to additional code-related tasks, enabling\norganizations to leverage high-quality program repair capabilities while\nmaintaining the customizability, transparency, and deployment flexibility\ninherent to open-source models.", "AI": {"tldr": "Repairity, a three-stage method, narrows the performance gap between open-source and closed-source LLMs in program repair tasks by leveraging reasoning extraction and reinforcement learning, improving open-source model performance by 8.68%.", "motivation": "Closed-source LLMs outperform open-source ones in program repair due to better reasoning and pre-training. The paper aims to bridge this gap.", "method": "1. Filter high-quality reasoning traces from closed-source models. 2. Transfer knowledge via supervised fine-tuning. 3. Optimize with reinforcement learning and LLM feedback.", "result": "Repairity improves Qwen2.5-Coder-32B-Instruct by 8.68%, reducing the gap with Claude-Sonnet3.7 from 10.05% to 1.35%.", "conclusion": "The method generalizes to code tasks, offering high-quality repair while preserving open-source benefits like customizability and transparency."}}
{"id": "2506.03930", "pdf": "https://arxiv.org/pdf/2506.03930", "abs": "https://arxiv.org/abs/2506.03930", "authors": ["Yuansheng Ni", "Ping Nie", "Kai Zou", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.", "AI": {"tldr": "VisCode-200K is a dataset for improving LLMs' visualization tasks by providing execution-grounded supervision and iterative code correction. VisCoder, fine-tuned on this dataset, outperforms open-source models and nears proprietary ones like GPT-4o-mini.", "motivation": "LLMs struggle with visualization tasks due to lack of execution-grounded supervision and iterative correction support in existing datasets.", "method": "Created VisCode-200K, a dataset with 200K examples from validated plotting code and multi-turn correction dialogues. Fine-tuned Qwen2.5-Coder-Instruct to produce VisCoder.", "result": "VisCoder outperforms open-source baselines and approaches GPT-4o-mini's performance on PandasPlotBench.", "conclusion": "Feedback-driven learning and iterative repair improve executable, visually accurate code generation in LLMs."}}
{"id": "2506.03946", "pdf": "https://arxiv.org/pdf/2506.03946", "abs": "https://arxiv.org/abs/2506.03946", "authors": ["Dongming Jin", "Zhi Jin", "Nianyu Li", "Kai Yang", "Linyu Li", "Suijing Guan"], "title": "Automatic Multi-level Feature Tree Construction for Domain-Specific Reusable Artifacts Management", "categories": ["cs.SE"], "comment": "9pages, 2figures", "summary": "With the rapid growth of open-source ecosystems (e.g., Linux) and\ndomain-specific software projects (e.g., aerospace), efficient management of\nreusable artifacts is becoming increasingly crucial for software reuse. The\nmulti-level feature tree enables semantic management based on functionality and\nsupports requirements-driven artifact selection. However, constructing such a\ntree heavily relies on domain expertise, which is time-consuming and\nlabor-intensive. To address this issue, this paper proposes an automatic\nmulti-level feature tree construction framework named FTBUILDER, which consists\nof three stages. It automatically crawls domain-specific software repositories\nand merges their metadata to construct a structured artifact library. It\nemploys clustering algorithms to identify a set of artifacts with common\nfeatures. It constructs a prompt and uses LLMs to summarize their common\nfeatures. FTBUILDER recursively applies the identification and summarization\nstages to construct a multi-level feature tree from the bottom up. To validate\nFTBUILDER, we conduct experiments from multiple aspects (e.g., tree quality and\ntime cost) using the Linux distribution ecosystem. Specifically, we first\nsimultaneously develop and evaluate 24 alternative solutions in the FTBUILDER.\nWe then construct a three-level feature tree using the best solution among\nthem. Compared to the official feature tree, our tree exhibits higher quality,\nwith a 9% improvement in the silhouette coefficient and an 11% increase in\nGValue. Furthermore, it can save developers more time in selecting artifacts by\n26% and improve the accuracy of artifact recommendations with GPT-4 by 235%.\nFTBUILDER can be extended to other open-source software communities and\ndomain-specific industrial enterprises.", "AI": {"tldr": "FTBUILDER automates multi-level feature tree construction for software reuse, improving efficiency and accuracy in artifact selection.", "motivation": "Manual construction of feature trees is time-consuming and relies on domain expertise, necessitating an automated solution.", "method": "FTBUILDER crawls repositories, clusters artifacts, and uses LLMs to summarize features, building the tree recursively.", "result": "The framework improves tree quality (9% silhouette coefficient, 11% GValue), saves 26% selection time, and boosts GPT-4 recommendation accuracy by 235%.", "conclusion": "FTBUILDER is effective for open-source and domain-specific projects, offering scalability and efficiency."}}
{"id": "2506.04038", "pdf": "https://arxiv.org/pdf/2506.04038", "abs": "https://arxiv.org/abs/2506.04038", "authors": ["Sven Kirchner", "Alois C. Knoll"], "title": "Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems", "categories": ["cs.SE", "cs.AI"], "comment": "8 pages; Accepted for publication at the 36th IEEE Intelligent\n  Vehicles Symposium (IV), Cluj-Napoca, Romania, June 22-25, 2025", "summary": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements.", "AI": {"tldr": "A novel framework integrates GenAI into automotive SDLC using LLMs for automated, safety-compliant code generation, validated via an ACC system.", "motivation": "Addressing challenges in safety-critical automotive software development due to complexity and regulatory demands.", "method": "Proposes a framework with LLMs for automated code generation, incorporating safety practices like static verification and test-driven development, validated through ACC system development.", "result": "Demonstrates automatic code generation compliant with safety-critical requirements, optimizing LLM selection for accuracy.", "conclusion": "Advances AI use in safety-critical domains, aligning generative models with real-world safety needs."}}
{"id": "2506.04090", "pdf": "https://arxiv.org/pdf/2506.04090", "abs": "https://arxiv.org/abs/2506.04090", "authors": ["Federico Martusciello", "Henry Muccini", "Antonio Bucchiarone"], "title": "A Reference Architecture for Gamified Cultural Heritage Applications Leveraging Generative AI and Augmented Reality", "categories": ["cs.SE"], "comment": null, "summary": "The rapid advancement of Information and Communication Technologies is\ntransforming Cultural Heritage access, experience, and preservation. However,\nmany digital heritage applications lack interactivity, personalization, and\nadaptability, limiting user engagement and educational impact. This short paper\npresents a reference architecture for gamified cultural heritage applications\nleveraging generative AI and augmented reality. Gamification enhances\nmotivation, artificial intelligence enables adaptive storytelling and\npersonalized content, and augmented reality fosters immersive, location-aware\nexperiences. Integrating AI with gamification supports dynamic mechanics,\npersonalized feedback, and user behavior prediction, improving engagement. The\nmodular design supports scalability, interoperability, and adaptability across\nheritage contexts. This research provides a framework for designing interactive\nand intelligent cultural heritage applications, promoting accessibility and\ndeeper appreciation among users and stakeholders.", "AI": {"tldr": "A reference architecture for gamified cultural heritage apps using generative AI and AR to enhance interactivity, personalization, and engagement.", "motivation": "Digital heritage apps often lack interactivity and personalization, limiting user engagement and educational impact.", "method": "Proposes a modular architecture integrating gamification, generative AI, and augmented reality for adaptive storytelling and immersive experiences.", "result": "The framework improves engagement through dynamic mechanics, personalized feedback, and behavior prediction, while ensuring scalability and adaptability.", "conclusion": "The research offers a scalable, interoperable framework for designing intelligent cultural heritage apps, enhancing accessibility and user appreciation."}}
{"id": "2506.04161", "pdf": "https://arxiv.org/pdf/2506.04161", "abs": "https://arxiv.org/abs/2506.04161", "authors": ["Parsa Alian", "Martin Tang", "Ali Mesbah"], "title": "VISCA: Inferring Component Abstractions for Automated End-to-End Testing", "categories": ["cs.SE"], "comment": null, "summary": "Providing optimal contextual input presents a significant challenge for\nautomated end-to-end (E2E) test generation using large language models (LLMs),\na limitation that current approaches inadequately address. This paper\nintroduces Visual-Semantic Component Abstractor (VISCA), a novel method that\ntransforms webpages into a hierarchical, semantically rich component\nabstraction. VISCA starts by partitioning webpages into candidate segments\nutilizing a novel heuristic-based segmentation method. These candidate segments\nsubsequently undergo classification and contextual information extraction via\nmultimodal LLM-driven analysis, facilitating their abstraction into a\npredefined vocabulary of user interface (UI) components. This component-centric\nabstraction offers a more effective contextual basis than prior approaches,\nenabling more accurate feature inference and robust E2E test case generation.\nOur evaluations demonstrate that the test cases generated by VISCA achieve an\naverage feature coverage of 92%, exceeding the performance of the\nstate-of-the-art LLM-based E2E test generation method by 16%.", "AI": {"tldr": "VISCA improves E2E test generation by transforming webpages into hierarchical, semantically rich UI components, outperforming state-of-the-art methods by 16% in feature coverage.", "motivation": "Current LLM-based E2E test generation lacks optimal contextual input, limiting accuracy and robustness.", "method": "VISCA partitions webpages into segments, classifies them, and abstracts them into UI components using multimodal LLM analysis.", "result": "VISCA achieves 92% average feature coverage, 16% higher than state-of-the-art methods.", "conclusion": "VISCA's component-centric abstraction enhances contextual input, improving E2E test generation accuracy and robustness."}}
{"id": "2506.03651", "pdf": "https://arxiv.org/pdf/2506.03651", "abs": "https://arxiv.org/abs/2506.03651", "authors": ["Zeyu Gao", "Junlin Zhou", "Bolun Zhang", "Yi He", "Chao Zhang", "Yuxin Cui", "Hao Wang"], "title": "Mono: Is Your \"Clean\" Vulnerability Dataset Really Solvable? Exposing and Trapping Undecidable Patches and Beyond", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "The quantity and quality of vulnerability datasets are essential for\ndeveloping deep learning solutions to vulnerability-related tasks. Due to the\nlimited availability of vulnerabilities, a common approach to building such\ndatasets is analyzing security patches in source code. However, existing\nsecurity patches often suffer from inaccurate labels, insufficient contextual\ninformation, and undecidable patches that fail to clearly represent the root\ncauses of vulnerabilities or their fixes. These issues introduce noise into the\ndataset, which can mislead detection models and undermine their effectiveness.\nTo address these issues, we present mono, a novel LLM-powered framework that\nsimulates human experts' reasoning process to construct reliable vulnerability\ndatasets. mono introduces three key components to improve security patch\ndatasets: (i) semantic-aware patch classification for precise vulnerability\nlabeling, (ii) iterative contextual analysis for comprehensive code\nunderstanding, and (iii) systematic root cause analysis to identify and filter\nundecidable patches. Our comprehensive evaluation on the MegaVul benchmark\ndemonstrates that mono can correct 31.0% of labeling errors, recover 89% of\ninter-procedural vulnerabilities, and reveals that 16.7% of CVEs contain\nundecidable patches. Furthermore, mono's enriched context representation\nimproves existing models' vulnerability detection accuracy by 15%. We open\nsource the framework mono and the dataset MonoLens in\nhttps://github.com/vul337/mono.", "AI": {"tldr": "The paper introduces mono, an LLM-powered framework to improve vulnerability datasets by addressing labeling errors, contextual gaps, and undecidable patches, enhancing detection accuracy by 15%.", "motivation": "Existing vulnerability datasets suffer from inaccurate labels, insufficient context, and undecidable patches, which mislead detection models.", "method": "mono uses semantic-aware patch classification, iterative contextual analysis, and systematic root cause analysis to refine datasets.", "result": "mono corrects 31.0% of labeling errors, recovers 89% of inter-procedural vulnerabilities, and identifies 16.7% of CVEs with undecidable patches, improving detection accuracy by 15%.", "conclusion": "mono effectively constructs reliable vulnerability datasets and enhances model performance, with the framework and dataset MonoLens made publicly available."}}
{"id": "2506.03283", "pdf": "https://arxiv.org/pdf/2506.03283", "abs": "https://arxiv.org/abs/2506.03283", "authors": ["Viola Campos", "Ridwan Shariffdeen", "Adrian Ulges", "Yannic Noller"], "title": "Empirical Evaluation of Generalizable Automated Program Repair with Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Automated Program Repair (APR) proposes bug fixes to aid developers in\nmaintaining software. The state of the art in this domain focuses on using\nLLMs, leveraging their strong capabilities to comprehend specifications in\nnatural language and to generate program code. Recent works have shown that\nLLMs can be used to generate repairs. However, despite the APR community's\nresearch achievements and several industry deployments in the last decade, APR\nstill lacks the capabilities to generalize broadly. In this work, we present an\nintensive empirical evaluation of LLMs for generating patches. We evaluate a\ndiverse set of 13 recent models, including open ones (e.g., Llama 3.3, Qwen 2.5\nCoder, and DeepSeek R1 (dist.)) and closed ones (e.g., o3-mini, GPT-4o, Claude\n3.7 Sonnet, Gemini 2.0 Flash). In particular, we explore language-agnostic\nrepairs by utilizing benchmarks for Java (e.g., Defects4J), JavaScript (e.g.,\nBugsJS), Python (e.g., BugsInPy), and PHP (e.g., BugsPHP). Besides the\ngeneralization between different languages and levels of patch complexity, we\nalso investigate the effects of fault localization (FL) as a preprocessing step\nand compare the progress for open vs closed models. Our evaluation represents a\nsnapshot of the current repair capabilities of the latest LLMs. Key results\ninclude: (1) Different LLMs tend to perform best for different languages, which\nmakes it hard to develop cross-platform repair techniques with single LLMs. (2)\nThe combinations of models add value with respect to uniquely fixed bugs, so a\ncommittee of expert models should be considered. (3) Under realistic\nassumptions of imperfect FL, we observe significant drops in accuracy from the\nusual practice of using perfect FL. Our findings and insights will help both\nresearchers and practitioners develop reliable and generalizable APR techniques\nand evaluate them in realistic and fair environments.", "AI": {"tldr": "An empirical evaluation of 13 LLMs for automated program repair (APR) reveals challenges in generalization, language-specific performance, and the impact of fault localization.", "motivation": "Despite advancements in APR using LLMs, broad generalization remains a challenge. This work evaluates the current capabilities of LLMs in APR across languages and patch complexities.", "method": "Evaluated 13 LLMs (open and closed) on benchmarks for Java, JavaScript, Python, and PHP, exploring language-agnostic repairs and the impact of fault localization.", "result": "(1) LLMs perform variably by language, complicating cross-platform repairs. (2) Combining models improves fixes. (3) Imperfect fault localization reduces accuracy.", "conclusion": "Findings guide researchers and practitioners in developing reliable APR techniques and realistic evaluations."}}
{"id": "2506.03396", "pdf": "https://arxiv.org/pdf/2506.03396", "abs": "https://arxiv.org/abs/2506.03396", "authors": ["Jinhan Kim", "Nargiz Humbatova", "Gunel Jahangirova", "Shin Yoo", "Paolo Tonella"], "title": "Fault Localisation and Repair for DL Systems: An Empirical Study with LLMs", "categories": ["cs.SE"], "comment": "arXiv admin note: text overlap with arXiv:2301.11568", "summary": "Numerous Fault Localisation (FL) and repair techniques have been proposed to\naddress faults in Deep Learning (DL) models. However, their effectiveness in\npractical applications remains uncertain due to the reliance on pre-defined\nrules. This paper presents a comprehensive evaluation of state-of-the-art FL\nand repair techniques, examining their advantages and limitations. Moreover, we\nintroduce a novel approach that harnesses the power of Large Language Models\n(LLMs) in localising and repairing DL faults. Our evaluation, conducted on a\ncarefully designed benchmark, reveals the strengths and weaknesses of current\nFL and repair techniques. We emphasise the importance of enhanced accuracy and\nthe need for more rigorous assessment methods that employ multiple ground truth\npatches. Notably, LLMs exhibit remarkable performance in both FL and repair\ntasks. For instance, the GPT-4 model achieves 44% and 82% improvements in FL\nand repair tasks respectively, compared to the second-best tool, demonstrating\nthe potential of LLMs in this domain. Our study sheds light on the current\nstate of FL and repair techniques and suggests that LLMs could be a promising\navenue for future advancements.", "AI": {"tldr": "The paper evaluates Fault Localisation (FL) and repair techniques for Deep Learning (DL) models, introduces a novel LLM-based approach, and highlights GPT-4's superior performance.", "motivation": "Uncertain effectiveness of existing FL and repair techniques due to reliance on pre-defined rules.", "method": "Comprehensive evaluation of state-of-the-art techniques and introduction of an LLM-based approach, tested on a benchmark.", "result": "LLMs, especially GPT-4, outperform others with 44% and 82% improvements in FL and repair tasks.", "conclusion": "LLMs show promise for advancing FL and repair techniques, emphasizing the need for better accuracy and assessment methods."}}
{"id": "2506.03401", "pdf": "https://arxiv.org/pdf/2506.03401", "abs": "https://arxiv.org/abs/2506.03401", "authors": ["Xiwei Xu", "Hans Weytjens", "Dawen Zhang", "Qinghua Lu", "Ingo Weber", "Liming Zhu"], "title": "RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines", "categories": ["cs.SE"], "comment": null, "summary": "Recent studies show that 60% of LLM-based compound systems in enterprise\nenvironments leverage some form of retrieval-augmented generation (RAG), which\nenhances the relevance and accuracy of LLM (or other genAI) outputs by\nretrieving relevant information from external data sources. LLMOps involves the\npractices and techniques for managing the lifecycle and operations of LLM\ncompound systems in production environments. It supports enhancing LLM systems\nthrough continuous operations and feedback evaluation. RAGOps extends LLMOps by\nincorporating a strong focus on data management to address the continuous\nchanges in external data sources. This necessitates automated methods for\nevaluating and testing data operations, enhancing retrieval relevance and\ngeneration quality. In this paper, we (1) characterize the generic architecture\nof RAG applications based on the 4+1 model view for describing software\narchitectures, (2) outline the lifecycle of RAG systems, which integrates the\nmanagement lifecycles of both the LLM and the data, (3) define the key design\nconsiderations of RAGOps across different stages of the RAG lifecycle and\nquality trade-off analyses, (4) highlight the overarching research challenges\naround RAGOps, and (5) present two use cases of RAG applications and the\ncorresponding RAGOps considerations.", "AI": {"tldr": "The paper discusses RAGOps, an extension of LLMOps, focusing on data management for LLM-based systems. It outlines RAG architecture, lifecycle, design considerations, challenges, and use cases.", "motivation": "To address the dynamic nature of external data sources in LLM systems and improve retrieval relevance and generation quality through RAGOps.", "method": "Characterizes RAG architecture, outlines lifecycle, defines design considerations, highlights challenges, and presents use cases.", "result": "A framework for RAGOps, integrating LLM and data management lifecycles, with practical insights from use cases.", "conclusion": "RAGOps is essential for managing RAG systems, requiring automated evaluation and addressing evolving data challenges."}}
{"id": "2506.03504", "pdf": "https://arxiv.org/pdf/2506.03504", "abs": "https://arxiv.org/abs/2506.03504", "authors": ["Zhuo Zhuo", "Xiangyu Zhang"], "title": "Beyond C/C++: Probabilistic and LLM Methods for Next-Generation Software Reverse Engineering", "categories": ["cs.SE"], "comment": null, "summary": "This proposal discusses the growing challenges in reverse engineering modern\nsoftware binaries, particularly those compiled from newer system programming\nlanguages such as Rust, Go, and Mojo. Traditional reverse engineering\ntechniques, developed with a focus on C and C++, fall short when applied to\nthese newer languages due to their reliance on outdated heuristics and failure\nto fully utilize the rich semantic information embedded in binary programs.\nThese challenges are exacerbated by the limitations of current data-driven\nmethods, which are susceptible to generating inaccurate results, commonly\nreferred to as hallucinations. To overcome these limitations, we propose a\nnovel approach that integrates probabilistic binary analysis with fine-tuned\nlarge language models (LLMs). Our method systematically models the\nuncertainties inherent in reverse engineering, enabling more accurate reasoning\nabout incomplete or ambiguous information. By incorporating LLMs, we extend the\nanalysis beyond traditional heuristics, allowing for more creative and\ncontext-aware inferences, particularly for binaries from diverse programming\nlanguages. This hybrid approach not only enhances the robustness and accuracy\nof reverse engineering efforts but also offers a scalable solution adaptable to\nthe rapidly evolving landscape of software development.", "AI": {"tldr": "A novel hybrid approach combining probabilistic binary analysis and fine-tuned LLMs addresses challenges in reverse engineering binaries from newer languages like Rust, Go, and Mojo, improving accuracy and scalability.", "motivation": "Traditional reverse engineering techniques for C/C++ are inadequate for newer languages due to outdated heuristics and lack of semantic utilization, compounded by data-driven method limitations.", "method": "Integrates probabilistic binary analysis with fine-tuned LLMs to model uncertainties and enable context-aware inferences.", "result": "Enhances robustness and accuracy of reverse engineering, offering scalable solutions for diverse programming languages.", "conclusion": "The hybrid approach overcomes limitations of traditional methods, adapting to modern software development trends."}}
{"id": "2506.03382", "pdf": "https://arxiv.org/pdf/2506.03382", "abs": "https://arxiv.org/abs/2506.03382", "authors": ["Matteo Palazzo", "Luca Roversi"], "title": "Towards a Characterization of Two-way Bijections in a Reversible Computational Model", "categories": ["cs.LO", "cs.CC", "cs.PL", "F.3.2"], "comment": "8 pages, 3 figures, 5 listings. Author's copy of the version which\n  will appear in the Proceedings of the 17th International Conference, RC 2025,\n  Odense, Denmark, July 3-4, 2025", "summary": "We introduce an imperative, stack-based, and reversible computational model\nthat characterizes Two-way Bijections both implicitly, concerning their\ncomputational complexity, and with zero-garbage.", "AI": {"tldr": "A reversible, stack-based computational model for Two-way Bijections with zero-garbage.", "motivation": "To characterize Two-way Bijections in terms of computational complexity and efficiency.", "method": "Introduces an imperative, stack-based, and reversible computational model.", "result": "Achieves zero-garbage and characterizes computational complexity.", "conclusion": "The model effectively represents Two-way Bijections with computational efficiency."}}
{"id": "2506.03507", "pdf": "https://arxiv.org/pdf/2506.03507", "abs": "https://arxiv.org/abs/2506.03507", "authors": ["Eric O'Donoghue", "Yvette Hastings", "Ernesto Ortiz", "A. Redempta Manzi Muneza"], "title": "Software Bill of Materials in Software Supply Chain Security A Systematic Literature Review", "categories": ["cs.SE", "cs.CR"], "comment": "16 pages, 4 figures, 5 tables", "summary": "Software Bill of Materials (SBOMs) are increasingly regarded as essential\ntools for securing software supply chains (SSCs), yet their real-world use and\nadoption barriers remain poorly understood. This systematic literature review\nsynthesizes evidence from 40 peer-reviewed studies to evaluate how SBOMs are\ncurrently used to bolster SSC security. We identify five primary application\nareas: vulnerability management, transparency, component assessment, risk\nassessment, and SSC integrity. Despite clear promise, adoption is hindered by\nsignificant barriers: generation tooling, data privacy, format/standardization,\nsharing/distribution, cost/overhead, vulnerability exploitability, maintenance,\nanalysis tooling, false positives, hidden packages, and tampering. To structure\nour analysis, we map these barriers to the ISO/IEC 25019:2023 Quality-in-Use\nmodel, revealing critical deficiencies in SBOM trustworthiness, usability, and\nsuitability for security tasks. We also highlight key gaps in the literature.\nThese include the absence of applying machine learning techniques to assess\nSBOMs and limited evaluation of SBOMs and SSCs using software quality assurance\ntechniques. Our findings provide actionable insights for researchers, tool\ndevelopers, and practitioners seeking to advance SBOM-driven SSC security and\nlay a foundation for future work at the intersection of SSC assurance,\nautomation, and empirical software engineering.", "AI": {"tldr": "The paper reviews SBOMs' role in securing software supply chains, identifying five application areas and adoption barriers, while highlighting research gaps like machine learning applications.", "motivation": "To understand SBOMs' real-world use and adoption barriers in securing software supply chains.", "method": "A systematic literature review of 40 peer-reviewed studies.", "result": "Identified five SBOM application areas and key adoption barriers, mapped to ISO/IEC 25019:2023. Highlighted gaps include lack of machine learning and quality assurance evaluations.", "conclusion": "SBOMs show promise but face adoption challenges. Future work should address gaps in automation, assurance, and empirical evaluation."}}
{"id": "2506.04019", "pdf": "https://arxiv.org/pdf/2506.04019", "abs": "https://arxiv.org/abs/2506.04019", "authors": ["Neeva Oza", "Ishaan Govil", "Parul Gupta", "Dinesh Khandelwal", "Dinesh Garg", "Parag Singla"], "title": "CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL", "68-02 (Primary) 68T50, 68T07, 68N19, 68N30 (Secondary)", "I.2.7; I.2.6; I.2.5; D.3.0; D.3.3; D.3.1; F.3.2; F.3.1; F.3.3;\n  D.2.3; D.2.5"], "comment": null, "summary": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code.", "AI": {"tldr": "The paper introduces CETBench, a benchmark for code-equivalence checking using LLMs, revealing performance drops with simple code transformations and proposing a fine-tuning solution.", "motivation": "To explore LLMs' capability in code-equivalence checking, a critical task for evaluating LLMs in code-related tasks like re-writing and translation.", "method": "Developed CETBench by applying random code transformations to program pairs, then analyzed LLM performance and proposed fine-tuning to improve it.", "result": "Simple code transformations significantly reduce LLM performance; fine-tuning boosts accuracy on transformed pairs.", "conclusion": "LLMs lack deep semantic understanding of code, highlighting the need for improved methods in code-equivalence tasks."}}
{"id": "2506.03535", "pdf": "https://arxiv.org/pdf/2506.03535", "abs": "https://arxiv.org/abs/2506.03535", "authors": ["Qiming Zhu", "Jialun Cao", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Shing-Chi Cheung"], "title": "Across Programming Language Silos: A Study on Cross-Lingual Retrieval-augmented Code Generation", "categories": ["cs.SE"], "comment": null, "summary": "Current research on large language models (LLMs) with retrieval-augmented\ncode generation (RACG) mainly focuses on single-language settings, leaving\ncross-lingual effectiveness and security unexplored. Multi-lingual RACG systems\nare valuable for migrating code-bases across programming languages (PLs), yet\nface risks from error (e.g. adversarial data corruption) propagation in\ncross-lingual transfer. We construct a dataset spanning 13 PLs with nearly 14k\ninstances to explore utility and robustness of multi-lingual RACG systems. Our\ninvestigation reveals four key insights: (1) Effectiveness: multi-lingual RACG\nsignificantly enhances multi-lingual code LLMs generation; (2) Inequality: Java\ndemonstrate superior cross-lingual utility over Python in RACG; (3) Robustness:\nAdversarial attacks degrade performance significantly in mono-lingual RACG but\nshow mitigated impacts in cross-lingual scenarios; Counterintuitively,\nperturbed code may improve RACG in cross-lingual scenarios; (4) Specialization:\nDomain-specific code retrievers outperform significantly general text\nretrievers. These findings establish foundation for developing effective and\nsecure multi-lingual code assistants.", "AI": {"tldr": "The paper explores multi-lingual retrieval-augmented code generation (RACG), highlighting its effectiveness, robustness, and inequalities across 13 programming languages.", "motivation": "To address the unexplored areas of cross-lingual effectiveness and security in RACG systems, especially for migrating code-bases across languages.", "method": "Constructed a dataset of 13 PLs with 14k instances to analyze utility and robustness of multi-lingual RACG.", "result": "Key findings include enhanced multi-lingual code generation, Java's superiority over Python, mitigated adversarial impacts in cross-lingual scenarios, and domain-specific retrievers outperforming general ones.", "conclusion": "The study lays groundwork for developing secure and effective multi-lingual code assistants."}}
{"id": "2506.03585", "pdf": "https://arxiv.org/pdf/2506.03585", "abs": "https://arxiv.org/abs/2506.03585", "authors": ["Inseok Yeo", "Duksan Ryu", "Jongmoon Baik"], "title": "Improving LLM-Based Fault Localization with External Memory and Project Context", "categories": ["cs.SE"], "comment": "12 Pages, 7 figures", "summary": "Fault localization, the process of identifying the software components\nresponsible for failures, is essential but often time-consuming. Recent\nadvances in Large Language Models (LLMs) have enabled fault localization\nwithout extensive defect datasets or model fine-tuning. However, existing\nLLM-based methods rely only on general LLM capabilities and lack integration of\nproject-specific knowledge, resulting in limited effectiveness, especially for\ncomplex software.\n  We introduce MemFL, a novel approach that enhances LLM-based fault\nlocalization by integrating project-specific knowledge via external memory.\nThis memory includes static summaries of the project and dynamic, iterative\ndebugging insights gathered from previous attempts. By leveraging external\nmemory, MemFL simplifies debugging into three streamlined steps, significantly\nimproving efficiency and accuracy. Iterative refinement through dynamic memory\nfurther enhances reasoning quality over time.\n  Evaluated on the Defects4J benchmark, MemFL using GPT-4o-mini localized 12.7%\nmore bugs than current LLM-based methods, achieving this improvement with just\n21% of the execution time (17.4 seconds per bug) and 33% of the API cost\n(0.0033 dollars per bug). On complex projects, MemFL's advantage increased to\n27.6%. Additionally, MemFL with GPT-4.1-mini outperformed existing methods by\n24.4%, requiring only 24.7 seconds and 0.0094 dollars per bug. MemFL thus\ndemonstrates significant improvements by effectively incorporating\nproject-specific knowledge into LLM-based fault localization, delivering high\naccuracy with reduced time and cost.", "AI": {"tldr": "MemFL enhances LLM-based fault localization by integrating project-specific knowledge via external memory, improving accuracy and efficiency.", "motivation": "Existing LLM-based fault localization lacks project-specific knowledge, limiting effectiveness, especially for complex software.", "method": "MemFL uses external memory (static summaries and dynamic debugging insights) to streamline debugging into three steps, refining reasoning iteratively.", "result": "On Defects4J, MemFL with GPT-4o-mini localized 12.7% more bugs, using 21% execution time and 33% API cost. GPT-4.1-mini improved results by 24.4%.", "conclusion": "MemFL significantly improves fault localization by leveraging project-specific knowledge, achieving higher accuracy with reduced time and cost."}}
{"id": "2506.03691", "pdf": "https://arxiv.org/pdf/2506.03691", "abs": "https://arxiv.org/abs/2506.03691", "authors": ["Weiyuan Xu", "Juntao Luo", "Tao Huang", "Kaixin Sui", "Jie Geng", "Qijun Ma", "Isami Akasaka", "Xiaoxue Shi", "Jing Tang", "Peng Cai"], "title": "A Two-Staged LLM-Based Framework for CI/CD Failure Detection and Remediation with Industrial Validation", "categories": ["cs.SE"], "comment": "12 pages, 5 figures", "summary": "Continuous Integration and Continuous Deployment (CI/CD) pipelines are\npivotal to modern software engineering, yet diagnosing and resolving their\nfailures remains a complex and labor-intensive challenge. In this paper, we\npresent LogSage, the first end-to-end LLM-powered framework that performs root\ncause analysis and solution generation from failed CI/CD pipeline logs. During\nthe root cause analysis stage, LogSage employs a specialized log preprocessing\npipeline tailored for LLMs, which extracts critical error logs and eliminates\nnoise to enhance the precision of LLM-driven root cause analysis. In the\nsolution generation stage, LogSage leverages RAG to integrate historical\nresolution strategies and utilizes tool-calling to deliver actionable,\nautomated fixes. We evaluated the root cause analysis stage using a newly\ncurated open-source dataset, achieving 98\\% in precision and 12\\% improvement\nover naively designed LLM-based log analysis baselines, while attaining\nnear-perfect recall. The end-to-end system was rigorously validated in a\nlarge-scale industrial CI/CD environment of production quality, processing more\nthan 3,000 executions daily and accumulating more than 1.07 million executions\nin its first year of deployment, with end-to-end precision exceeding 88\\%.\nThese two forms of evaluation confirm that LogSage providing a scalable and\npractical solution to manage CI/CD pipeline failures in real-world DevOps\nworkflows.", "AI": {"tldr": "LogSage is an LLM-powered framework for diagnosing and fixing CI/CD pipeline failures, achieving high precision and recall in root cause analysis and solution generation.", "motivation": "CI/CD pipeline failures are complex and labor-intensive to diagnose and resolve, necessitating an automated solution.", "method": "LogSage uses a specialized log preprocessing pipeline for LLMs, RAG for historical strategies, and tool-calling for automated fixes.", "result": "Achieved 98% precision in root cause analysis and 88% end-to-end precision in industrial deployment.", "conclusion": "LogSage is a scalable, practical solution for managing CI/CD failures in real-world DevOps workflows."}}
{"id": "2506.03801", "pdf": "https://arxiv.org/pdf/2506.03801", "abs": "https://arxiv.org/abs/2506.03801", "authors": ["Peter Pfeiffer", "Alexander Rombach", "Maxim Majlatow", "Nijat Mehdiyev"], "title": "From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation", "categories": ["cs.SE", "cs.LG", "cs.MA"], "comment": "Accepted to the Next Gen Data and Process Management: Large Language\n  Models and Beyond workshop at SIGMOD 2025", "summary": "Traditional Business Process Management (BPM) struggles with rigidity,\nopacity, and scalability in dynamic environments while emerging Large Language\nModels (LLMs) present transformative opportunities alongside risks. This paper\nexplores four real-world use cases that demonstrate how LLMs, augmented with\ntrustworthy process intelligence, redefine process modeling, prediction, and\nautomation. Grounded in early-stage research projects with industrial partners,\nthe work spans manufacturing, modeling, life-science, and design processes,\naddressing domain-specific challenges through human-AI collaboration. In\nmanufacturing, an LLM-driven framework integrates uncertainty-aware explainable\nMachine Learning (ML) with interactive dialogues, transforming opaque\npredictions into auditable workflows. For process modeling, conversational\ninterfaces democratize BPMN design. Pharmacovigilance agents automate drug\nsafety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable\ntextile design employs multi-agent systems to navigate regulatory and\nenvironmental trade-offs. We intend to examine tensions between transparency\nand efficiency, generalization and specialization, and human agency versus\nautomation. By mapping these trade-offs, we advocate for context-sensitive\nintegration prioritizing domain needs, stakeholder values, and iterative\nhuman-in-the-loop workflows over universal solutions. This work provides\nactionable insights for researchers and practitioners aiming to operationalize\nLLMs in critical BPM environments.", "AI": {"tldr": "LLMs enhance BPM by addressing rigidity and opacity through human-AI collaboration, demonstrated in manufacturing, modeling, life-science, and design use cases.", "motivation": "Traditional BPM lacks adaptability and transparency in dynamic environments; LLMs offer transformative potential but require trustworthy integration.", "method": "Four real-world use cases explore LLMs augmented with process intelligence for modeling, prediction, and automation, emphasizing human-AI collaboration.", "result": "LLMs improve process transparency, democratize modeling, automate monitoring, and navigate trade-offs in diverse domains.", "conclusion": "Context-sensitive LLM integration, prioritizing domain needs and human-in-the-loop workflows, is advocated over universal solutions."}}
{"id": "2506.03840", "pdf": "https://arxiv.org/pdf/2506.03840", "abs": "https://arxiv.org/abs/2506.03840", "authors": ["Pragya Verma", "Marcos Vinicius Cruz", "Grischa Liebel"], "title": "Differences between Neurodivergent and Neurotypical Software Engineers: Analyzing the 2022 Stack Overflow Survey", "categories": ["cs.SE"], "comment": null, "summary": "Neurodiversity describes variation in brain function among people, including\ncommon conditions such as Autism spectrum disorder (ASD), Attention deficit\nhyperactivity disorder (ADHD), and dyslexia. While Software Engineering (SE)\nliterature has started to explore the experiences of neurodivergent software\nengineers, there is a lack of research that compares their challenges to those\nof neurotypical software engineers. To address this gap, we analyze existing\ndata from the 2022 Stack Overflow Developer survey that collected data on\nneurodiversity. We quantitatively compare the answers of professional engineers\nwith ASD (n=374), ADHD (n=1305), and dyslexia (n=363) with neurotypical\nengineers. Our findings indicate that neurodivergent engineers face more\ndifficulties than neurotypical engineers. Specifically, engineers with ADHD\nreport that they face more interruptions caused by waiting for answers, and\nthat they less frequently interact with individuals outside their team. This\nstudy provides a baseline for future research comparing neurodivergent\nengineers with neurotypical ones. Several factors in the Stack Overflow survey\nand in our analysis are likely to lead to conservative estimates of the actual\neffects between neurodivergent and neurotypical engineers, e.g., the effects of\nthe COVID-19 pandemic and our focus on employed professionals.", "AI": {"tldr": "The study compares challenges faced by neurodivergent (ASD, ADHD, dyslexia) and neurotypical software engineers using 2022 Stack Overflow data, finding neurodivergent engineers face more difficulties.", "motivation": "To address the lack of research comparing challenges of neurodivergent and neurotypical software engineers.", "method": "Quantitative analysis of 2022 Stack Overflow Developer survey data, comparing responses of neurodivergent (ASD, ADHD, dyslexia) and neurotypical engineers.", "result": "Neurodivergent engineers face more difficulties; ADHD engineers report more interruptions and less interaction outside their team.", "conclusion": "The study provides a baseline for future research, noting conservative estimates due to survey limitations and COVID-19 impacts."}}
{"id": "2506.03877", "pdf": "https://arxiv.org/pdf/2506.03877", "abs": "https://arxiv.org/abs/2506.03877", "authors": ["Christian Gang Liu", "Peter Bodorik", "Dawn Jutla"], "title": "Automated Mechanism to Support Trade Transactions in Smart Contracts with Upgrade and Repair", "categories": ["cs.SE"], "comment": null, "summary": "In our previous research, we addressed the problem of automated\ntransformation of models, represented using the business process model and\nnotation (BPMN) standard, into the methods of a smart contract. The\ntransformation supports BPMN models that contain complex multi-step activities\nthat are supported using our concept of multi-step nested trade transactions,\nwherein the transactional properties are enforced by a mechanism generated\nautomatically by the transformation process from a BPMN model to a smart\ncontract. In this paper, we present a methodology for repairing a smart\ncontract that cannot be completed due to events that were not anticipated by\nthe developer and thus prevent the completion of the smart contract. The repair\nprocess starts with the original BPMN model fragment causing the issue,\nproviding the modeler with the innermost transaction fragment containing the\nfailed activity. The modeler amends the BPMN pattern on the basis of successful\ncompletion of previous activities. If repairs exceed the inner transaction's\nscope, they are addressed using the parent transaction's BPMN model. The\namended BPMN model is then transformed into a new smart contract, ensuring\nconsistent data and logic transitions. We previously developed a tool, called\nTABS+, as a proof of concept (PoC) to transform BPMN models into smart\ncontracts for nested transactions. This paper describes the tool TABS+R,\ndeveloped by extending the TABS+ tool, to allow the repair of smart contracts.", "AI": {"tldr": "This paper introduces TABS+R, an extension of the TABS+ tool, for repairing smart contracts derived from BPMN models when unanticipated events prevent completion.", "motivation": "Addresses the issue of smart contracts failing due to unanticipated events, requiring a repair methodology to ensure completion.", "method": "Repairs start with the problematic BPMN fragment, allowing modelers to amend the model based on successful prior activities. If repairs exceed the inner transaction's scope, parent transactions are used. The amended model is transformed into a new smart contract.", "result": "TABS+R is developed to enable smart contract repair, ensuring consistent data and logic transitions.", "conclusion": "The methodology and tool (TABS+R) provide a solution for repairing incomplete smart contracts, enhancing reliability and adaptability."}}
{"id": "2506.03903", "pdf": "https://arxiv.org/pdf/2506.03903", "abs": "https://arxiv.org/abs/2506.03903", "authors": ["Hugo Andrade", "Jo\u00e3o Bispo", "Filipe F. Correia"], "title": "Multi-Language Detection of Design Pattern Instances", "categories": ["cs.SE"], "comment": "Preprint accepted for publication in Journal of Software: Evolution\n  and Process, 2024", "summary": "Code comprehension is often supported by source code analysis tools which\nprovide more abstract views over software systems, such as those detecting\ndesign patterns. These tools encompass analysis of source code and ensuing\nextraction of relevant information. However, the analysis of the source code is\noften specific to the target programming language.\n  We propose DP-LARA, a multi-language pattern detection tool that uses the\nmulti-language capability of the LARA framework to support finding pattern\ninstances in a code base. LARA provides a virtual AST, which is common to\nmultiple OOP programming languages, and DP-LARA then performs code analysis of\ndetecting pattern instances on this abstract representation.\n  We evaluate the detection performance and consistency of DP-LARA with a few\nsoftware projects. Results show that a multi-language approach does not\ncompromise detection performance, and DP-LARA is consistent across the\nlanguages we tested it for (i.e., Java and C/C++). Moreover, by providing a\nvirtual AST as the abstract representation, we believe to have decreased the\neffort of extending the tool to new programming languages and maintaining\nexisting ones.", "AI": {"tldr": "DP-LARA is a multi-language design pattern detection tool leveraging LARA's virtual AST for consistent performance across languages like Java and C/C++.", "motivation": "Existing tools for code comprehension and pattern detection are often language-specific, limiting their applicability.", "method": "DP-LARA uses LARA's virtual AST to analyze code and detect patterns in a language-agnostic way.", "result": "DP-LARA maintains detection performance across languages (Java, C/C++) and reduces effort for tool extension/maintenance.", "conclusion": "DP-LARA offers a scalable, multi-language solution for design pattern detection with consistent results."}}
{"id": "2506.03909", "pdf": "https://arxiv.org/pdf/2506.03909", "abs": "https://arxiv.org/abs/2506.03909", "authors": ["Lantian Li", "Zhihao Liu", "Zhongxing Yu"], "title": "Solsmith: Solidity Random Program Generator for Compiler Testing", "categories": ["cs.SE"], "comment": "11 pages, 12 figures", "summary": "Smart contracts are computer programs that run on blockchain platforms, with\nSolidity being the most widely used language for their development. As\nblockchain technology advances, smart contracts have become increasingly\nimportant across various fields. In order for smart contracts to operate\ncorrectly, the correctness of the compiler is particularly crucial. Although\nsome research efforts have been devoted to testing Solidity compilers, they\nprimarily focus on testing methods and do not address the core issue of\ngenerating test programs. To fill this gap, this paper designs and implements\nSolsmith, a test program generator specifically aimed at uncovering defects in\nSolidity compilers. It tests the compiler correctness by generating valid and\ndiverse Solidity programs. We have designed a series of unique program\ngeneration strategies tailored to Solidity, including enabling optimizations\nmore frequently, avoiding undefined behaviour, and mitigating behavioural\ndifferences caused by intermediate representations. To validate the\neffectiveness of Solsmith, we assess the effectiveness of the test programs\ngenerated by Solsmith using the approach of differential testing. The\npreliminary results show that Solsmith can generate the expected test programs\nand uncover four confirmed defects in Solidity compilers, demonstrating the\neffectiveness and potential of Solsmith.", "AI": {"tldr": "Solsmith is a test program generator for Solidity compilers, designed to uncover defects by generating diverse and valid Solidity programs. It employs unique strategies and has successfully identified four confirmed defects.", "motivation": "Existing research on Solidity compiler testing lacks focus on generating test programs, creating a gap Solsmith aims to fill.", "method": "Solsmith generates test programs using tailored strategies like enabling optimizations, avoiding undefined behavior, and addressing intermediate representation differences.", "result": "Solsmith uncovered four confirmed defects in Solidity compilers, validating its effectiveness.", "conclusion": "Solsmith demonstrates potential as an effective tool for testing Solidity compiler correctness."}}
{"id": "2506.03921", "pdf": "https://arxiv.org/pdf/2506.03921", "abs": "https://arxiv.org/abs/2506.03921", "authors": ["Xunzhu Tang", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Boosting Open-Source LLMs for Program Repair via Reasoning Transfer and LLM-Guided Reinforcement Learning", "categories": ["cs.SE"], "comment": null, "summary": "Several closed-source LLMs have consistently outperformed open-source\nalternatives in program repair tasks, primarily due to their superior reasoning\ncapabilities and extensive pre-training. This paper introduces Repairity, a\nnovel three-stage methodology that significantly narrows this performance gap\nthrough reasoning extraction and reinforcement learning. Our approach: (1)\nsystematically filters high-quality reasoning traces from closed-source models\nusing correctness verification, (2) transfers this reasoning knowledge to\nopen-source models via supervised fine-tuning, and (3) develops reinforcement\nlearning with LLM-based feedback to further optimize performance. Empirical\nevaluation across multiple program repair benchmarks demonstrates that\nRepairity improves the performance of Qwen2.5-Coder-32B-Instruct, a base open\nsource LLM, by 8.68\\% on average, reducing the capability gap with\nClaude-Sonnet3.7, a state-of-the-art closed-source model, from 10.05% to 1.35%.\nAblation studies confirm that both reasoning extraction and LLM-guided\nreinforcement learning contribute significantly to these improvements. Our\nmethodology generalizes effectively to additional code-related tasks, enabling\norganizations to leverage high-quality program repair capabilities while\nmaintaining the customizability, transparency, and deployment flexibility\ninherent to open-source models.", "AI": {"tldr": "Repairity, a three-stage method, narrows the performance gap between open-source and closed-source LLMs in program repair by extracting reasoning from closed-source models and using reinforcement learning.", "motivation": "Closed-source LLMs outperform open-source ones in program repair due to better reasoning and pre-training. Repairity aims to bridge this gap.", "method": "Three stages: (1) filter high-quality reasoning traces from closed-source models, (2) transfer knowledge via fine-tuning, (3) optimize with reinforcement learning and LLM feedback.", "result": "Repairity improves Qwen2.5-Coder-32B-Instruct by 8.68%, reducing the gap with Claude-Sonnet3.7 from 10.05% to 1.35%.", "conclusion": "The method generalizes to other code tasks, offering open-source benefits like customizability and transparency while matching closed-source performance."}}
{"id": "2506.03930", "pdf": "https://arxiv.org/pdf/2506.03930", "abs": "https://arxiv.org/abs/2506.03930", "authors": ["Yuansheng Ni", "Ping Nie", "Kai Zou", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.", "AI": {"tldr": "VisCode-200K is a large-scale dataset for instruction-tuning LLMs on Python-based visualization tasks, enabling self-correction and outperforming baselines.", "motivation": "LLMs struggle with visualization tasks due to lack of execution-grounded supervision and iterative correction support.", "method": "Created VisCode-200K with 200K examples, including validated plotting code and multi-turn correction dialogues, and fine-tuned Qwen2.5-Coder-Instruct.", "result": "VisCoder outperforms open-source baselines and nears proprietary models like GPT-4o-mini.", "conclusion": "Feedback-driven learning improves executable and visually accurate code generation."}}
{"id": "2506.03946", "pdf": "https://arxiv.org/pdf/2506.03946", "abs": "https://arxiv.org/abs/2506.03946", "authors": ["Dongming Jin", "Zhi Jin", "Nianyu Li", "Kai Yang", "Linyu Li", "Suijing Guan"], "title": "Automatic Multi-level Feature Tree Construction for Domain-Specific Reusable Artifacts Management", "categories": ["cs.SE"], "comment": "9pages, 2figures", "summary": "With the rapid growth of open-source ecosystems (e.g., Linux) and\ndomain-specific software projects (e.g., aerospace), efficient management of\nreusable artifacts is becoming increasingly crucial for software reuse. The\nmulti-level feature tree enables semantic management based on functionality and\nsupports requirements-driven artifact selection. However, constructing such a\ntree heavily relies on domain expertise, which is time-consuming and\nlabor-intensive. To address this issue, this paper proposes an automatic\nmulti-level feature tree construction framework named FTBUILDER, which consists\nof three stages. It automatically crawls domain-specific software repositories\nand merges their metadata to construct a structured artifact library. It\nemploys clustering algorithms to identify a set of artifacts with common\nfeatures. It constructs a prompt and uses LLMs to summarize their common\nfeatures. FTBUILDER recursively applies the identification and summarization\nstages to construct a multi-level feature tree from the bottom up. To validate\nFTBUILDER, we conduct experiments from multiple aspects (e.g., tree quality and\ntime cost) using the Linux distribution ecosystem. Specifically, we first\nsimultaneously develop and evaluate 24 alternative solutions in the FTBUILDER.\nWe then construct a three-level feature tree using the best solution among\nthem. Compared to the official feature tree, our tree exhibits higher quality,\nwith a 9% improvement in the silhouette coefficient and an 11% increase in\nGValue. Furthermore, it can save developers more time in selecting artifacts by\n26% and improve the accuracy of artifact recommendations with GPT-4 by 235%.\nFTBUILDER can be extended to other open-source software communities and\ndomain-specific industrial enterprises.", "AI": {"tldr": "FTBUILDER automates multi-level feature tree construction for software reuse, improving efficiency and quality over manual methods.", "motivation": "Manual construction of feature trees for software reuse is time-consuming and requires domain expertise, necessitating an automated solution.", "method": "FTBUILDER crawls repositories, clusters artifacts, and uses LLMs to summarize features, recursively building a tree from the bottom up.", "result": "The framework improves tree quality (9% silhouette coefficient, 11% GValue), saves 26% time, and boosts recommendation accuracy by 235%.", "conclusion": "FTBUILDER is effective for open-source and domain-specific projects, offering scalable automation for artifact management."}}
{"id": "2506.04038", "pdf": "https://arxiv.org/pdf/2506.04038", "abs": "https://arxiv.org/abs/2506.04038", "authors": ["Sven Kirchner", "Alois C. Knoll"], "title": "Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems", "categories": ["cs.SE", "cs.AI"], "comment": "8 pages; Accepted for publication at the 36th IEEE Intelligent\n  Vehicles Symposium (IV), Cluj-Napoca, Romania, June 22-25, 2025", "summary": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements.", "AI": {"tldr": "A novel framework integrates GenAI into automotive SDLC, automating code generation with LLMs while ensuring safety compliance, validated via an ACC system.", "motivation": "Addressing challenges in safety-critical automotive software due to complexity and strict regulations by leveraging AI.", "method": "Uses LLMs for automated code generation in C++, incorporating safety practices like static verification and test-driven development, with a feedback-driven pipeline for compliance.", "result": "Demonstrates automatic code generation meeting safety-critical requirements, validated through ACC system development.", "conclusion": "Advances AI use in safety-critical domains by bridging generative models with real-world safety needs."}}
{"id": "2506.04090", "pdf": "https://arxiv.org/pdf/2506.04090", "abs": "https://arxiv.org/abs/2506.04090", "authors": ["Federico Martusciello", "Henry Muccini", "Antonio Bucchiarone"], "title": "A Reference Architecture for Gamified Cultural Heritage Applications Leveraging Generative AI and Augmented Reality", "categories": ["cs.SE"], "comment": null, "summary": "The rapid advancement of Information and Communication Technologies is\ntransforming Cultural Heritage access, experience, and preservation. However,\nmany digital heritage applications lack interactivity, personalization, and\nadaptability, limiting user engagement and educational impact. This short paper\npresents a reference architecture for gamified cultural heritage applications\nleveraging generative AI and augmented reality. Gamification enhances\nmotivation, artificial intelligence enables adaptive storytelling and\npersonalized content, and augmented reality fosters immersive, location-aware\nexperiences. Integrating AI with gamification supports dynamic mechanics,\npersonalized feedback, and user behavior prediction, improving engagement. The\nmodular design supports scalability, interoperability, and adaptability across\nheritage contexts. This research provides a framework for designing interactive\nand intelligent cultural heritage applications, promoting accessibility and\ndeeper appreciation among users and stakeholders.", "AI": {"tldr": "A reference architecture for gamified cultural heritage apps using generative AI and AR to enhance interactivity, personalization, and engagement.", "motivation": "Digital heritage apps often lack interactivity and personalization, limiting user engagement and educational impact.", "method": "Proposes a modular architecture integrating gamification, generative AI for adaptive storytelling, and AR for immersive experiences.", "result": "The framework improves engagement through dynamic mechanics, personalized feedback, and user behavior prediction.", "conclusion": "Provides a scalable, interoperable design for intelligent cultural heritage apps, promoting accessibility and deeper appreciation."}}
{"id": "2506.04161", "pdf": "https://arxiv.org/pdf/2506.04161", "abs": "https://arxiv.org/abs/2506.04161", "authors": ["Parsa Alian", "Martin Tang", "Ali Mesbah"], "title": "VISCA: Inferring Component Abstractions for Automated End-to-End Testing", "categories": ["cs.SE"], "comment": null, "summary": "Providing optimal contextual input presents a significant challenge for\nautomated end-to-end (E2E) test generation using large language models (LLMs),\na limitation that current approaches inadequately address. This paper\nintroduces Visual-Semantic Component Abstractor (VISCA), a novel method that\ntransforms webpages into a hierarchical, semantically rich component\nabstraction. VISCA starts by partitioning webpages into candidate segments\nutilizing a novel heuristic-based segmentation method. These candidate segments\nsubsequently undergo classification and contextual information extraction via\nmultimodal LLM-driven analysis, facilitating their abstraction into a\npredefined vocabulary of user interface (UI) components. This component-centric\nabstraction offers a more effective contextual basis than prior approaches,\nenabling more accurate feature inference and robust E2E test case generation.\nOur evaluations demonstrate that the test cases generated by VISCA achieve an\naverage feature coverage of 92%, exceeding the performance of the\nstate-of-the-art LLM-based E2E test generation method by 16%.", "AI": {"tldr": "VISCA transforms webpages into hierarchical, semantically rich UI components for better E2E test generation, outperforming state-of-the-art methods by 16%.", "motivation": "Current LLM-based E2E test generation lacks optimal contextual input, limiting accuracy and robustness.", "method": "VISCA partitions webpages into segments, classifies them, and abstracts them into UI components using multimodal LLM analysis.", "result": "VISCA achieves 92% feature coverage, outperforming the best existing method by 16%.", "conclusion": "VISCA's component-centric abstraction improves contextual input for more accurate and robust E2E test generation."}}
{"id": "2506.03651", "pdf": "https://arxiv.org/pdf/2506.03651", "abs": "https://arxiv.org/abs/2506.03651", "authors": ["Zeyu Gao", "Junlin Zhou", "Bolun Zhang", "Yi He", "Chao Zhang", "Yuxin Cui", "Hao Wang"], "title": "Mono: Is Your \"Clean\" Vulnerability Dataset Really Solvable? Exposing and Trapping Undecidable Patches and Beyond", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "The quantity and quality of vulnerability datasets are essential for\ndeveloping deep learning solutions to vulnerability-related tasks. Due to the\nlimited availability of vulnerabilities, a common approach to building such\ndatasets is analyzing security patches in source code. However, existing\nsecurity patches often suffer from inaccurate labels, insufficient contextual\ninformation, and undecidable patches that fail to clearly represent the root\ncauses of vulnerabilities or their fixes. These issues introduce noise into the\ndataset, which can mislead detection models and undermine their effectiveness.\nTo address these issues, we present mono, a novel LLM-powered framework that\nsimulates human experts' reasoning process to construct reliable vulnerability\ndatasets. mono introduces three key components to improve security patch\ndatasets: (i) semantic-aware patch classification for precise vulnerability\nlabeling, (ii) iterative contextual analysis for comprehensive code\nunderstanding, and (iii) systematic root cause analysis to identify and filter\nundecidable patches. Our comprehensive evaluation on the MegaVul benchmark\ndemonstrates that mono can correct 31.0% of labeling errors, recover 89% of\ninter-procedural vulnerabilities, and reveals that 16.7% of CVEs contain\nundecidable patches. Furthermore, mono's enriched context representation\nimproves existing models' vulnerability detection accuracy by 15%. We open\nsource the framework mono and the dataset MonoLens in\nhttps://github.com/vul337/mono.", "AI": {"tldr": "The paper introduces mono, an LLM-powered framework to improve vulnerability datasets by addressing labeling errors, contextual gaps, and undecidable patches, enhancing detection accuracy by 15%.", "motivation": "Existing vulnerability datasets suffer from noisy labels, incomplete context, and undecidable patches, which hinder deep learning solutions for vulnerability tasks.", "method": "mono uses semantic-aware patch classification, iterative contextual analysis, and systematic root cause analysis to refine datasets.", "result": "mono corrects 31.0% of labeling errors, recovers 89% of inter-procedural vulnerabilities, and identifies 16.7% of CVEs with undecidable patches, boosting detection accuracy by 15%.", "conclusion": "mono effectively improves vulnerability dataset quality, demonstrating significant enhancements in labeling, context, and detection performance."}}
