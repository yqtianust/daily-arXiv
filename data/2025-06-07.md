<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [math.NA](#math.NA) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges](https://arxiv.org/abs/2506.04418)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: The paper introduces HUNK4J, a dataset for multi-hunk bug fixes, and proposes metrics (hunk divergence, spatial proximity) to analyze patch complexity. It finds LLMs struggle with highly divergent and dispersed patches, revealing a gap in automated repair.


<details>
  <summary>Details</summary>
Motivation: Multi-hunk bugs are common but underrepresented in automated repair. Existing techniques focus on single-hunk fixes, ignoring the complexity of coordinating changes across code.

Method: Characterized HUNK4J (372 real-world defects), proposed hunk divergence (lexical, structural, file-level differences) and spatial proximity (patch distribution). Tested six LLMs.

Result: LLM success rates decline with higher divergence and spatial dispersion. No model succeeded in the most dispersed Fragment class.

Conclusion: Highlights a gap in LLM capabilities for multi-hunk repairs and calls for divergence-aware strategies.

Abstract: Multi-hunk bugs, where fixes span disjoint regions of code, are common in
practice, yet remain underrepresented in automated repair. Existing techniques
and benchmarks pre-dominantly target single-hunk scenarios, overlooking the
added complexity of coordinating semantically related changes across the
codebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches
derived from 372 real-world defects. We propose hunk divergence, a metric that
quantifies the variation among edits in a patch by capturing lexical,
structural, and file-level differences, while incorporating the number of hunks
involved. We further define spatial proximity, a classification that models how
hunks are spatially distributed across the program hierarchy. Our empirical
study spanning six LLMs reveals that model success rates decline with increased
divergence and spatial dispersion. Notably, when using the LLM alone, no model
succeeds in the most dispersed Fragment class. These findings highlight a
critical gap in LLM capabilities and motivate divergence-aware repair
strategies.

</details>


### [2] [On the Practices of Autonomous Systems Development: Survey-based Empirical Findings](https://arxiv.org/abs/2506.04438)
*Katerina Goseva-Popstojanova,Denny Hood,Johann Schumann,Noble Nkwocha*

Main category: cs.SE

TL;DR: A study on the state-of-the-practice in developing autonomous systems, identifying challenges, benefits, processes, and V&V practices, based on a 2019 survey of experts.


<details>
  <summary>Details</summary>
Motivation: Lack of information on autonomous systems development practices due to dynamic applications and proprietary constraints.

Method: Anonymous online survey in 2019 with experts in autonomous systems and/or MBSwE.

Result: Data on autonomous systems' development practices, challenges, benefits, and V&V methods.

Conclusion: Ongoing study aims to track evolution of autonomous systems development practices over time.

Abstract: Autonomous systems have gained an important role in many industry domains and
are beginning to change everyday life. However, due to dynamically emerging
applications and often proprietary constraints, there is a lack of information
about the practice of developing autonomous systems. This paper presents the
first part of the longitudinal study focused on establishing
state-of-the-practice, identifying and quantifying the challenges and benefits,
identifying the processes and standards used, and exploring verification and
validation (V&V) practices used for the development of autonomous systems. The
results presented in this paper are based on data about software systems that
have autonomous functionality and may employ model-based software engineering
(MBSwE) and reuse. These data were collected using an anonymous online survey
that was administered in 2019 and were provided by experts with experience in
development of autonomous systems and /or the use of MBSwE. Our current work is
focused on repeating the survey to collect more recent data and discover how
the development of autonomous systems has evolved over time.

</details>


### [3] [Leveraging Reward Models for Guiding Code Review Comment Generation](https://arxiv.org/abs/2506.04464)
*Oussama Ben Sghaier,Rosalia Tufano,Gabriele Bavota,Houari Sahraoui*

Main category: cs.SE

TL;DR: CoRAL is a DL framework using reinforcement learning to automate code review comment generation, focusing on semantic similarity and usefulness for code refinement.


<details>
  <summary>Details</summary>
Motivation: Code review is time-consuming and subjective; automating it with DL can improve efficiency and consistency.

Method: CoRAL uses reinforcement learning with a reward mechanism based on comment semantics and usefulness for code refinement.

Result: CoRAL outperforms baseline techniques in generating meaningful and useful review comments.

Conclusion: CoRAL effectively automates code review comment generation, enhancing the review process.

Abstract: Code review is a crucial component of modern software development, involving
the evaluation of code quality, providing feedback on potential issues, and
refining the code to address identified problems. Despite these benefits, code
review can be rather time consuming, and influenced by subjectivity and human
factors. For these reasons, techniques to (partially) automate the code review
process have been proposed in the literature. Among those, the ones exploiting
deep learning (DL) are able to tackle the generative aspect of code review, by
commenting on a given code as a human reviewer would do (i.e., comment
generation task) or by automatically implementing code changes required to
address a reviewer's comment (i.e., code refinement task). In this paper, we
introduce CoRAL, a deep learning framework automating review comment generation
by exploiting reinforcement learning with a reward mechanism considering both
the semantics of the generated comments as well as their usefulness as input
for other models automating the code refinement task. The core idea is that if
the DL model generates comments that are semantically similar to the expected
ones or can be successfully implemented by a second model specialized in code
refinement, these comments are likely to be meaningful and useful, thus
deserving a high reward in the reinforcement learning framework. We present
both quantitative and qualitative comparisons between the comments generated by
CoRAL and those produced by the latest baseline techniques, highlighting the
effectiveness and superiority of our approach.

</details>


### [4] [BINGO! Simple Optimizers Win Big if Problems Collapse to a Few Buckets](https://arxiv.org/abs/2506.04509)
*Kishan Kumar Ganguly,Tim Menzies*

Main category: cs.SE

TL;DR: The paper introduces the BINGO effect, where SE data collapses into few solution 'buckets,' enabling 10,000x faster optimization with simple methods like LITE and LINE, challenging the need for complex, CPU-heavy approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-objective optimization in SE is slow and complex. The BINGO effect reveals that SE data occupies a small fraction of possible solutions, simplifying optimization.

Method: The authors analyze 39 SE problems to demonstrate the BINGO effect. They develop new algorithms (LITE and LINE) that use simple stochastic selection, outperforming complex optimizers like DEHB.

Result: The methods achieve 10,000x faster optimization with comparable effectiveness to state-of-the-art techniques.

Conclusion: The BINGO effect explains why simple methods work well in SE and challenges the necessity of resource-intensive optimization. Data and code are publicly available.

Abstract: Traditional multi-objective optimization in software engineering (SE) can be
slow and complex. This paper introduces the BINGO effect: a novel phenomenon
where SE data surprisingly collapses into a tiny fraction of possible solution
"buckets" (e.g., only 100 used from 4,096 expected).
  We show the BINGO effect's prevalence across 39 optimization in SE problems.
Exploiting this, we optimize 10,000 times faster than state-of-the-art methods,
with comparable effectiveness. Our new algorithms (LITE and LINE), demonstrate
that simple stochastic selection can match complex optimizers like DEHB. This
work explains why simple methods succeed in SE-real data occupies a small
corner of possibilities-and guides when to apply them, challenging the need for
CPU-heavy optimization.
  Our data and code are public at GitHub (see anon-artifacts/bingo).

</details>


### [5] [KPIRoot+: An Efficient Integrated Framework for Anomaly Detection and Root Cause Analysis in Large-Scale Cloud Systems](https://arxiv.org/abs/2506.04569)
*Wenwei Gu,Renyi Zhong,Guangba Yu,Xinying Sun,Jinyang Liu,Yintong Huo,Zhuangbin Chen,Jianping Zhang,Jiazhen Gu,Yongqiang Yang,Michael R. Lyu*

Main category: cs.SE

TL;DR: KPIRoot+ improves root cause localization in cloud systems by combining similarity and causality analysis, outperforming baselines and reducing time costs.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for root cause localization in cloud systems are ineffective in complex environments, and deep learning approaches face computational and interpretability challenges.

Method: KPIRoot+ combines similarity and causality analysis, using symbolic aggregate approximation for efficient KPI representation, and addresses limitations of its predecessor.

Result: KPIRoot+ outperforms eight baselines by 2.9% to 35.7% and reduces time cost by 34.7%.

Conclusion: KPIRoot+ is an efficient solution for root cause localization in cloud systems, validated in a large-scale production environment.

Abstract: To ensure the reliability of cloud systems, their performance is monitored
using KPIs (key performance indicators). When issues arise, root cause
localization identifies KPIs responsible for service degradation, aiding in
quick diagnosis and resolution. Traditional methods rely on similarity
calculations, which can be ineffective in complex, interdependent cloud
environments. While deep learning-based approaches model these dependencies
better, they often face challenges such as high computational demands and lack
of interpretability.
  To address these issues, KPIRoot is proposed as an efficient method combining
similarity and causality analysis. It uses symbolic aggregate approximation for
compact KPI representation, improving analysis efficiency. However, deployment
in Cloud H revealed two drawbacks: 1) threshold-based anomaly detection misses
some performance anomalies, and 2) SAX representation fails to capture
intricate variation trends. KPIRoot+ addresses these limitations, outperforming
eight state-of-the-art baselines by 2.9% to 35.7%, while reducing time cost by
34.7%. We also share our experience deploying KPIRoot in a large-scale cloud
provider's production environment.

</details>


### [6] [QuanUML: Towards A Modeling Language for Model-Driven Quantum Software Development](https://arxiv.org/abs/2506.04639)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: QuanUML extends UML for quantum software, integrating quantum constructs like qubits and gates. Demonstrated with quantum algorithms, it aids in model-driven development and design.


<details>
  <summary>Details</summary>
Motivation: To address the lack of modeling tools for quantum software systems by extending UML with quantum-specific constructs.

Method: Extends UML with quantum elements (qubits, gates) and applies it to quantum algorithms like Efficient Long-Range Entanglement and Shor's Algorithm.

Result: QuanUML successfully models quantum and hybrid systems, offering a structured framework for quantum software design.

Conclusion: QuanUML is a practical tool for quantum software development, with potential for future enhancements.

Abstract: This paper introduces QuanUML, an extension of the Unified Modeling Language
(UML) tailored for quantum software systems. QuanUML integrates
quantum-specific constructs, such as qubits and quantum gates, into the UML
framework, enabling the modeling of both quantum and hybrid quantum-classical
systems. We apply QuanUML to Efficient Long-Range Entanglement using Dynamic
Circuits and Shor's Algorithm, demonstrating its utility in designing and
visualizing quantum algorithms. Our approach supports model-driven development
of quantum software and offers a structured framework for quantum software
design. We also highlight its advantages over existing methods and discuss
future improvements.

</details>


### [7] [From Developer Pairs to AI Copilots: A Comparative Study on Knowledge Transfer](https://arxiv.org/abs/2506.04785)
*Alisa Welter,Niklas Schneider,Tobias Dick,Kallistos Weis,Christof Tinnes,Marvin Wyrich,Sven Apel*

Main category: cs.SE

TL;DR: The paper examines knowledge transfer in human-human and human-AI pair programming, finding similar success rates but noting developers scrutinize AI suggestions less than human ones.


<details>
  <summary>Details</summary>
Motivation: To understand how knowledge transfer differs between human-human and human-AI pair programming, given the rise of AI coding assistants like GitHub Copilot.

Method: An empirical study comparing developer pairs without AI support to individuals using GitHub Copilot, using an extended knowledge transfer framework and semi-automated evaluation.

Result: Similar frequency of successful knowledge transfer episodes in both settings, but developers scrutinize AI suggestions less and Copilot can remind developers of overlooked details.

Conclusion: AI coding assistants like GitHub Copilot can facilitate knowledge transfer but require careful scrutiny to match human pair programming effectiveness.

Abstract: Knowledge transfer is fundamental to human collaboration and is therefore
common in software engineering. Pair programming is a prominent instance. With
the rise of AI coding assistants, developers now not only work with human
partners but also, as some claim, with AI pair programmers. Although studies
confirm knowledge transfer during human pair programming, its effectiveness
with AI coding assistants remains uncertain. To analyze knowledge transfer in
both human-human and human-AI settings, we conducted an empirical study where
developer pairs solved a programming task without AI support, while a separate
group of individual developers completed the same task using the AI coding
assistant GitHub Copilot. We extended an existing knowledge transfer framework
and employed a semi-automated evaluation pipeline to assess differences in
knowledge transfer episodes across both settings. We found a similar frequency
of successful knowledge transfer episodes and overlapping topical categories
across both settings. Two of our key findings are that developers tend to
accept GitHub Copilot's suggestions with less scrutiny than those from human
pair programming partners, but also that GitHub Copilot can subtly remind
developers of important code details they might otherwise overlook.

</details>


### [8] [A Multi-Dataset Evaluation of Models for Automated Vulnerability Repair](https://arxiv.org/abs/2506.04987)
*Zanis Ali Khan,Aayush Garg,Qiang Tang*

Main category: cs.SE

TL;DR: The study evaluates CodeBERT and CodeT5 for automated vulnerability patching, finding CodeBERT better with sparse context and CodeT5 superior in handling complex patterns and scalability. Fine-tuning improves in-distribution performance but struggles with generalization.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored area of vulnerability patching in Automated Program Repair (APR) and assess the effectiveness of pre-trained language models like CodeBERT and CodeT5.

Method: Evaluated CodeBERT and CodeT5 across six datasets and four languages, testing accuracy and generalization to unknown vulnerabilities, including fine-tuned models on in-distribution and out-of-distribution data.

Result: CodeBERT performs better in fragmented/sparse contexts, while CodeT5 excels in complex patterns and scalability. Fine-tuning improves in-distribution performance but models struggle with generalization.

Conclusion: The study benchmarks model performance, identifies generalization challenges, and provides insights for advancing automated vulnerability patching in real-world security applications.

Abstract: Software vulnerabilities pose significant security threats, requiring
effective mitigation. While Automated Program Repair (APR) has advanced in
fixing general bugs, vulnerability patching, a security-critical aspect of APR
remains underexplored. This study investigates pre-trained language models,
CodeBERT and CodeT5, for automated vulnerability patching across six datasets
and four languages. We evaluate their accuracy and generalization to unknown
vulnerabilities. Results show that while both models face challenges with
fragmented or sparse context, CodeBERT performs comparatively better in such
scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns.
CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned
models on both in-distribution (trained) and out-of-distribution (unseen)
datasets. While fine-tuning improves in-distribution performance, models
struggle to generalize to unseen data, highlighting challenges in robust
vulnerability detection. This study benchmarks model performance, identifies
limitations in generalization, and provides actionable insights to advance
automated vulnerability patching for real-world security applications.

</details>


### [9] [BacPrep: An Experimental Platform for Evaluating LLM-Based Bacalaureat Assessment](https://arxiv.org/abs/2506.04989)
*Dumitran Adrian Marius,Dita Radu*

Main category: cs.SE

TL;DR: BacPrep is an online platform using Google's Gemini 2.0 Flash LLM to provide automated feedback for Romanian Bacalaureat exam prep, addressing accessibility gaps.


<details>
  <summary>Details</summary>
Motivation: To tackle the lack of quality exam preparation and feedback for students in remote or underserved areas.

Method: Uses official exam questions and grading schemes with Gemini 2.0 Flash for automated feedback, collecting student solutions and LLM outputs for validation.

Result: Currently operational, focusing on data collection for expert validation to assess LLM feasibility and accuracy.

Conclusion: BacPrep aims to validate LLM use in exam feedback before reliable deployment, detailing design, ethics, and validation plans.

Abstract: Accessing quality preparation and feedback for the Romanian Bacalaureat exam
is challenging, particularly for students in remote or underserved areas. This
paper introduces BacPrep, an experimental online platform exploring Large
Language Model (LLM) potential for automated assessment, aiming to offer a
free, accessible resource. Using official exam questions from the last 5 years,
BacPrep employs one of Google's newest models, Gemini 2.0 Flash (released Feb
2025), guided by official grading schemes, to provide experimental feedback.
Currently operational, its primary research function is collecting student
solutions and LLM outputs. This focused dataset is vital for planned expert
validation to rigorously evaluate the feasibility and accuracy of this
cutting-edge LLM in the specific Bacalaureat context before reliable
deployment. We detail the design, data strategy, status, validation plan, and
ethics.

</details>


### [10] [Tech-ASan: Two-stage check for Address Sanitizer](https://arxiv.org/abs/2506.05022)
*Yixuan Cao,Yuhong Feng,Huafeng Li,Chongyi Huang,Fangcao Jian,Haoran Li,Xu Wang*

Main category: cs.SE

TL;DR: Tech-ASan reduces ASan's runtime overhead by 33.70% and 17.89% compared to ASan and ASan--, respectively, while maintaining detection capabilities.


<details>
  <summary>Details</summary>
Motivation: ASan's high runtime overhead limits its efficiency in large software testing, and existing methods either fail to eliminate redundant checks or compromise safety.

Method: Tech-ASan introduces a two-stage check algorithm using magic value comparison, an optimizer for redundant checks, and implementation on LLVM.

Result: Tech-ASan shows reduced overhead and detects fewer false negatives than ASan and ASan--.

Conclusion: Tech-ASan effectively balances performance and safety, making it a superior alternative for memory error detection.

Abstract: Address Sanitizer (ASan) is a sharp weapon for detecting memory safety
violations, including temporal and spatial errors hidden in C/C++ programs
during execution. However, ASan incurs significant runtime overhead, which
limits its efficiency in testing large software. The overhead mainly comes from
sanitizer checks due to the frequent and expensive shadow memory access. Over
the past decade, many methods have been developed to speed up ASan by
eliminating and accelerating sanitizer checks, however, they either fail to
adequately eliminate redundant checks or compromise detection capabilities. To
address this issue, this paper presents Tech-ASan, a two-stage check based
technique to accelerate ASan with safety assurance. First, we propose a novel
two-stage check algorithm for ASan, which leverages magic value comparison to
reduce most of the costly shadow memory accesses. Second, we design an
efficient optimizer to eliminate redundant checks, which integrates a novel
algorithm for removing checks in loops. Third, we implement Tech-ASan as a
memory safety tool based on the LLVM compiler infrastructure. Our evaluation
using the SPEC CPU2006 benchmark shows that Tech-ASan outperforms the
state-of-the-art methods with 33.70% and 17.89% less runtime overhead than ASan
and ASan--, respectively. Moreover, Tech-ASan detects 56 fewer false negative
cases than ASan and ASan-- when testing on the Juliet Test Suite under the same
redzone setting.

</details>


### [11] [LLM-Guided Scenario-based GUI Testing](https://arxiv.org/abs/2506.05079)
*Shengcheng Yu,Yuchen Ling,Chunrong Fang,Quan Zhou,Chunyang Chen,Shaomin Zhu,Zhenyu Chen*

Main category: cs.SE

TL;DR: ScenGen is an LLM-guided, multi-agent approach for generating scenario-based GUI tests, addressing gaps in automated testing by aligning with app business logic.


<details>
  <summary>Details</summary>
Motivation: Existing automated GUI testing lacks alignment with app business logic, missing critical functionalities. Manual testing's scenario-based approach inspired this work.

Method: ScenGen uses five agents (Observer, Decider, Executor, Supervisor, Recorder) to simulate manual testing, leveraging LLMs for GUI semantics and scenario completion.

Result: ScenGen effectively generates scenario-based GUI tests, ensuring traceability and alignment with testing scenarios.

Conclusion: ScenGen bridges the gap between automated testing and app business logic, improving test coverage and effectiveness.

Abstract: The assurance of mobile app GUI is more and more significant. Automated GUI
testing approaches of different strategies have been developed, while there are
still huge gaps between the approaches and the app business logic, not taking
the completion of specific testing scenarios as the exploration target, leading
to the exploration missing of critical app functionalities. Learning from the
manual testing, which takes testing scenarios with app business logic as the
basic granularity, in this paper, we utilize the LLMs to understand the
semantics presented in app GUI and how they are mapped in the testing context
based on specific testing scenarios. Then, scenario-based GUI tests are
generated with the guidance of multi-agent collaboration. Specifically, we
propose ScenGen, a novel LLM-guided scenario-based GUI testing approach
involving five agents to respectively take responsibilities of different phases
of the manual testing process. The Observer perceives the app GUI state by
extracting GUI widgets and forming GUI layouts, understanding the expressed
semantics. Then the app GUI info is sent to the Decider to make decisions on
target widgets based on the target testing scenarios. The decision-making
process takes the completion of specific testing scenarios as the exploration
target. The Executor then executes the demanding operations on the apps. The
execution results are checked by the Supervisor on whether the generated tests
are consistent with the completion target of the testing scenarios, ensuring
the traceability of the test generation and execution. Furthermore, the
corresponding GUI test operations are recorded to the context memory by
Recorder as an important basis for further decision-making, meanwhile
monitoring the runtime bug occurrences. ScenGen is evaluated and the results
show that ScenGen can effectively generate scenario-based GUI tests guided by
LLMs.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [12] [Tensor-based multivariate function approximation: methods benchmarking and comparison](https://arxiv.org/abs/2506.04791)
*Athanasios C. Antoulas,Ion Victor Gosea,Charles Poussot-Vassal,Pierre Vuillemin*

Main category: math.NA

TL;DR: The paper evaluates methods for tensor-based multivariate function approximation, comparing their performance and features using a diverse function collection.


<details>
  <summary>Details</summary>
Motivation: To fairly assess and guide users on the strengths and limitations of various tensor approximation tools, without ranking them.

Method: Construct tensors from diverse functions, evaluate method performances (accuracy, computational time, etc.), and compare methods using multiple criteria.

Result: Provides a benchmark collection for tensor approximation tools and detailed insights into the multivariate Loewner Framework (mLF).

Conclusion: The note aims to help users understand and choose appropriate tensor approximation methods, highlighting mLF's contributions.

Abstract: In this note, we evaluate the performances, the features and the
user-experience of some methods (and their implementations) designed for
tensor- (or data-) based multivariate function construction and approximation.
To this aim, a collection of multivariate functions extracted from contributive
works coming from different communities, is suggested. First, these functions
with varying complexity (e.g. number and degree of the variables) and nature
(e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to
construct tensors, each of different dimension and size on the disk. Second,
grounded on this tensor, we inspect performances of each considered method
(e.g. the accuracy, the computational time, the parameters tuning impact,
etc.). Finally, considering the "best" parameter tuning set, we compare each
method using multiple evaluation criteria. The purpose of this note is not to
rank the methods but rather to evaluate as fairly as possible the different
available strategies, with the idea in mind to guide users to understand the
process, the possibilities, the advantages and the limits brought by each
tools. The contribution claimed is to suggest a complete benchmark collection
of some available tools for tensor approximation by surrogate models (e.g.
rational functions, networks, etc.). In addition, as contributors of the
multivariate Loewner Framework (mLF) approach (and its side implementation in
MDSPACK), attention and details of the latter are more explicitly given, in
order to provide readers a digest of this contributive work and some details
with simple examples.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation](https://arxiv.org/abs/2506.04544)
*Charles Hong,Brendan Roberts,Huijae An,Alex Um,Advay Ratan,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: The paper introduces hdl2v, a dataset translating VHDL, Chisel, and PyMTL3 to Verilog to address the scarcity of human-written Verilog data, improving LLM Verilog generation performance by up to 23%.


<details>
  <summary>Details</summary>
Motivation: The lack of publicly available Verilog code compared to software languages like Python limits LLM performance in hardware code generation.

Method: The hdl2v dataset is created by translating VHDL, Chisel, and PyMTL3 to Verilog, and its impact is tested on a 32B-parameter LLM.

Result: hdl2v improves LLM Verilog generation by 23% (pass@10) and boosts a data augmentation-based approach by 63%.

Conclusion: hdl2v enhances LLM performance in Verilog generation, and future work can expand on dataset characteristics for further improvements.

Abstract: Large language models (LLMs) are playing an increasingly large role in
domains such as code generation, including hardware code generation, where
Verilog is the key language. However, the amount of publicly available Verilog
code pales in comparison to the amount of code available for software languages
like Python. In this work, we present hdl2v ("HDL-to-Verilog"), a dataset which
seeks to increase the amount of available human-written Verilog data by
translating or compiling three other hardware description languages - VHDL,
Chisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v
in enhancing LLM Verilog generation by improving performance of a 32
billion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,
without utilizing any data augmentation or knowledge distillation from larger
models. We also show hdl2v's ability to boost the performance of a data
augmentation-based fine-tuning approach by 63%. Finally, we characterize and
analyze our dataset to better understand which characteristics of
HDL-to-Verilog datasets can be expanded upon in future work for even better
performance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [14] [PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm Packages](https://arxiv.org/abs/2506.04962)
*Deniz Simsek,Aryaz Eghbali,Michael Pradel*

Main category: cs.CR

TL;DR: PoCGen autonomously generates PoC exploits for npm vulnerabilities using LLMs, static/dynamic analysis, achieving high success rates at low cost.


<details>
  <summary>Details</summary>
Motivation: Vulnerability reports often lack PoC exploits, hindering timely patching. Manual PoC creation is challenging due to incomplete reports and complex API interactions.

Method: PoCGen combines LLMs with static/dynamic analysis to understand reports, generate/validate PoC exploits.

Result: Success rates: 77% (SecBench.js) and 39% (new dataset), outperforming baselines by 45%. Cost: $0.02 per exploit.

Conclusion: PoCGen is effective, scalable, and cost-efficient for generating PoC exploits, aiding vulnerability remediation.

Abstract: Security vulnerabilities in software packages are a significant concern for
developers and users alike. Patching these vulnerabilities in a timely manner
is crucial to restoring the integrity and security of software systems.
However, previous work has shown that vulnerability reports often lack
proof-of-concept (PoC) exploits, which are essential for fixing the
vulnerability, testing patches, and avoiding regressions. Creating a PoC
exploit is challenging because vulnerability reports are informal and often
incomplete, and because it requires a detailed understanding of how inputs
passed to potentially vulnerable APIs may reach security-relevant sinks. In
this paper, we present PoCGen, a novel approach to autonomously generate and
validate PoC exploits for vulnerabilities in npm packages. This is the first
fully autonomous approach to use large language models (LLMs) in tandem with
static and dynamic analysis techniques for PoC exploit generation. PoCGen
leverages an LLM for understanding vulnerability reports, for generating
candidate PoC exploits, and for validating and refining them. Our approach
successfully generates exploits for 77% of the vulnerabilities in the
SecBench.js dataset and 39% in a new, more challenging dataset of 794 recent
vulnerabilities. This success rate significantly outperforms a recent baseline
(by 45 absolute percentage points), while imposing an average cost of $0.02 per
generated exploit.

</details>
