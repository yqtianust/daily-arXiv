<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [math.NA](#math.NA) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges](https://arxiv.org/abs/2506.04418)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: The paper introduces HUNK4J, a dataset for multi-hunk bug fixes, and metrics (hunk divergence and spatial proximity) to analyze patch complexity. It shows LLMs struggle with highly divergent and dispersed patches.


<details>
  <summary>Details</summary>
Motivation: Multi-hunk bugs are common but underrepresented in automated repair. Existing tools focus on single-hunk fixes, ignoring the complexity of coordinating changes across code.

Method: The authors characterize HUNK4J, a dataset of 372 real-world multi-hunk patches, and propose metrics (hunk divergence and spatial proximity) to quantify patch complexity. They evaluate six LLMs on these patches.

Result: LLM success rates decline with increased divergence and spatial dispersion. No model succeeds in the most dispersed Fragment class without assistance.

Conclusion: The findings reveal a gap in LLM capabilities for multi-hunk repairs, motivating the need for divergence-aware strategies.

Abstract: Multi-hunk bugs, where fixes span disjoint regions of code, are common in
practice, yet remain underrepresented in automated repair. Existing techniques
and benchmarks pre-dominantly target single-hunk scenarios, overlooking the
added complexity of coordinating semantically related changes across the
codebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches
derived from 372 real-world defects. We propose hunk divergence, a metric that
quantifies the variation among edits in a patch by capturing lexical,
structural, and file-level differences, while incorporating the number of hunks
involved. We further define spatial proximity, a classification that models how
hunks are spatially distributed across the program hierarchy. Our empirical
study spanning six LLMs reveals that model success rates decline with increased
divergence and spatial dispersion. Notably, when using the LLM alone, no model
succeeds in the most dispersed Fragment class. These findings highlight a
critical gap in LLM capabilities and motivate divergence-aware repair
strategies.

</details>


### [2] [On the Practices of Autonomous Systems Development: Survey-based Empirical Findings](https://arxiv.org/abs/2506.04438)
*Katerina Goseva-Popstojanova,Denny Hood,Johann Schumann,Noble Nkwocha*

Main category: cs.SE

TL;DR: A study on the development practices of autonomous systems, focusing on challenges, benefits, processes, and V&V methods, based on a 2019 survey.


<details>
  <summary>Details</summary>
Motivation: To address the lack of information about the practical development of autonomous systems due to emerging applications and proprietary constraints.

Method: Anonymous online survey in 2019, collecting data from experts in autonomous systems and MBSwE.

Result: Initial findings on state-of-the-practice, challenges, benefits, processes, and V&V methods in autonomous systems development.

Conclusion: The study aims to repeat the survey to track evolution in autonomous systems development practices over time.

Abstract: Autonomous systems have gained an important role in many industry domains and
are beginning to change everyday life. However, due to dynamically emerging
applications and often proprietary constraints, there is a lack of information
about the practice of developing autonomous systems. This paper presents the
first part of the longitudinal study focused on establishing
state-of-the-practice, identifying and quantifying the challenges and benefits,
identifying the processes and standards used, and exploring verification and
validation (V&V) practices used for the development of autonomous systems. The
results presented in this paper are based on data about software systems that
have autonomous functionality and may employ model-based software engineering
(MBSwE) and reuse. These data were collected using an anonymous online survey
that was administered in 2019 and were provided by experts with experience in
development of autonomous systems and /or the use of MBSwE. Our current work is
focused on repeating the survey to collect more recent data and discover how
the development of autonomous systems has evolved over time.

</details>


### [3] [Leveraging Reward Models for Guiding Code Review Comment Generation](https://arxiv.org/abs/2506.04464)
*Oussama Ben Sghaier,Rosalia Tufano,Gabriele Bavota,Houari Sahraoui*

Main category: cs.SE

TL;DR: CoRAL is a DL framework using reinforcement learning to automate code review comment generation, focusing on semantic similarity and usefulness for code refinement.


<details>
  <summary>Details</summary>
Motivation: Code review is time-consuming and subjective; automating it with DL can improve efficiency and consistency.

Method: CoRAL employs reinforcement learning with a reward mechanism based on comment semantics and usefulness for code refinement.

Result: CoRAL outperforms baseline techniques in generating meaningful and useful review comments.

Conclusion: CoRAL effectively automates code review comment generation, offering superior performance over existing methods.

Abstract: Code review is a crucial component of modern software development, involving
the evaluation of code quality, providing feedback on potential issues, and
refining the code to address identified problems. Despite these benefits, code
review can be rather time consuming, and influenced by subjectivity and human
factors. For these reasons, techniques to (partially) automate the code review
process have been proposed in the literature. Among those, the ones exploiting
deep learning (DL) are able to tackle the generative aspect of code review, by
commenting on a given code as a human reviewer would do (i.e., comment
generation task) or by automatically implementing code changes required to
address a reviewer's comment (i.e., code refinement task). In this paper, we
introduce CoRAL, a deep learning framework automating review comment generation
by exploiting reinforcement learning with a reward mechanism considering both
the semantics of the generated comments as well as their usefulness as input
for other models automating the code refinement task. The core idea is that if
the DL model generates comments that are semantically similar to the expected
ones or can be successfully implemented by a second model specialized in code
refinement, these comments are likely to be meaningful and useful, thus
deserving a high reward in the reinforcement learning framework. We present
both quantitative and qualitative comparisons between the comments generated by
CoRAL and those produced by the latest baseline techniques, highlighting the
effectiveness and superiority of our approach.

</details>


### [4] [BINGO! Simple Optimizers Win Big if Problems Collapse to a Few Buckets](https://arxiv.org/abs/2506.04509)
*Kishan Kumar Ganguly,Tim Menzies*

Main category: cs.SE

TL;DR: The paper introduces the BINGO effect, where SE data collapses into a small fraction of possible solutions, enabling 10,000x faster optimization with simple methods like LITE and LINE.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-objective optimization in SE is slow and complex. The BINGO effect reveals that SE data occupies a tiny fraction of possible solutions, simplifying optimization.

Method: The authors analyze 39 SE problems, demonstrating the BINGO effect. They develop algorithms (LITE and LINE) using simple stochastic selection, comparing them to complex optimizers like DEHB.

Result: The new methods optimize 10,000 times faster than state-of-the-art techniques while maintaining comparable effectiveness.

Conclusion: The BINGO effect explains why simple methods work in SE and challenges the need for resource-heavy optimization, guiding when to apply simpler approaches.

Abstract: Traditional multi-objective optimization in software engineering (SE) can be
slow and complex. This paper introduces the BINGO effect: a novel phenomenon
where SE data surprisingly collapses into a tiny fraction of possible solution
"buckets" (e.g., only 100 used from 4,096 expected).
  We show the BINGO effect's prevalence across 39 optimization in SE problems.
Exploiting this, we optimize 10,000 times faster than state-of-the-art methods,
with comparable effectiveness. Our new algorithms (LITE and LINE), demonstrate
that simple stochastic selection can match complex optimizers like DEHB. This
work explains why simple methods succeed in SE-real data occupies a small
corner of possibilities-and guides when to apply them, challenging the need for
CPU-heavy optimization.
  Our data and code are public at GitHub (see anon-artifacts/bingo).

</details>


### [5] [KPIRoot+: An Efficient Integrated Framework for Anomaly Detection and Root Cause Analysis in Large-Scale Cloud Systems](https://arxiv.org/abs/2506.04569)
*Wenwei Gu,Renyi Zhong,Guangba Yu,Xinying Sun,Jinyang Liu,Yintong Huo,Zhuangbin Chen,Jianping Zhang,Jiazhen Gu,Yongqiang Yang,Michael R. Lyu*

Main category: cs.SE

TL;DR: KPIRoot+ improves root cause localization in cloud systems by combining similarity and causality analysis, addressing limitations of traditional and deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods and deep learning approaches for root cause localization in cloud systems have inefficiencies and interpretability issues.

Method: KPIRoot+ uses symbolic aggregate approximation for compact KPI representation and combines similarity and causality analysis.

Result: KPIRoot+ outperforms baselines by 2.9% to 35.7% and reduces time cost by 34.7%.

Conclusion: KPIRoot+ is effective and efficient for root cause localization in large-scale cloud environments.

Abstract: To ensure the reliability of cloud systems, their performance is monitored
using KPIs (key performance indicators). When issues arise, root cause
localization identifies KPIs responsible for service degradation, aiding in
quick diagnosis and resolution. Traditional methods rely on similarity
calculations, which can be ineffective in complex, interdependent cloud
environments. While deep learning-based approaches model these dependencies
better, they often face challenges such as high computational demands and lack
of interpretability.
  To address these issues, KPIRoot is proposed as an efficient method combining
similarity and causality analysis. It uses symbolic aggregate approximation for
compact KPI representation, improving analysis efficiency. However, deployment
in Cloud H revealed two drawbacks: 1) threshold-based anomaly detection misses
some performance anomalies, and 2) SAX representation fails to capture
intricate variation trends. KPIRoot+ addresses these limitations, outperforming
eight state-of-the-art baselines by 2.9% to 35.7%, while reducing time cost by
34.7%. We also share our experience deploying KPIRoot in a large-scale cloud
provider's production environment.

</details>


### [6] [QuanUML: Towards A Modeling Language for Model-Driven Quantum Software Development](https://arxiv.org/abs/2506.04639)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: QuanUML extends UML for quantum software, integrating quantum constructs like qubits and gates, and is applied to quantum algorithms like Shor's.


<details>
  <summary>Details</summary>
Motivation: To provide a structured framework for modeling quantum and hybrid quantum-classical systems, addressing gaps in existing UML for quantum software.

Method: Extends UML with quantum-specific constructs (qubits, gates) and applies it to quantum algorithms (e.g., Shor's) and hybrid systems.

Result: Demonstrates QuanUML's utility in designing and visualizing quantum algorithms, supporting model-driven quantum software development.

Conclusion: QuanUML offers advantages over existing methods and provides a foundation for future improvements in quantum software design.

Abstract: This paper introduces QuanUML, an extension of the Unified Modeling Language
(UML) tailored for quantum software systems. QuanUML integrates
quantum-specific constructs, such as qubits and quantum gates, into the UML
framework, enabling the modeling of both quantum and hybrid quantum-classical
systems. We apply QuanUML to Efficient Long-Range Entanglement using Dynamic
Circuits and Shor's Algorithm, demonstrating its utility in designing and
visualizing quantum algorithms. Our approach supports model-driven development
of quantum software and offers a structured framework for quantum software
design. We also highlight its advantages over existing methods and discuss
future improvements.

</details>


### [7] [From Developer Pairs to AI Copilots: A Comparative Study on Knowledge Transfer](https://arxiv.org/abs/2506.04785)
*Alisa Welter,Niklas Schneider,Tobias Dick,Kallistos Weis,Christof Tinnes,Marvin Wyrich,Sven Apel*

Main category: cs.SE

TL;DR: The paper examines knowledge transfer in human-human and human-AI pair programming, finding similar frequencies of successful transfer but noting developers scrutinize AI suggestions less than human ones.


<details>
  <summary>Details</summary>
Motivation: To understand the effectiveness of knowledge transfer in human-AI pair programming compared to human-human pair programming, given the rise of AI coding assistants like GitHub Copilot.

Method: An empirical study where developer pairs solved tasks without AI, while individuals used GitHub Copilot. Extended a knowledge transfer framework and used a semi-automated evaluation pipeline.

Result: Similar frequency of successful knowledge transfer in both settings, but developers accepted AI suggestions with less scrutiny. AI also helped remind developers of overlooked code details.

Conclusion: Human-AI pair programming shows comparable knowledge transfer to human-human, but developers' trust in AI suggestions may require careful consideration.

Abstract: Knowledge transfer is fundamental to human collaboration and is therefore
common in software engineering. Pair programming is a prominent instance. With
the rise of AI coding assistants, developers now not only work with human
partners but also, as some claim, with AI pair programmers. Although studies
confirm knowledge transfer during human pair programming, its effectiveness
with AI coding assistants remains uncertain. To analyze knowledge transfer in
both human-human and human-AI settings, we conducted an empirical study where
developer pairs solved a programming task without AI support, while a separate
group of individual developers completed the same task using the AI coding
assistant GitHub Copilot. We extended an existing knowledge transfer framework
and employed a semi-automated evaluation pipeline to assess differences in
knowledge transfer episodes across both settings. We found a similar frequency
of successful knowledge transfer episodes and overlapping topical categories
across both settings. Two of our key findings are that developers tend to
accept GitHub Copilot's suggestions with less scrutiny than those from human
pair programming partners, but also that GitHub Copilot can subtly remind
developers of important code details they might otherwise overlook.

</details>


### [8] [A Multi-Dataset Evaluation of Models for Automated Vulnerability Repair](https://arxiv.org/abs/2506.04987)
*Zanis Ali Khan,Aayush Garg,Qiang Tang*

Main category: cs.SE

TL;DR: The study evaluates CodeBERT and CodeT5 for automated vulnerability patching, finding CodeBERT better with fragmented context and CodeT5 superior in handling complex patterns and scalability. Fine-tuning improves performance on trained data but struggles with unseen vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Software vulnerabilities are a major security threat, but automated patching (a critical aspect of APR) is underexplored. The study aims to assess pre-trained models for this task.

Method: The study tests CodeBERT and CodeT5 on six datasets across four languages, evaluating accuracy and generalization to unknown vulnerabilities. Fine-tuning is also examined for in-distribution and out-of-distribution data.

Result: CodeBERT performs better with fragmented context, while CodeT5 excels in complex patterns and scalability. Fine-tuning improves in-distribution performance but fails to generalize to unseen data.

Conclusion: The study benchmarks model performance, identifies generalization challenges, and offers insights to improve automated vulnerability patching for real-world security.

Abstract: Software vulnerabilities pose significant security threats, requiring
effective mitigation. While Automated Program Repair (APR) has advanced in
fixing general bugs, vulnerability patching, a security-critical aspect of APR
remains underexplored. This study investigates pre-trained language models,
CodeBERT and CodeT5, for automated vulnerability patching across six datasets
and four languages. We evaluate their accuracy and generalization to unknown
vulnerabilities. Results show that while both models face challenges with
fragmented or sparse context, CodeBERT performs comparatively better in such
scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns.
CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned
models on both in-distribution (trained) and out-of-distribution (unseen)
datasets. While fine-tuning improves in-distribution performance, models
struggle to generalize to unseen data, highlighting challenges in robust
vulnerability detection. This study benchmarks model performance, identifies
limitations in generalization, and provides actionable insights to advance
automated vulnerability patching for real-world security applications.

</details>


### [9] [BacPrep: An Experimental Platform for Evaluating LLM-Based Bacalaureat Assessment](https://arxiv.org/abs/2506.04989)
*Dumitran Adrian Marius,Dita Radu*

Main category: cs.SE

TL;DR: BacPrep is an online platform using Google's Gemini 2.0 Flash LLM to provide automated feedback for the Romanian Bacalaureat exam, targeting underserved students.


<details>
  <summary>Details</summary>
Motivation: To address the lack of accessible exam preparation resources for students in remote or underserved areas.

Method: Uses official exam questions and grading schemes with Gemini 2.0 Flash for automated feedback, collecting student solutions and LLM outputs for validation.

Result: Currently operational, focusing on data collection for expert validation to assess LLM feasibility and accuracy.

Conclusion: BacPrep aims to validate LLM-based automated assessment for reliable deployment in the Bacalaureat context.

Abstract: Accessing quality preparation and feedback for the Romanian Bacalaureat exam
is challenging, particularly for students in remote or underserved areas. This
paper introduces BacPrep, an experimental online platform exploring Large
Language Model (LLM) potential for automated assessment, aiming to offer a
free, accessible resource. Using official exam questions from the last 5 years,
BacPrep employs one of Google's newest models, Gemini 2.0 Flash (released Feb
2025), guided by official grading schemes, to provide experimental feedback.
Currently operational, its primary research function is collecting student
solutions and LLM outputs. This focused dataset is vital for planned expert
validation to rigorously evaluate the feasibility and accuracy of this
cutting-edge LLM in the specific Bacalaureat context before reliable
deployment. We detail the design, data strategy, status, validation plan, and
ethics.

</details>


### [10] [Tech-ASan: Two-stage check for Address Sanitizer](https://arxiv.org/abs/2506.05022)
*Yixuan Cao,Yuhong Feng,Huafeng Li,Chongyi Huang,Fangcao Jian,Haoran Li,Xu Wang*

Main category: cs.SE

TL;DR: Tech-ASan reduces ASan's runtime overhead by 33.70% and 17.89% compared to ASan and ASan--, respectively, while maintaining detection capabilities.


<details>
  <summary>Details</summary>
Motivation: ASan's high runtime overhead limits its efficiency for large software due to frequent shadow memory access. Existing methods either fail to eliminate redundant checks or compromise detection.

Method: Tech-ASan uses a two-stage check algorithm (magic value comparison), an optimizer for redundant checks, and is implemented in LLVM.

Result: Tech-ASan reduces overhead significantly and detects fewer false negatives (56 fewer cases) than ASan and ASan--.

Conclusion: Tech-ASan effectively accelerates ASan with safety assurance, outperforming state-of-the-art methods.

Abstract: Address Sanitizer (ASan) is a sharp weapon for detecting memory safety
violations, including temporal and spatial errors hidden in C/C++ programs
during execution. However, ASan incurs significant runtime overhead, which
limits its efficiency in testing large software. The overhead mainly comes from
sanitizer checks due to the frequent and expensive shadow memory access. Over
the past decade, many methods have been developed to speed up ASan by
eliminating and accelerating sanitizer checks, however, they either fail to
adequately eliminate redundant checks or compromise detection capabilities. To
address this issue, this paper presents Tech-ASan, a two-stage check based
technique to accelerate ASan with safety assurance. First, we propose a novel
two-stage check algorithm for ASan, which leverages magic value comparison to
reduce most of the costly shadow memory accesses. Second, we design an
efficient optimizer to eliminate redundant checks, which integrates a novel
algorithm for removing checks in loops. Third, we implement Tech-ASan as a
memory safety tool based on the LLVM compiler infrastructure. Our evaluation
using the SPEC CPU2006 benchmark shows that Tech-ASan outperforms the
state-of-the-art methods with 33.70% and 17.89% less runtime overhead than ASan
and ASan--, respectively. Moreover, Tech-ASan detects 56 fewer false negative
cases than ASan and ASan-- when testing on the Juliet Test Suite under the same
redzone setting.

</details>


### [11] [LLM-Guided Scenario-based GUI Testing](https://arxiv.org/abs/2506.05079)
*Shengcheng Yu,Yuchen Ling,Chunrong Fang,Quan Zhou,Chunyang Chen,Shaomin Zhu,Zhenyu Chen*

Main category: cs.SE

TL;DR: ScenGen is an LLM-guided, multi-agent approach for scenario-based GUI testing of mobile apps, addressing gaps in automated testing by aligning with app business logic.


<details>
  <summary>Details</summary>
Motivation: Automated GUI testing often misses critical functionalities due to misalignment with app business logic. Manual testing, which uses testing scenarios as granularity, inspired this work.

Method: ScenGen employs five agents (Observer, Decider, Executor, Supervisor, Recorder) to simulate manual testing phases, using LLMs to understand GUI semantics and generate tests.

Result: ScenGen effectively generates scenario-based GUI tests, ensuring traceability and alignment with testing scenarios.

Conclusion: ScenGen bridges the gap between automated testing and app business logic, leveraging LLMs and multi-agent collaboration for effective GUI testing.

Abstract: The assurance of mobile app GUI is more and more significant. Automated GUI
testing approaches of different strategies have been developed, while there are
still huge gaps between the approaches and the app business logic, not taking
the completion of specific testing scenarios as the exploration target, leading
to the exploration missing of critical app functionalities. Learning from the
manual testing, which takes testing scenarios with app business logic as the
basic granularity, in this paper, we utilize the LLMs to understand the
semantics presented in app GUI and how they are mapped in the testing context
based on specific testing scenarios. Then, scenario-based GUI tests are
generated with the guidance of multi-agent collaboration. Specifically, we
propose ScenGen, a novel LLM-guided scenario-based GUI testing approach
involving five agents to respectively take responsibilities of different phases
of the manual testing process. The Observer perceives the app GUI state by
extracting GUI widgets and forming GUI layouts, understanding the expressed
semantics. Then the app GUI info is sent to the Decider to make decisions on
target widgets based on the target testing scenarios. The decision-making
process takes the completion of specific testing scenarios as the exploration
target. The Executor then executes the demanding operations on the apps. The
execution results are checked by the Supervisor on whether the generated tests
are consistent with the completion target of the testing scenarios, ensuring
the traceability of the test generation and execution. Furthermore, the
corresponding GUI test operations are recorded to the context memory by
Recorder as an important basis for further decision-making, meanwhile
monitoring the runtime bug occurrences. ScenGen is evaluated and the results
show that ScenGen can effectively generate scenario-based GUI tests guided by
LLMs.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [12] [Tensor-based multivariate function approximation: methods benchmarking and comparison](https://arxiv.org/abs/2506.04791)
*Athanasios C. Antoulas,Ion Victor Gosea,Charles Poussot-Vassal,Pierre Vuillemin*

Main category: math.NA

TL;DR: The paper evaluates tensor-based multivariate function approximation methods using a diverse function collection, comparing performance and guiding users on tool selection.


<details>
  <summary>Details</summary>
Motivation: To fairly assess and compare various tensor approximation methods, providing insights into their strengths, limitations, and practical use.

Method: Construct tensors from diverse functions, evaluate method performance (accuracy, computational time, tuning impact), and compare using multiple criteria.

Result: A benchmark collection for tensor approximation tools is provided, with detailed insights into the multivariate Loewner Framework (mLF).

Conclusion: The paper offers a comprehensive evaluation of tensor approximation methods, aiding users in understanding and selecting appropriate tools.

Abstract: In this note, we evaluate the performances, the features and the
user-experience of some methods (and their implementations) designed for
tensor- (or data-) based multivariate function construction and approximation.
To this aim, a collection of multivariate functions extracted from contributive
works coming from different communities, is suggested. First, these functions
with varying complexity (e.g. number and degree of the variables) and nature
(e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to
construct tensors, each of different dimension and size on the disk. Second,
grounded on this tensor, we inspect performances of each considered method
(e.g. the accuracy, the computational time, the parameters tuning impact,
etc.). Finally, considering the "best" parameter tuning set, we compare each
method using multiple evaluation criteria. The purpose of this note is not to
rank the methods but rather to evaluate as fairly as possible the different
available strategies, with the idea in mind to guide users to understand the
process, the possibilities, the advantages and the limits brought by each
tools. The contribution claimed is to suggest a complete benchmark collection
of some available tools for tensor approximation by surrogate models (e.g.
rational functions, networks, etc.). In addition, as contributors of the
multivariate Loewner Framework (mLF) approach (and its side implementation in
MDSPACK), attention and details of the latter are more explicitly given, in
order to provide readers a digest of this contributive work and some details
with simple examples.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation](https://arxiv.org/abs/2506.04544)
*Charles Hong,Brendan Roberts,Huijae An,Alex Um,Advay Ratan,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: The paper introduces hdl2v, a dataset translating VHDL, Chisel, and PyMTL3 to Verilog to address the scarcity of human-written Verilog data. It shows significant improvements in LLM Verilog generation performance.


<details>
  <summary>Details</summary>
Motivation: The lack of publicly available Verilog code compared to software languages like Python motivates the creation of hdl2v to enhance LLM capabilities in hardware code generation.

Method: hdl2v translates VHDL, Chisel, and PyMTL3 to Verilog, creating a dataset to fine-tune LLMs. Performance is evaluated using VerilogEvalV2.

Result: hdl2v improves a 32B-parameter model's performance by 23% (pass@10) and boosts a data augmentation-based approach by 63%.

Conclusion: hdl2v effectively addresses Verilog data scarcity and enhances LLM performance, with potential for further dataset expansion.

Abstract: Large language models (LLMs) are playing an increasingly large role in
domains such as code generation, including hardware code generation, where
Verilog is the key language. However, the amount of publicly available Verilog
code pales in comparison to the amount of code available for software languages
like Python. In this work, we present hdl2v ("HDL-to-Verilog"), a dataset which
seeks to increase the amount of available human-written Verilog data by
translating or compiling three other hardware description languages - VHDL,
Chisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v
in enhancing LLM Verilog generation by improving performance of a 32
billion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,
without utilizing any data augmentation or knowledge distillation from larger
models. We also show hdl2v's ability to boost the performance of a data
augmentation-based fine-tuning approach by 63%. Finally, we characterize and
analyze our dataset to better understand which characteristics of
HDL-to-Verilog datasets can be expanded upon in future work for even better
performance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [14] [PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm Packages](https://arxiv.org/abs/2506.04962)
*Deniz Simsek,Aryaz Eghbali,Michael Pradel*

Main category: cs.CR

TL;DR: PoCGen autonomously generates and validates PoC exploits for npm package vulnerabilities using LLMs, static/dynamic analysis, achieving high success rates at low cost.


<details>
  <summary>Details</summary>
Motivation: Vulnerability reports often lack PoC exploits, hindering timely patching and testing. Existing methods struggle due to incomplete reports and complex API interactions.

Method: PoCGen combines LLMs with static/dynamic analysis to understand reports, generate/validate PoC exploits.

Result: Success rates: 77% (SecBench.js) and 39% (new dataset), outperforming baselines by 45 percentage points at $0.02 per exploit.

Conclusion: PoCGen is effective, scalable, and cost-efficient for generating PoC exploits, aiding vulnerability remediation.

Abstract: Security vulnerabilities in software packages are a significant concern for
developers and users alike. Patching these vulnerabilities in a timely manner
is crucial to restoring the integrity and security of software systems.
However, previous work has shown that vulnerability reports often lack
proof-of-concept (PoC) exploits, which are essential for fixing the
vulnerability, testing patches, and avoiding regressions. Creating a PoC
exploit is challenging because vulnerability reports are informal and often
incomplete, and because it requires a detailed understanding of how inputs
passed to potentially vulnerable APIs may reach security-relevant sinks. In
this paper, we present PoCGen, a novel approach to autonomously generate and
validate PoC exploits for vulnerabilities in npm packages. This is the first
fully autonomous approach to use large language models (LLMs) in tandem with
static and dynamic analysis techniques for PoC exploit generation. PoCGen
leverages an LLM for understanding vulnerability reports, for generating
candidate PoC exploits, and for validating and refining them. Our approach
successfully generates exploits for 77% of the vulnerabilities in the
SecBench.js dataset and 39% in a new, more challenging dataset of 794 recent
vulnerabilities. This success rate significantly outperforms a recent baseline
(by 45 absolute percentage points), while imposing an average cost of $0.02 per
generated exploit.

</details>
