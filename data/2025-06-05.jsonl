{"id": "2506.03283", "pdf": "https://arxiv.org/pdf/2506.03283", "abs": "https://arxiv.org/abs/2506.03283", "authors": ["Viola Campos", "Ridwan Shariffdeen", "Adrian Ulges", "Yannic Noller"], "title": "Empirical Evaluation of Generalizable Automated Program Repair with Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Automated Program Repair (APR) proposes bug fixes to aid developers in\nmaintaining software. The state of the art in this domain focuses on using\nLLMs, leveraging their strong capabilities to comprehend specifications in\nnatural language and to generate program code. Recent works have shown that\nLLMs can be used to generate repairs. However, despite the APR community's\nresearch achievements and several industry deployments in the last decade, APR\nstill lacks the capabilities to generalize broadly. In this work, we present an\nintensive empirical evaluation of LLMs for generating patches. We evaluate a\ndiverse set of 13 recent models, including open ones (e.g., Llama 3.3, Qwen 2.5\nCoder, and DeepSeek R1 (dist.)) and closed ones (e.g., o3-mini, GPT-4o, Claude\n3.7 Sonnet, Gemini 2.0 Flash). In particular, we explore language-agnostic\nrepairs by utilizing benchmarks for Java (e.g., Defects4J), JavaScript (e.g.,\nBugsJS), Python (e.g., BugsInPy), and PHP (e.g., BugsPHP). Besides the\ngeneralization between different languages and levels of patch complexity, we\nalso investigate the effects of fault localization (FL) as a preprocessing step\nand compare the progress for open vs closed models. Our evaluation represents a\nsnapshot of the current repair capabilities of the latest LLMs. Key results\ninclude: (1) Different LLMs tend to perform best for different languages, which\nmakes it hard to develop cross-platform repair techniques with single LLMs. (2)\nThe combinations of models add value with respect to uniquely fixed bugs, so a\ncommittee of expert models should be considered. (3) Under realistic\nassumptions of imperfect FL, we observe significant drops in accuracy from the\nusual practice of using perfect FL. Our findings and insights will help both\nresearchers and practitioners develop reliable and generalizable APR techniques\nand evaluate them in realistic and fair environments."}
{"id": "2506.03396", "pdf": "https://arxiv.org/pdf/2506.03396", "abs": "https://arxiv.org/abs/2506.03396", "authors": ["Jinhan Kim", "Nargiz Humbatova", "Gunel Jahangirova", "Shin Yoo", "Paolo Tonella"], "title": "Fault Localisation and Repair for DL Systems: An Empirical Study with LLMs", "categories": ["cs.SE"], "comment": "arXiv admin note: text overlap with arXiv:2301.11568", "summary": "Numerous Fault Localisation (FL) and repair techniques have been proposed to\naddress faults in Deep Learning (DL) models. However, their effectiveness in\npractical applications remains uncertain due to the reliance on pre-defined\nrules. This paper presents a comprehensive evaluation of state-of-the-art FL\nand repair techniques, examining their advantages and limitations. Moreover, we\nintroduce a novel approach that harnesses the power of Large Language Models\n(LLMs) in localising and repairing DL faults. Our evaluation, conducted on a\ncarefully designed benchmark, reveals the strengths and weaknesses of current\nFL and repair techniques. We emphasise the importance of enhanced accuracy and\nthe need for more rigorous assessment methods that employ multiple ground truth\npatches. Notably, LLMs exhibit remarkable performance in both FL and repair\ntasks. For instance, the GPT-4 model achieves 44% and 82% improvements in FL\nand repair tasks respectively, compared to the second-best tool, demonstrating\nthe potential of LLMs in this domain. Our study sheds light on the current\nstate of FL and repair techniques and suggests that LLMs could be a promising\navenue for future advancements."}
{"id": "2506.03401", "pdf": "https://arxiv.org/pdf/2506.03401", "abs": "https://arxiv.org/abs/2506.03401", "authors": ["Xiwei Xu", "Hans Weytjens", "Dawen Zhang", "Qinghua Lu", "Ingo Weber", "Liming Zhu"], "title": "RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines", "categories": ["cs.SE"], "comment": null, "summary": "Recent studies show that 60% of LLM-based compound systems in enterprise\nenvironments leverage some form of retrieval-augmented generation (RAG), which\nenhances the relevance and accuracy of LLM (or other genAI) outputs by\nretrieving relevant information from external data sources. LLMOps involves the\npractices and techniques for managing the lifecycle and operations of LLM\ncompound systems in production environments. It supports enhancing LLM systems\nthrough continuous operations and feedback evaluation. RAGOps extends LLMOps by\nincorporating a strong focus on data management to address the continuous\nchanges in external data sources. This necessitates automated methods for\nevaluating and testing data operations, enhancing retrieval relevance and\ngeneration quality. In this paper, we (1) characterize the generic architecture\nof RAG applications based on the 4+1 model view for describing software\narchitectures, (2) outline the lifecycle of RAG systems, which integrates the\nmanagement lifecycles of both the LLM and the data, (3) define the key design\nconsiderations of RAGOps across different stages of the RAG lifecycle and\nquality trade-off analyses, (4) highlight the overarching research challenges\naround RAGOps, and (5) present two use cases of RAG applications and the\ncorresponding RAGOps considerations."}
{"id": "2506.03504", "pdf": "https://arxiv.org/pdf/2506.03504", "abs": "https://arxiv.org/abs/2506.03504", "authors": ["Zhuo Zhuo", "Xiangyu Zhang"], "title": "Beyond C/C++: Probabilistic and LLM Methods for Next-Generation Software Reverse Engineering", "categories": ["cs.SE"], "comment": null, "summary": "This proposal discusses the growing challenges in reverse engineering modern\nsoftware binaries, particularly those compiled from newer system programming\nlanguages such as Rust, Go, and Mojo. Traditional reverse engineering\ntechniques, developed with a focus on C and C++, fall short when applied to\nthese newer languages due to their reliance on outdated heuristics and failure\nto fully utilize the rich semantic information embedded in binary programs.\nThese challenges are exacerbated by the limitations of current data-driven\nmethods, which are susceptible to generating inaccurate results, commonly\nreferred to as hallucinations. To overcome these limitations, we propose a\nnovel approach that integrates probabilistic binary analysis with fine-tuned\nlarge language models (LLMs). Our method systematically models the\nuncertainties inherent in reverse engineering, enabling more accurate reasoning\nabout incomplete or ambiguous information. By incorporating LLMs, we extend the\nanalysis beyond traditional heuristics, allowing for more creative and\ncontext-aware inferences, particularly for binaries from diverse programming\nlanguages. This hybrid approach not only enhances the robustness and accuracy\nof reverse engineering efforts but also offers a scalable solution adaptable to\nthe rapidly evolving landscape of software development."}
{"id": "2506.03382", "pdf": "https://arxiv.org/pdf/2506.03382", "abs": "https://arxiv.org/abs/2506.03382", "authors": ["Matteo Palazzo", "Luca Roversi"], "title": "Towards a Characterization of Two-way Bijections in a Reversible Computational Model", "categories": ["cs.LO", "cs.CC", "cs.PL", "F.3.2"], "comment": "8 pages, 3 figures, 5 listings. Author's copy of the version which\n  will appear in the Proceedings of the 17th International Conference, RC 2025,\n  Odense, Denmark, July 3-4, 2025", "summary": "We introduce an imperative, stack-based, and reversible computational model\nthat characterizes Two-way Bijections both implicitly, concerning their\ncomputational complexity, and with zero-garbage."}
{"id": "2506.03507", "pdf": "https://arxiv.org/pdf/2506.03507", "abs": "https://arxiv.org/abs/2506.03507", "authors": ["Eric O'Donoghue", "Yvette Hastings", "Ernesto Ortiz", "A. Redempta Manzi Muneza"], "title": "Software Bill of Materials in Software Supply Chain Security A Systematic Literature Review", "categories": ["cs.SE", "cs.CR"], "comment": "16 pages, 4 figures, 5 tables", "summary": "Software Bill of Materials (SBOMs) are increasingly regarded as essential\ntools for securing software supply chains (SSCs), yet their real-world use and\nadoption barriers remain poorly understood. This systematic literature review\nsynthesizes evidence from 40 peer-reviewed studies to evaluate how SBOMs are\ncurrently used to bolster SSC security. We identify five primary application\nareas: vulnerability management, transparency, component assessment, risk\nassessment, and SSC integrity. Despite clear promise, adoption is hindered by\nsignificant barriers: generation tooling, data privacy, format/standardization,\nsharing/distribution, cost/overhead, vulnerability exploitability, maintenance,\nanalysis tooling, false positives, hidden packages, and tampering. To structure\nour analysis, we map these barriers to the ISO/IEC 25019:2023 Quality-in-Use\nmodel, revealing critical deficiencies in SBOM trustworthiness, usability, and\nsuitability for security tasks. We also highlight key gaps in the literature.\nThese include the absence of applying machine learning techniques to assess\nSBOMs and limited evaluation of SBOMs and SSCs using software quality assurance\ntechniques. Our findings provide actionable insights for researchers, tool\ndevelopers, and practitioners seeking to advance SBOM-driven SSC security and\nlay a foundation for future work at the intersection of SSC assurance,\nautomation, and empirical software engineering."}
{"id": "2506.04019", "pdf": "https://arxiv.org/pdf/2506.04019", "abs": "https://arxiv.org/abs/2506.04019", "authors": ["Neeva Oza", "Ishaan Govil", "Parul Gupta", "Dinesh Khandelwal", "Dinesh Garg", "Parag Singla"], "title": "CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL", "68-02 (Primary) 68T50, 68T07, 68N19, 68N30 (Secondary)", "I.2.7; I.2.6; I.2.5; D.3.0; D.3.3; D.3.1; F.3.2; F.3.1; F.3.3;\n  D.2.3; D.2.5"], "comment": null, "summary": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code."}
{"id": "2506.03535", "pdf": "https://arxiv.org/pdf/2506.03535", "abs": "https://arxiv.org/abs/2506.03535", "authors": ["Qiming Zhu", "Jialun Cao", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Shing-Chi Cheung"], "title": "Across Programming Language Silos: A Study on Cross-Lingual Retrieval-augmented Code Generation", "categories": ["cs.SE"], "comment": null, "summary": "Current research on large language models (LLMs) with retrieval-augmented\ncode generation (RACG) mainly focuses on single-language settings, leaving\ncross-lingual effectiveness and security unexplored. Multi-lingual RACG systems\nare valuable for migrating code-bases across programming languages (PLs), yet\nface risks from error (e.g. adversarial data corruption) propagation in\ncross-lingual transfer. We construct a dataset spanning 13 PLs with nearly 14k\ninstances to explore utility and robustness of multi-lingual RACG systems. Our\ninvestigation reveals four key insights: (1) Effectiveness: multi-lingual RACG\nsignificantly enhances multi-lingual code LLMs generation; (2) Inequality: Java\ndemonstrate superior cross-lingual utility over Python in RACG; (3) Robustness:\nAdversarial attacks degrade performance significantly in mono-lingual RACG but\nshow mitigated impacts in cross-lingual scenarios; Counterintuitively,\nperturbed code may improve RACG in cross-lingual scenarios; (4) Specialization:\nDomain-specific code retrievers outperform significantly general text\nretrievers. These findings establish foundation for developing effective and\nsecure multi-lingual code assistants."}
{"id": "2506.03585", "pdf": "https://arxiv.org/pdf/2506.03585", "abs": "https://arxiv.org/abs/2506.03585", "authors": ["Inseok Yeo", "Duksan Ryu", "Jongmoon Baik"], "title": "Improving LLM-Based Fault Localization with External Memory and Project Context", "categories": ["cs.SE"], "comment": "12 Pages, 7 figures", "summary": "Fault localization, the process of identifying the software components\nresponsible for failures, is essential but often time-consuming. Recent\nadvances in Large Language Models (LLMs) have enabled fault localization\nwithout extensive defect datasets or model fine-tuning. However, existing\nLLM-based methods rely only on general LLM capabilities and lack integration of\nproject-specific knowledge, resulting in limited effectiveness, especially for\ncomplex software.\n  We introduce MemFL, a novel approach that enhances LLM-based fault\nlocalization by integrating project-specific knowledge via external memory.\nThis memory includes static summaries of the project and dynamic, iterative\ndebugging insights gathered from previous attempts. By leveraging external\nmemory, MemFL simplifies debugging into three streamlined steps, significantly\nimproving efficiency and accuracy. Iterative refinement through dynamic memory\nfurther enhances reasoning quality over time.\n  Evaluated on the Defects4J benchmark, MemFL using GPT-4o-mini localized 12.7%\nmore bugs than current LLM-based methods, achieving this improvement with just\n21% of the execution time (17.4 seconds per bug) and 33% of the API cost\n(0.0033 dollars per bug). On complex projects, MemFL's advantage increased to\n27.6%. Additionally, MemFL with GPT-4.1-mini outperformed existing methods by\n24.4%, requiring only 24.7 seconds and 0.0094 dollars per bug. MemFL thus\ndemonstrates significant improvements by effectively incorporating\nproject-specific knowledge into LLM-based fault localization, delivering high\naccuracy with reduced time and cost."}
{"id": "2506.03691", "pdf": "https://arxiv.org/pdf/2506.03691", "abs": "https://arxiv.org/abs/2506.03691", "authors": ["Weiyuan Xu", "Juntao Luo", "Tao Huang", "Kaixin Sui", "Jie Geng", "Qijun Ma", "Isami Akasaka", "Xiaoxue Shi", "Jing Tang", "Peng Cai"], "title": "A Two-Staged LLM-Based Framework for CI/CD Failure Detection and Remediation with Industrial Validation", "categories": ["cs.SE"], "comment": "12 pages, 5 figures", "summary": "Continuous Integration and Continuous Deployment (CI/CD) pipelines are\npivotal to modern software engineering, yet diagnosing and resolving their\nfailures remains a complex and labor-intensive challenge. In this paper, we\npresent LogSage, the first end-to-end LLM-powered framework that performs root\ncause analysis and solution generation from failed CI/CD pipeline logs. During\nthe root cause analysis stage, LogSage employs a specialized log preprocessing\npipeline tailored for LLMs, which extracts critical error logs and eliminates\nnoise to enhance the precision of LLM-driven root cause analysis. In the\nsolution generation stage, LogSage leverages RAG to integrate historical\nresolution strategies and utilizes tool-calling to deliver actionable,\nautomated fixes. We evaluated the root cause analysis stage using a newly\ncurated open-source dataset, achieving 98\\% in precision and 12\\% improvement\nover naively designed LLM-based log analysis baselines, while attaining\nnear-perfect recall. The end-to-end system was rigorously validated in a\nlarge-scale industrial CI/CD environment of production quality, processing more\nthan 3,000 executions daily and accumulating more than 1.07 million executions\nin its first year of deployment, with end-to-end precision exceeding 88\\%.\nThese two forms of evaluation confirm that LogSage providing a scalable and\npractical solution to manage CI/CD pipeline failures in real-world DevOps\nworkflows."}
{"id": "2506.03801", "pdf": "https://arxiv.org/pdf/2506.03801", "abs": "https://arxiv.org/abs/2506.03801", "authors": ["Peter Pfeiffer", "Alexander Rombach", "Maxim Majlatow", "Nijat Mehdiyev"], "title": "From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation", "categories": ["cs.SE", "cs.LG", "cs.MA"], "comment": "Accepted to the Next Gen Data and Process Management: Large Language\n  Models and Beyond workshop at SIGMOD 2025", "summary": "Traditional Business Process Management (BPM) struggles with rigidity,\nopacity, and scalability in dynamic environments while emerging Large Language\nModels (LLMs) present transformative opportunities alongside risks. This paper\nexplores four real-world use cases that demonstrate how LLMs, augmented with\ntrustworthy process intelligence, redefine process modeling, prediction, and\nautomation. Grounded in early-stage research projects with industrial partners,\nthe work spans manufacturing, modeling, life-science, and design processes,\naddressing domain-specific challenges through human-AI collaboration. In\nmanufacturing, an LLM-driven framework integrates uncertainty-aware explainable\nMachine Learning (ML) with interactive dialogues, transforming opaque\npredictions into auditable workflows. For process modeling, conversational\ninterfaces democratize BPMN design. Pharmacovigilance agents automate drug\nsafety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable\ntextile design employs multi-agent systems to navigate regulatory and\nenvironmental trade-offs. We intend to examine tensions between transparency\nand efficiency, generalization and specialization, and human agency versus\nautomation. By mapping these trade-offs, we advocate for context-sensitive\nintegration prioritizing domain needs, stakeholder values, and iterative\nhuman-in-the-loop workflows over universal solutions. This work provides\nactionable insights for researchers and practitioners aiming to operationalize\nLLMs in critical BPM environments."}
{"id": "2506.03840", "pdf": "https://arxiv.org/pdf/2506.03840", "abs": "https://arxiv.org/abs/2506.03840", "authors": ["Pragya Verma", "Marcos Vinicius Cruz", "Grischa Liebel"], "title": "Differences between Neurodivergent and Neurotypical Software Engineers: Analyzing the 2022 Stack Overflow Survey", "categories": ["cs.SE"], "comment": null, "summary": "Neurodiversity describes variation in brain function among people, including\ncommon conditions such as Autism spectrum disorder (ASD), Attention deficit\nhyperactivity disorder (ADHD), and dyslexia. While Software Engineering (SE)\nliterature has started to explore the experiences of neurodivergent software\nengineers, there is a lack of research that compares their challenges to those\nof neurotypical software engineers. To address this gap, we analyze existing\ndata from the 2022 Stack Overflow Developer survey that collected data on\nneurodiversity. We quantitatively compare the answers of professional engineers\nwith ASD (n=374), ADHD (n=1305), and dyslexia (n=363) with neurotypical\nengineers. Our findings indicate that neurodivergent engineers face more\ndifficulties than neurotypical engineers. Specifically, engineers with ADHD\nreport that they face more interruptions caused by waiting for answers, and\nthat they less frequently interact with individuals outside their team. This\nstudy provides a baseline for future research comparing neurodivergent\nengineers with neurotypical ones. Several factors in the Stack Overflow survey\nand in our analysis are likely to lead to conservative estimates of the actual\neffects between neurodivergent and neurotypical engineers, e.g., the effects of\nthe COVID-19 pandemic and our focus on employed professionals."}
{"id": "2506.03877", "pdf": "https://arxiv.org/pdf/2506.03877", "abs": "https://arxiv.org/abs/2506.03877", "authors": ["Christian Gang Liu", "Peter Bodorik", "Dawn Jutla"], "title": "Automated Mechanism to Support Trade Transactions in Smart Contracts with Upgrade and Repair", "categories": ["cs.SE"], "comment": null, "summary": "In our previous research, we addressed the problem of automated\ntransformation of models, represented using the business process model and\nnotation (BPMN) standard, into the methods of a smart contract. The\ntransformation supports BPMN models that contain complex multi-step activities\nthat are supported using our concept of multi-step nested trade transactions,\nwherein the transactional properties are enforced by a mechanism generated\nautomatically by the transformation process from a BPMN model to a smart\ncontract. In this paper, we present a methodology for repairing a smart\ncontract that cannot be completed due to events that were not anticipated by\nthe developer and thus prevent the completion of the smart contract. The repair\nprocess starts with the original BPMN model fragment causing the issue,\nproviding the modeler with the innermost transaction fragment containing the\nfailed activity. The modeler amends the BPMN pattern on the basis of successful\ncompletion of previous activities. If repairs exceed the inner transaction's\nscope, they are addressed using the parent transaction's BPMN model. The\namended BPMN model is then transformed into a new smart contract, ensuring\nconsistent data and logic transitions. We previously developed a tool, called\nTABS+, as a proof of concept (PoC) to transform BPMN models into smart\ncontracts for nested transactions. This paper describes the tool TABS+R,\ndeveloped by extending the TABS+ tool, to allow the repair of smart contracts."}
{"id": "2506.03903", "pdf": "https://arxiv.org/pdf/2506.03903", "abs": "https://arxiv.org/abs/2506.03903", "authors": ["Hugo Andrade", "João Bispo", "Filipe F. Correia"], "title": "Multi-Language Detection of Design Pattern Instances", "categories": ["cs.SE"], "comment": "Preprint accepted for publication in Journal of Software: Evolution\n  and Process, 2024", "summary": "Code comprehension is often supported by source code analysis tools which\nprovide more abstract views over software systems, such as those detecting\ndesign patterns. These tools encompass analysis of source code and ensuing\nextraction of relevant information. However, the analysis of the source code is\noften specific to the target programming language.\n  We propose DP-LARA, a multi-language pattern detection tool that uses the\nmulti-language capability of the LARA framework to support finding pattern\ninstances in a code base. LARA provides a virtual AST, which is common to\nmultiple OOP programming languages, and DP-LARA then performs code analysis of\ndetecting pattern instances on this abstract representation.\n  We evaluate the detection performance and consistency of DP-LARA with a few\nsoftware projects. Results show that a multi-language approach does not\ncompromise detection performance, and DP-LARA is consistent across the\nlanguages we tested it for (i.e., Java and C/C++). Moreover, by providing a\nvirtual AST as the abstract representation, we believe to have decreased the\neffort of extending the tool to new programming languages and maintaining\nexisting ones."}
{"id": "2506.03909", "pdf": "https://arxiv.org/pdf/2506.03909", "abs": "https://arxiv.org/abs/2506.03909", "authors": ["Lantian Li", "Zhihao Liu", "Zhongxing Yu"], "title": "Solsmith: Solidity Random Program Generator for Compiler Testing", "categories": ["cs.SE"], "comment": "11 pages, 12 figures", "summary": "Smart contracts are computer programs that run on blockchain platforms, with\nSolidity being the most widely used language for their development. As\nblockchain technology advances, smart contracts have become increasingly\nimportant across various fields. In order for smart contracts to operate\ncorrectly, the correctness of the compiler is particularly crucial. Although\nsome research efforts have been devoted to testing Solidity compilers, they\nprimarily focus on testing methods and do not address the core issue of\ngenerating test programs. To fill this gap, this paper designs and implements\nSolsmith, a test program generator specifically aimed at uncovering defects in\nSolidity compilers. It tests the compiler correctness by generating valid and\ndiverse Solidity programs. We have designed a series of unique program\ngeneration strategies tailored to Solidity, including enabling optimizations\nmore frequently, avoiding undefined behaviour, and mitigating behavioural\ndifferences caused by intermediate representations. To validate the\neffectiveness of Solsmith, we assess the effectiveness of the test programs\ngenerated by Solsmith using the approach of differential testing. The\npreliminary results show that Solsmith can generate the expected test programs\nand uncover four confirmed defects in Solidity compilers, demonstrating the\neffectiveness and potential of Solsmith."}
{"id": "2506.03921", "pdf": "https://arxiv.org/pdf/2506.03921", "abs": "https://arxiv.org/abs/2506.03921", "authors": ["Xunzhu Tang", "Jacques Klein", "Tegawendé F. Bissyandé"], "title": "Boosting Open-Source LLMs for Program Repair via Reasoning Transfer and LLM-Guided Reinforcement Learning", "categories": ["cs.SE"], "comment": null, "summary": "Several closed-source LLMs have consistently outperformed open-source\nalternatives in program repair tasks, primarily due to their superior reasoning\ncapabilities and extensive pre-training. This paper introduces Repairity, a\nnovel three-stage methodology that significantly narrows this performance gap\nthrough reasoning extraction and reinforcement learning. Our approach: (1)\nsystematically filters high-quality reasoning traces from closed-source models\nusing correctness verification, (2) transfers this reasoning knowledge to\nopen-source models via supervised fine-tuning, and (3) develops reinforcement\nlearning with LLM-based feedback to further optimize performance. Empirical\nevaluation across multiple program repair benchmarks demonstrates that\nRepairity improves the performance of Qwen2.5-Coder-32B-Instruct, a base open\nsource LLM, by 8.68\\% on average, reducing the capability gap with\nClaude-Sonnet3.7, a state-of-the-art closed-source model, from 10.05% to 1.35%.\nAblation studies confirm that both reasoning extraction and LLM-guided\nreinforcement learning contribute significantly to these improvements. Our\nmethodology generalizes effectively to additional code-related tasks, enabling\norganizations to leverage high-quality program repair capabilities while\nmaintaining the customizability, transparency, and deployment flexibility\ninherent to open-source models."}
{"id": "2506.03930", "pdf": "https://arxiv.org/pdf/2506.03930", "abs": "https://arxiv.org/abs/2506.03930", "authors": ["Yuansheng Ni", "Ping Nie", "Kai Zou", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation."}
{"id": "2506.03946", "pdf": "https://arxiv.org/pdf/2506.03946", "abs": "https://arxiv.org/abs/2506.03946", "authors": ["Dongming Jin", "Zhi Jin", "Nianyu Li", "Kai Yang", "Linyu Li", "Suijing Guan"], "title": "Automatic Multi-level Feature Tree Construction for Domain-Specific Reusable Artifacts Management", "categories": ["cs.SE"], "comment": "9pages, 2figures", "summary": "With the rapid growth of open-source ecosystems (e.g., Linux) and\ndomain-specific software projects (e.g., aerospace), efficient management of\nreusable artifacts is becoming increasingly crucial for software reuse. The\nmulti-level feature tree enables semantic management based on functionality and\nsupports requirements-driven artifact selection. However, constructing such a\ntree heavily relies on domain expertise, which is time-consuming and\nlabor-intensive. To address this issue, this paper proposes an automatic\nmulti-level feature tree construction framework named FTBUILDER, which consists\nof three stages. It automatically crawls domain-specific software repositories\nand merges their metadata to construct a structured artifact library. It\nemploys clustering algorithms to identify a set of artifacts with common\nfeatures. It constructs a prompt and uses LLMs to summarize their common\nfeatures. FTBUILDER recursively applies the identification and summarization\nstages to construct a multi-level feature tree from the bottom up. To validate\nFTBUILDER, we conduct experiments from multiple aspects (e.g., tree quality and\ntime cost) using the Linux distribution ecosystem. Specifically, we first\nsimultaneously develop and evaluate 24 alternative solutions in the FTBUILDER.\nWe then construct a three-level feature tree using the best solution among\nthem. Compared to the official feature tree, our tree exhibits higher quality,\nwith a 9% improvement in the silhouette coefficient and an 11% increase in\nGValue. Furthermore, it can save developers more time in selecting artifacts by\n26% and improve the accuracy of artifact recommendations with GPT-4 by 235%.\nFTBUILDER can be extended to other open-source software communities and\ndomain-specific industrial enterprises."}
{"id": "2506.04019", "pdf": "https://arxiv.org/pdf/2506.04019", "abs": "https://arxiv.org/abs/2506.04019", "authors": ["Neeva Oza", "Ishaan Govil", "Parul Gupta", "Dinesh Khandelwal", "Dinesh Garg", "Parag Singla"], "title": "CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL", "68-02 (Primary) 68T50, 68T07, 68N19, 68N30 (Secondary)", "I.2.7; I.2.6; I.2.5; D.3.0; D.3.3; D.3.1; F.3.2; F.3.1; F.3.3;\n  D.2.3; D.2.5"], "comment": null, "summary": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code."}
{"id": "2506.04038", "pdf": "https://arxiv.org/pdf/2506.04038", "abs": "https://arxiv.org/abs/2506.04038", "authors": ["Sven Kirchner", "Alois C. Knoll"], "title": "Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems", "categories": ["cs.SE", "cs.AI"], "comment": "8 pages; Accepted for publication at the 36th IEEE Intelligent\n  Vehicles Symposium (IV), Cluj-Napoca, Romania, June 22-25, 2025", "summary": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements."}
{"id": "2506.04090", "pdf": "https://arxiv.org/pdf/2506.04090", "abs": "https://arxiv.org/abs/2506.04090", "authors": ["Federico Martusciello", "Henry Muccini", "Antonio Bucchiarone"], "title": "A Reference Architecture for Gamified Cultural Heritage Applications Leveraging Generative AI and Augmented Reality", "categories": ["cs.SE"], "comment": null, "summary": "The rapid advancement of Information and Communication Technologies is\ntransforming Cultural Heritage access, experience, and preservation. However,\nmany digital heritage applications lack interactivity, personalization, and\nadaptability, limiting user engagement and educational impact. This short paper\npresents a reference architecture for gamified cultural heritage applications\nleveraging generative AI and augmented reality. Gamification enhances\nmotivation, artificial intelligence enables adaptive storytelling and\npersonalized content, and augmented reality fosters immersive, location-aware\nexperiences. Integrating AI with gamification supports dynamic mechanics,\npersonalized feedback, and user behavior prediction, improving engagement. The\nmodular design supports scalability, interoperability, and adaptability across\nheritage contexts. This research provides a framework for designing interactive\nand intelligent cultural heritage applications, promoting accessibility and\ndeeper appreciation among users and stakeholders."}
{"id": "2506.04161", "pdf": "https://arxiv.org/pdf/2506.04161", "abs": "https://arxiv.org/abs/2506.04161", "authors": ["Parsa Alian", "Martin Tang", "Ali Mesbah"], "title": "VISCA: Inferring Component Abstractions for Automated End-to-End Testing", "categories": ["cs.SE"], "comment": null, "summary": "Providing optimal contextual input presents a significant challenge for\nautomated end-to-end (E2E) test generation using large language models (LLMs),\na limitation that current approaches inadequately address. This paper\nintroduces Visual-Semantic Component Abstractor (VISCA), a novel method that\ntransforms webpages into a hierarchical, semantically rich component\nabstraction. VISCA starts by partitioning webpages into candidate segments\nutilizing a novel heuristic-based segmentation method. These candidate segments\nsubsequently undergo classification and contextual information extraction via\nmultimodal LLM-driven analysis, facilitating their abstraction into a\npredefined vocabulary of user interface (UI) components. This component-centric\nabstraction offers a more effective contextual basis than prior approaches,\nenabling more accurate feature inference and robust E2E test case generation.\nOur evaluations demonstrate that the test cases generated by VISCA achieve an\naverage feature coverage of 92%, exceeding the performance of the\nstate-of-the-art LLM-based E2E test generation method by 16%."}
{"id": "2506.03651", "pdf": "https://arxiv.org/pdf/2506.03651", "abs": "https://arxiv.org/abs/2506.03651", "authors": ["Zeyu Gao", "Junlin Zhou", "Bolun Zhang", "Yi He", "Chao Zhang", "Yuxin Cui", "Hao Wang"], "title": "Mono: Is Your \"Clean\" Vulnerability Dataset Really Solvable? Exposing and Trapping Undecidable Patches and Beyond", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "The quantity and quality of vulnerability datasets are essential for\ndeveloping deep learning solutions to vulnerability-related tasks. Due to the\nlimited availability of vulnerabilities, a common approach to building such\ndatasets is analyzing security patches in source code. However, existing\nsecurity patches often suffer from inaccurate labels, insufficient contextual\ninformation, and undecidable patches that fail to clearly represent the root\ncauses of vulnerabilities or their fixes. These issues introduce noise into the\ndataset, which can mislead detection models and undermine their effectiveness.\nTo address these issues, we present mono, a novel LLM-powered framework that\nsimulates human experts' reasoning process to construct reliable vulnerability\ndatasets. mono introduces three key components to improve security patch\ndatasets: (i) semantic-aware patch classification for precise vulnerability\nlabeling, (ii) iterative contextual analysis for comprehensive code\nunderstanding, and (iii) systematic root cause analysis to identify and filter\nundecidable patches. Our comprehensive evaluation on the MegaVul benchmark\ndemonstrates that mono can correct 31.0% of labeling errors, recover 89% of\ninter-procedural vulnerabilities, and reveals that 16.7% of CVEs contain\nundecidable patches. Furthermore, mono's enriched context representation\nimproves existing models' vulnerability detection accuracy by 15%. We open\nsource the framework mono and the dataset MonoLens in\nhttps://github.com/vul337/mono."}
