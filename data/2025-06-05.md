<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 20]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Empirical Evaluation of Generalizable Automated Program Repair with Large Language Models](https://arxiv.org/abs/2506.03283)
*Viola Campos,Ridwan Shariffdeen,Adrian Ulges,Yannic Noller*

Main category: cs.SE

TL;DR: The paper evaluates 13 LLMs for automated program repair (APR) across multiple languages, highlighting challenges in generalization, the benefits of model combinations, and the impact of imperfect fault localization.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in APR using LLMs, broad generalization remains a challenge. This work aims to empirically assess the repair capabilities of diverse LLMs to guide future research and practical applications.

Method: An intensive empirical evaluation of 13 LLMs (open and closed) was conducted using benchmarks for Java, JavaScript, Python, and PHP. The study explored language-agnostic repairs, patch complexity, and the role of fault localization.

Result: Key findings: (1) LLMs perform variably across languages, complicating cross-platform repairs. (2) Combining models improves bug fixes. (3) Imperfect fault localization significantly reduces accuracy.

Conclusion: The study provides insights for developing reliable APR techniques and emphasizes realistic evaluation environments, benefiting both researchers and practitioners.

Abstract: Automated Program Repair (APR) proposes bug fixes to aid developers in
maintaining software. The state of the art in this domain focuses on using
LLMs, leveraging their strong capabilities to comprehend specifications in
natural language and to generate program code. Recent works have shown that
LLMs can be used to generate repairs. However, despite the APR community's
research achievements and several industry deployments in the last decade, APR
still lacks the capabilities to generalize broadly. In this work, we present an
intensive empirical evaluation of LLMs for generating patches. We evaluate a
diverse set of 13 recent models, including open ones (e.g., Llama 3.3, Qwen 2.5
Coder, and DeepSeek R1 (dist.)) and closed ones (e.g., o3-mini, GPT-4o, Claude
3.7 Sonnet, Gemini 2.0 Flash). In particular, we explore language-agnostic
repairs by utilizing benchmarks for Java (e.g., Defects4J), JavaScript (e.g.,
BugsJS), Python (e.g., BugsInPy), and PHP (e.g., BugsPHP). Besides the
generalization between different languages and levels of patch complexity, we
also investigate the effects of fault localization (FL) as a preprocessing step
and compare the progress for open vs closed models. Our evaluation represents a
snapshot of the current repair capabilities of the latest LLMs. Key results
include: (1) Different LLMs tend to perform best for different languages, which
makes it hard to develop cross-platform repair techniques with single LLMs. (2)
The combinations of models add value with respect to uniquely fixed bugs, so a
committee of expert models should be considered. (3) Under realistic
assumptions of imperfect FL, we observe significant drops in accuracy from the
usual practice of using perfect FL. Our findings and insights will help both
researchers and practitioners develop reliable and generalizable APR techniques
and evaluate them in realistic and fair environments.

</details>


### [2] [Fault Localisation and Repair for DL Systems: An Empirical Study with LLMs](https://arxiv.org/abs/2506.03396)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: The paper evaluates Fault Localisation (FL) and repair techniques for Deep Learning (DL) models, highlighting their limitations and proposing a novel LLM-based approach. GPT-4 shows significant improvements over existing tools.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of current FL and repair techniques for DL models and explore the potential of LLMs in improving accuracy and performance.

Method: A comprehensive evaluation of state-of-the-art FL and repair techniques, alongside a novel LLM-based approach, tested on a designed benchmark.

Result: LLMs, particularly GPT-4, outperform existing tools with 44% and 82% improvements in FL and repair tasks, respectively.

Conclusion: LLMs like GPT-4 show promise for advancing FL and repair in DL models, but more rigorous assessment methods are needed.

Abstract: Numerous Fault Localisation (FL) and repair techniques have been proposed to
address faults in Deep Learning (DL) models. However, their effectiveness in
practical applications remains uncertain due to the reliance on pre-defined
rules. This paper presents a comprehensive evaluation of state-of-the-art FL
and repair techniques, examining their advantages and limitations. Moreover, we
introduce a novel approach that harnesses the power of Large Language Models
(LLMs) in localising and repairing DL faults. Our evaluation, conducted on a
carefully designed benchmark, reveals the strengths and weaknesses of current
FL and repair techniques. We emphasise the importance of enhanced accuracy and
the need for more rigorous assessment methods that employ multiple ground truth
patches. Notably, LLMs exhibit remarkable performance in both FL and repair
tasks. For instance, the GPT-4 model achieves 44% and 82% improvements in FL
and repair tasks respectively, compared to the second-best tool, demonstrating
the potential of LLMs in this domain. Our study sheds light on the current
state of FL and repair techniques and suggests that LLMs could be a promising
avenue for future advancements.

</details>


### [3] [RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines](https://arxiv.org/abs/2506.03401)
*Xiwei Xu,Hans Weytjens,Dawen Zhang,Qinghua Lu,Ingo Weber,Liming Zhu*

Main category: cs.SE

TL;DR: The paper discusses RAGOps, an extension of LLMOps, focusing on data management for RAG systems. It outlines architecture, lifecycle, design considerations, challenges, and use cases.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM systems by addressing continuous changes in external data sources through RAGOps, improving retrieval relevance and generation quality.

Method: Characterizes RAG architecture, outlines lifecycle, defines design considerations, highlights challenges, and presents use cases.

Result: A framework for RAGOps, integrating LLM and data management lifecycles, with practical insights from use cases.

Conclusion: RAGOps is crucial for managing RAG systems, with ongoing research challenges and practical implications.

Abstract: Recent studies show that 60% of LLM-based compound systems in enterprise
environments leverage some form of retrieval-augmented generation (RAG), which
enhances the relevance and accuracy of LLM (or other genAI) outputs by
retrieving relevant information from external data sources. LLMOps involves the
practices and techniques for managing the lifecycle and operations of LLM
compound systems in production environments. It supports enhancing LLM systems
through continuous operations and feedback evaluation. RAGOps extends LLMOps by
incorporating a strong focus on data management to address the continuous
changes in external data sources. This necessitates automated methods for
evaluating and testing data operations, enhancing retrieval relevance and
generation quality. In this paper, we (1) characterize the generic architecture
of RAG applications based on the 4+1 model view for describing software
architectures, (2) outline the lifecycle of RAG systems, which integrates the
management lifecycles of both the LLM and the data, (3) define the key design
considerations of RAGOps across different stages of the RAG lifecycle and
quality trade-off analyses, (4) highlight the overarching research challenges
around RAGOps, and (5) present two use cases of RAG applications and the
corresponding RAGOps considerations.

</details>


### [4] [Beyond C/C++: Probabilistic and LLM Methods for Next-Generation Software Reverse Engineering](https://arxiv.org/abs/2506.03504)
*Zhuo Zhuo,Xiangyu Zhang*

Main category: cs.SE

TL;DR: A novel hybrid approach combining probabilistic binary analysis and fine-tuned LLMs addresses challenges in reverse engineering modern software binaries from languages like Rust, Go, and Mojo, improving accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional reverse engineering techniques fail for newer languages due to outdated heuristics and lack of semantic utilization, while current data-driven methods suffer from inaccuracies (hallucinations).

Method: Integrates probabilistic binary analysis with fine-tuned LLMs to model uncertainties and enable context-aware inferences.

Result: Enhances robustness, accuracy, and scalability of reverse engineering for diverse programming languages.

Conclusion: The hybrid approach offers a promising solution for modern software binaries, adapting to evolving development trends.

Abstract: This proposal discusses the growing challenges in reverse engineering modern
software binaries, particularly those compiled from newer system programming
languages such as Rust, Go, and Mojo. Traditional reverse engineering
techniques, developed with a focus on C and C++, fall short when applied to
these newer languages due to their reliance on outdated heuristics and failure
to fully utilize the rich semantic information embedded in binary programs.
These challenges are exacerbated by the limitations of current data-driven
methods, which are susceptible to generating inaccurate results, commonly
referred to as hallucinations. To overcome these limitations, we propose a
novel approach that integrates probabilistic binary analysis with fine-tuned
large language models (LLMs). Our method systematically models the
uncertainties inherent in reverse engineering, enabling more accurate reasoning
about incomplete or ambiguous information. By incorporating LLMs, we extend the
analysis beyond traditional heuristics, allowing for more creative and
context-aware inferences, particularly for binaries from diverse programming
languages. This hybrid approach not only enhances the robustness and accuracy
of reverse engineering efforts but also offers a scalable solution adaptable to
the rapidly evolving landscape of software development.

</details>


### [5] [Software Bill of Materials in Software Supply Chain Security A Systematic Literature Review](https://arxiv.org/abs/2506.03507)
*Eric O'Donoghue,Yvette Hastings,Ernesto Ortiz,A. Redempta Manzi Muneza*

Main category: cs.SE

TL;DR: This paper reviews 40 studies on SBOMs, identifying five key uses and 11 adoption barriers, mapped to ISO/IEC 25019:2023. It highlights gaps like lack of ML techniques and SQA evaluations, offering insights for future research.


<details>
  <summary>Details</summary>
Motivation: To understand real-world SBOM adoption and barriers in securing software supply chains, as current knowledge is limited.

Method: A systematic literature review of 40 peer-reviewed studies, analyzing SBOM applications and barriers, mapped to ISO/IEC 25019:2023.

Result: Identified five SBOM uses (vulnerability management, transparency, etc.) and 11 barriers (tooling, privacy, etc.), revealing gaps in trustworthiness and usability.

Conclusion: SBOMs show promise but face adoption challenges. Future work should explore ML and SQA techniques to enhance SBOM-driven security.

Abstract: Software Bill of Materials (SBOMs) are increasingly regarded as essential
tools for securing software supply chains (SSCs), yet their real-world use and
adoption barriers remain poorly understood. This systematic literature review
synthesizes evidence from 40 peer-reviewed studies to evaluate how SBOMs are
currently used to bolster SSC security. We identify five primary application
areas: vulnerability management, transparency, component assessment, risk
assessment, and SSC integrity. Despite clear promise, adoption is hindered by
significant barriers: generation tooling, data privacy, format/standardization,
sharing/distribution, cost/overhead, vulnerability exploitability, maintenance,
analysis tooling, false positives, hidden packages, and tampering. To structure
our analysis, we map these barriers to the ISO/IEC 25019:2023 Quality-in-Use
model, revealing critical deficiencies in SBOM trustworthiness, usability, and
suitability for security tasks. We also highlight key gaps in the literature.
These include the absence of applying machine learning techniques to assess
SBOMs and limited evaluation of SBOMs and SSCs using software quality assurance
techniques. Our findings provide actionable insights for researchers, tool
developers, and practitioners seeking to advance SBOM-driven SSC security and
lay a foundation for future work at the intersection of SSC assurance,
automation, and empirical software engineering.

</details>


### [6] [CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking](https://arxiv.org/abs/2506.04019)
*Neeva Oza,Ishaan Govil,Parul Gupta,Dinesh Khandelwal,Dinesh Garg,Parag Singla*

Main category: cs.SE

TL;DR: The paper explores LLMs for code-equivalence checking, introduces CETBench for benchmarking, and finds simple transformations significantly reduce LLM performance. A fine-tuning approach is proposed to improve results.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' capabilities in code-equivalence checking, which is crucial for tasks like code re-writing and translation.

Method: Introduces CETBench, a dataset of program pairs with random transformations, and tests SOTA LLMs. Proposes fine-tuning to enhance performance.

Result: Simple code transformations cause significant performance drops in LLMs. Fine-tuning improves accuracy on transformed pairs.

Conclusion: LLMs lack deep semantic understanding of code, and fine-tuning can mitigate performance issues for code-equivalence checking.

Abstract: LLMs have been extensively used for the task of automated code generation. In
this work, we examine the applicability of LLMs for the related but relatively
unexplored task of code-equivalence checking, i.e., given two programs, whether
they are functionally equivalent or not. This is an important problem since
benchmarking code equivalence can play a critical role in evaluating LLM
capabilities for tasks such as code re-writing and code translation. Towards
this end, we present CETBench - Code Equivalence with Transformations
Benchmark, constructed via a repository of programs, where two programs in the
repository may be solving the same or different tasks. Each instance in our
dataset is obtained by taking a pair of programs in the repository and applying
a random series of pre-defined code transformations, resulting in
(non-)equivalent pairs. Our analysis on this dataset reveals a surprising
finding that very simple code transformations in the underlying pair of
programs can result in a significant drop in performance of SOTA LLMs for the
task of code-equivalence checking. To remedy this, we present a simple
fine-tuning-based approach to boost LLM performance on the transformed pairs of
programs. Our approach for dataset generation is generic, and can be used with
repositories with varying program difficulty levels and allows for applying
varying numbers as well as kinds of transformations. In our experiments, we
perform ablations over the difficulty level of original programs, as well as
the kind of transformations used in generating pairs for equivalence checking.
Our analysis presents deep insights into the working of LLMs for the task of
code-equivalence, and points to the fact that they may still be far from what
could be termed as a semantic understanding of the underlying code.

</details>


### [7] [Across Programming Language Silos: A Study on Cross-Lingual Retrieval-augmented Code Generation](https://arxiv.org/abs/2506.03535)
*Qiming Zhu,Jialun Cao,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: The paper explores multi-lingual retrieval-augmented code generation (RACG) in LLMs, highlighting its effectiveness, robustness, and specialization across 13 programming languages.


<details>
  <summary>Details</summary>
Motivation: Current research lacks focus on cross-lingual effectiveness and security in RACG, despite its value for code migration across languages.

Method: A dataset of 13 PLs with 14k instances was constructed to analyze utility and robustness of multi-lingual RACG systems.

Result: Key insights include enhanced multi-lingual code generation, Java's superiority over Python, mitigated adversarial impacts in cross-lingual scenarios, and domain-specific retrievers outperforming general ones.

Conclusion: The findings provide a foundation for developing secure and effective multi-lingual code assistants.

Abstract: Current research on large language models (LLMs) with retrieval-augmented
code generation (RACG) mainly focuses on single-language settings, leaving
cross-lingual effectiveness and security unexplored. Multi-lingual RACG systems
are valuable for migrating code-bases across programming languages (PLs), yet
face risks from error (e.g. adversarial data corruption) propagation in
cross-lingual transfer. We construct a dataset spanning 13 PLs with nearly 14k
instances to explore utility and robustness of multi-lingual RACG systems. Our
investigation reveals four key insights: (1) Effectiveness: multi-lingual RACG
significantly enhances multi-lingual code LLMs generation; (2) Inequality: Java
demonstrate superior cross-lingual utility over Python in RACG; (3) Robustness:
Adversarial attacks degrade performance significantly in mono-lingual RACG but
show mitigated impacts in cross-lingual scenarios; Counterintuitively,
perturbed code may improve RACG in cross-lingual scenarios; (4) Specialization:
Domain-specific code retrievers outperform significantly general text
retrievers. These findings establish foundation for developing effective and
secure multi-lingual code assistants.

</details>


### [8] [Improving LLM-Based Fault Localization with External Memory and Project Context](https://arxiv.org/abs/2506.03585)
*Inseok Yeo,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: MemFL enhances LLM-based fault localization by integrating project-specific knowledge via external memory, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based fault localization lacks project-specific knowledge, limiting effectiveness, especially for complex software.

Method: MemFL uses external memory (static project summaries and dynamic debugging insights) to streamline debugging into three steps.

Result: MemFL localized 12.7% more bugs than current methods, with reduced execution time (17.4s/bug) and cost ($0.0033/bug). On complex projects, improvement rose to 27.6%.

Conclusion: MemFL significantly improves fault localization by incorporating project-specific knowledge, achieving high accuracy with lower time and cost.

Abstract: Fault localization, the process of identifying the software components
responsible for failures, is essential but often time-consuming. Recent
advances in Large Language Models (LLMs) have enabled fault localization
without extensive defect datasets or model fine-tuning. However, existing
LLM-based methods rely only on general LLM capabilities and lack integration of
project-specific knowledge, resulting in limited effectiveness, especially for
complex software.
  We introduce MemFL, a novel approach that enhances LLM-based fault
localization by integrating project-specific knowledge via external memory.
This memory includes static summaries of the project and dynamic, iterative
debugging insights gathered from previous attempts. By leveraging external
memory, MemFL simplifies debugging into three streamlined steps, significantly
improving efficiency and accuracy. Iterative refinement through dynamic memory
further enhances reasoning quality over time.
  Evaluated on the Defects4J benchmark, MemFL using GPT-4o-mini localized 12.7%
more bugs than current LLM-based methods, achieving this improvement with just
21% of the execution time (17.4 seconds per bug) and 33% of the API cost
(0.0033 dollars per bug). On complex projects, MemFL's advantage increased to
27.6%. Additionally, MemFL with GPT-4.1-mini outperformed existing methods by
24.4%, requiring only 24.7 seconds and 0.0094 dollars per bug. MemFL thus
demonstrates significant improvements by effectively incorporating
project-specific knowledge into LLM-based fault localization, delivering high
accuracy with reduced time and cost.

</details>


### [9] [A Two-Staged LLM-Based Framework for CI/CD Failure Detection and Remediation with Industrial Validation](https://arxiv.org/abs/2506.03691)
*Weiyuan Xu,Juntao Luo,Tao Huang,Kaixin Sui,Jie Geng,Qijun Ma,Isami Akasaka,Xiaoxue Shi,Jing Tang,Peng Cai*

Main category: cs.SE

TL;DR: LogSage is an LLM-powered framework for diagnosing and fixing CI/CD pipeline failures, achieving high precision and recall in root cause analysis and solution generation.


<details>
  <summary>Details</summary>
Motivation: CI/CD pipeline failures are complex and labor-intensive to diagnose and resolve, necessitating an automated solution.

Method: LogSage uses a specialized log preprocessing pipeline for LLMs and RAG for solution generation, integrating historical data and tool-calling.

Result: Achieved 98% precision in root cause analysis and 88% end-to-end precision in real-world deployment, processing over 3,000 executions daily.

Conclusion: LogSage is a scalable and practical solution for managing CI/CD pipeline failures in DevOps workflows.

Abstract: Continuous Integration and Continuous Deployment (CI/CD) pipelines are
pivotal to modern software engineering, yet diagnosing and resolving their
failures remains a complex and labor-intensive challenge. In this paper, we
present LogSage, the first end-to-end LLM-powered framework that performs root
cause analysis and solution generation from failed CI/CD pipeline logs. During
the root cause analysis stage, LogSage employs a specialized log preprocessing
pipeline tailored for LLMs, which extracts critical error logs and eliminates
noise to enhance the precision of LLM-driven root cause analysis. In the
solution generation stage, LogSage leverages RAG to integrate historical
resolution strategies and utilizes tool-calling to deliver actionable,
automated fixes. We evaluated the root cause analysis stage using a newly
curated open-source dataset, achieving 98\% in precision and 12\% improvement
over naively designed LLM-based log analysis baselines, while attaining
near-perfect recall. The end-to-end system was rigorously validated in a
large-scale industrial CI/CD environment of production quality, processing more
than 3,000 executions daily and accumulating more than 1.07 million executions
in its first year of deployment, with end-to-end precision exceeding 88\%.
These two forms of evaluation confirm that LogSage providing a scalable and
practical solution to manage CI/CD pipeline failures in real-world DevOps
workflows.

</details>


### [10] [From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation](https://arxiv.org/abs/2506.03801)
*Peter Pfeiffer,Alexander Rombach,Maxim Majlatow,Nijat Mehdiyev*

Main category: cs.SE

TL;DR: LLMs enhance BPM by addressing rigidity and opacity through human-AI collaboration in manufacturing, modeling, life-science, and design, balancing transparency, efficiency, and human agency.


<details>
  <summary>Details</summary>
Motivation: Traditional BPM lacks adaptability in dynamic environments; LLMs offer transformative potential but require trustworthy integration.

Method: Four real-world use cases demonstrate LLM-augmented BPM, combining uncertainty-aware ML, conversational interfaces, knowledge graphs, and multi-agent systems.

Result: LLMs improve process modeling, prediction, and automation while addressing domain-specific challenges through human-AI collaboration.

Conclusion: Context-sensitive LLM integration, prioritizing domain needs and stakeholder values, is advocated over universal solutions for critical BPM.

Abstract: Traditional Business Process Management (BPM) struggles with rigidity,
opacity, and scalability in dynamic environments while emerging Large Language
Models (LLMs) present transformative opportunities alongside risks. This paper
explores four real-world use cases that demonstrate how LLMs, augmented with
trustworthy process intelligence, redefine process modeling, prediction, and
automation. Grounded in early-stage research projects with industrial partners,
the work spans manufacturing, modeling, life-science, and design processes,
addressing domain-specific challenges through human-AI collaboration. In
manufacturing, an LLM-driven framework integrates uncertainty-aware explainable
Machine Learning (ML) with interactive dialogues, transforming opaque
predictions into auditable workflows. For process modeling, conversational
interfaces democratize BPMN design. Pharmacovigilance agents automate drug
safety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable
textile design employs multi-agent systems to navigate regulatory and
environmental trade-offs. We intend to examine tensions between transparency
and efficiency, generalization and specialization, and human agency versus
automation. By mapping these trade-offs, we advocate for context-sensitive
integration prioritizing domain needs, stakeholder values, and iterative
human-in-the-loop workflows over universal solutions. This work provides
actionable insights for researchers and practitioners aiming to operationalize
LLMs in critical BPM environments.

</details>


### [11] [Differences between Neurodivergent and Neurotypical Software Engineers: Analyzing the 2022 Stack Overflow Survey](https://arxiv.org/abs/2506.03840)
*Pragya Verma,Marcos Vinicius Cruz,Grischa Liebel*

Main category: cs.SE

TL;DR: The paper compares challenges faced by neurodivergent (ASD, ADHD, dyslexia) and neurotypical software engineers using 2022 Stack Overflow data, finding neurodivergent engineers face more difficulties.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research comparing challenges of neurodivergent and neurotypical software engineers.

Method: Quantitative analysis of 2022 Stack Overflow Developer survey data, comparing responses of neurodivergent (ASD, ADHD, dyslexia) and neurotypical engineers.

Result: Neurodivergent engineers face more difficulties; ADHD engineers report more interruptions and less interaction outside their team.

Conclusion: The study establishes a baseline for future research, noting conservative estimates due to survey limitations and pandemic effects.

Abstract: Neurodiversity describes variation in brain function among people, including
common conditions such as Autism spectrum disorder (ASD), Attention deficit
hyperactivity disorder (ADHD), and dyslexia. While Software Engineering (SE)
literature has started to explore the experiences of neurodivergent software
engineers, there is a lack of research that compares their challenges to those
of neurotypical software engineers. To address this gap, we analyze existing
data from the 2022 Stack Overflow Developer survey that collected data on
neurodiversity. We quantitatively compare the answers of professional engineers
with ASD (n=374), ADHD (n=1305), and dyslexia (n=363) with neurotypical
engineers. Our findings indicate that neurodivergent engineers face more
difficulties than neurotypical engineers. Specifically, engineers with ADHD
report that they face more interruptions caused by waiting for answers, and
that they less frequently interact with individuals outside their team. This
study provides a baseline for future research comparing neurodivergent
engineers with neurotypical ones. Several factors in the Stack Overflow survey
and in our analysis are likely to lead to conservative estimates of the actual
effects between neurodivergent and neurotypical engineers, e.g., the effects of
the COVID-19 pandemic and our focus on employed professionals.

</details>


### [12] [Automated Mechanism to Support Trade Transactions in Smart Contracts with Upgrade and Repair](https://arxiv.org/abs/2506.03877)
*Christian Gang Liu,Peter Bodorik,Dawn Jutla*

Main category: cs.SE

TL;DR: This paper introduces TABS+R, an extension of the TABS+ tool, for repairing smart contracts derived from BPMN models when unforeseen events prevent their completion.


<details>
  <summary>Details</summary>
Motivation: The need arises from unanticipated events in smart contracts derived from BPMN models, which can halt execution. A repair methodology is proposed to address such issues.

Method: The repair process involves identifying the problematic BPMN fragment, amending it based on prior successful activities, and transforming the updated model into a new smart contract.

Result: TABS+R is developed as an extension of TABS+ to enable smart contract repair, ensuring data and logic consistency.

Conclusion: The proposed methodology and tool (TABS+R) effectively address smart contract repair, enhancing reliability in nested transaction scenarios.

Abstract: In our previous research, we addressed the problem of automated
transformation of models, represented using the business process model and
notation (BPMN) standard, into the methods of a smart contract. The
transformation supports BPMN models that contain complex multi-step activities
that are supported using our concept of multi-step nested trade transactions,
wherein the transactional properties are enforced by a mechanism generated
automatically by the transformation process from a BPMN model to a smart
contract. In this paper, we present a methodology for repairing a smart
contract that cannot be completed due to events that were not anticipated by
the developer and thus prevent the completion of the smart contract. The repair
process starts with the original BPMN model fragment causing the issue,
providing the modeler with the innermost transaction fragment containing the
failed activity. The modeler amends the BPMN pattern on the basis of successful
completion of previous activities. If repairs exceed the inner transaction's
scope, they are addressed using the parent transaction's BPMN model. The
amended BPMN model is then transformed into a new smart contract, ensuring
consistent data and logic transitions. We previously developed a tool, called
TABS+, as a proof of concept (PoC) to transform BPMN models into smart
contracts for nested transactions. This paper describes the tool TABS+R,
developed by extending the TABS+ tool, to allow the repair of smart contracts.

</details>


### [13] [Multi-Language Detection of Design Pattern Instances](https://arxiv.org/abs/2506.03903)
*Hugo Andrade,João Bispo,Filipe F. Correia*

Main category: cs.SE

TL;DR: DP-LARA is a multi-language pattern detection tool leveraging LARA's virtual AST for consistent design pattern detection across languages like Java and C/C++.


<details>
  <summary>Details</summary>
Motivation: Existing tools for code comprehension are often language-specific, limiting their applicability. DP-LARA addresses this by using a multi-language approach.

Method: DP-LARA utilizes LARA's virtual AST, a common abstract representation for multiple OOP languages, to detect design patterns.

Result: DP-LARA maintains detection performance and consistency across languages (Java and C/C++), with reduced effort for extending to new languages.

Conclusion: DP-LARA demonstrates the feasibility and benefits of a multi-language approach for design pattern detection, simplifying tool extension and maintenance.

Abstract: Code comprehension is often supported by source code analysis tools which
provide more abstract views over software systems, such as those detecting
design patterns. These tools encompass analysis of source code and ensuing
extraction of relevant information. However, the analysis of the source code is
often specific to the target programming language.
  We propose DP-LARA, a multi-language pattern detection tool that uses the
multi-language capability of the LARA framework to support finding pattern
instances in a code base. LARA provides a virtual AST, which is common to
multiple OOP programming languages, and DP-LARA then performs code analysis of
detecting pattern instances on this abstract representation.
  We evaluate the detection performance and consistency of DP-LARA with a few
software projects. Results show that a multi-language approach does not
compromise detection performance, and DP-LARA is consistent across the
languages we tested it for (i.e., Java and C/C++). Moreover, by providing a
virtual AST as the abstract representation, we believe to have decreased the
effort of extending the tool to new programming languages and maintaining
existing ones.

</details>


### [14] [Solsmith: Solidity Random Program Generator for Compiler Testing](https://arxiv.org/abs/2506.03909)
*Lantian Li,Zhihao Liu,Zhongxing Yu*

Main category: cs.SE

TL;DR: Solsmith is a test program generator for Solidity compilers, designed to uncover defects by generating diverse and valid Solidity programs. It successfully identified four confirmed defects.


<details>
  <summary>Details</summary>
Motivation: Existing research on Solidity compiler testing lacks focus on generating test programs, leaving a gap in ensuring compiler correctness.

Method: Solsmith employs unique program generation strategies for Solidity, such as frequent optimizations and avoiding undefined behavior, and uses differential testing for validation.

Result: Solsmith generated effective test programs and uncovered four confirmed defects in Solidity compilers.

Conclusion: Solsmith demonstrates effectiveness in testing Solidity compilers, highlighting its potential for improving compiler reliability.

Abstract: Smart contracts are computer programs that run on blockchain platforms, with
Solidity being the most widely used language for their development. As
blockchain technology advances, smart contracts have become increasingly
important across various fields. In order for smart contracts to operate
correctly, the correctness of the compiler is particularly crucial. Although
some research efforts have been devoted to testing Solidity compilers, they
primarily focus on testing methods and do not address the core issue of
generating test programs. To fill this gap, this paper designs and implements
Solsmith, a test program generator specifically aimed at uncovering defects in
Solidity compilers. It tests the compiler correctness by generating valid and
diverse Solidity programs. We have designed a series of unique program
generation strategies tailored to Solidity, including enabling optimizations
more frequently, avoiding undefined behaviour, and mitigating behavioural
differences caused by intermediate representations. To validate the
effectiveness of Solsmith, we assess the effectiveness of the test programs
generated by Solsmith using the approach of differential testing. The
preliminary results show that Solsmith can generate the expected test programs
and uncover four confirmed defects in Solidity compilers, demonstrating the
effectiveness and potential of Solsmith.

</details>


### [15] [Boosting Open-Source LLMs for Program Repair via Reasoning Transfer and LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2506.03921)
*Xunzhu Tang,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: Repairity, a three-stage method, narrows the performance gap between open-source and closed-source LLMs in program repair tasks by leveraging reasoning extraction and reinforcement learning, improving open-source model performance by 8.68%.


<details>
  <summary>Details</summary>
Motivation: Closed-source LLMs outperform open-source ones in program repair due to better reasoning and pre-training. The paper aims to bridge this gap.

Method: 1. Filter high-quality reasoning traces from closed-source models. 2. Transfer knowledge via supervised fine-tuning. 3. Optimize with reinforcement learning and LLM feedback.

Result: Repairity improves Qwen2.5-Coder-32B-Instruct by 8.68%, reducing the gap with Claude-Sonnet3.7 from 10.05% to 1.35%.

Conclusion: The method generalizes to code tasks, offering high-quality repair while preserving open-source benefits like customizability and transparency.

Abstract: Several closed-source LLMs have consistently outperformed open-source
alternatives in program repair tasks, primarily due to their superior reasoning
capabilities and extensive pre-training. This paper introduces Repairity, a
novel three-stage methodology that significantly narrows this performance gap
through reasoning extraction and reinforcement learning. Our approach: (1)
systematically filters high-quality reasoning traces from closed-source models
using correctness verification, (2) transfers this reasoning knowledge to
open-source models via supervised fine-tuning, and (3) develops reinforcement
learning with LLM-based feedback to further optimize performance. Empirical
evaluation across multiple program repair benchmarks demonstrates that
Repairity improves the performance of Qwen2.5-Coder-32B-Instruct, a base open
source LLM, by 8.68\% on average, reducing the capability gap with
Claude-Sonnet3.7, a state-of-the-art closed-source model, from 10.05% to 1.35%.
Ablation studies confirm that both reasoning extraction and LLM-guided
reinforcement learning contribute significantly to these improvements. Our
methodology generalizes effectively to additional code-related tasks, enabling
organizations to leverage high-quality program repair capabilities while
maintaining the customizability, transparency, and deployment flexibility
inherent to open-source models.

</details>


### [16] [VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation](https://arxiv.org/abs/2506.03930)
*Yuansheng Ni,Ping Nie,Kai Zou,Xiang Yue,Wenhu Chen*

Main category: cs.SE

TL;DR: VisCode-200K is a dataset for improving LLMs' visualization tasks by providing execution-grounded supervision and iterative code correction. VisCoder, fine-tuned on this dataset, outperforms open-source models and nears proprietary ones like GPT-4o-mini.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with visualization tasks due to lack of execution-grounded supervision and iterative correction support in existing datasets.

Method: Created VisCode-200K, a dataset with 200K examples from validated plotting code and multi-turn correction dialogues. Fine-tuned Qwen2.5-Coder-Instruct to produce VisCoder.

Result: VisCoder outperforms open-source baselines and approaches GPT-4o-mini's performance on PandasPlotBench.

Conclusion: Feedback-driven learning and iterative repair improve executable, visually accurate code generation in LLMs.

Abstract: Large language models (LLMs) often struggle with visualization tasks like
plotting diagrams, charts, where success depends on both code correctness and
visual semantics. Existing instruction-tuning datasets lack execution-grounded
supervision and offer limited support for iterative code correction, resulting
in fragile and unreliable plot generation. We present VisCode-200K, a
large-scale instruction tuning dataset for Python-based visualization and
self-correction. It contains over 200K examples from two sources: (1) validated
plotting code from open-source repositories, paired with natural language
instructions and rendered plots; and (2) 45K multi-turn correction dialogues
from Code-Feedback, enabling models to revise faulty code using runtime
feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create
VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly
outperforms strong open-source baselines and approaches the performance of
proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation
protocol to assess iterative repair, demonstrating the benefits of
feedback-driven learning for executable, visually accurate code generation.

</details>


### [17] [Automatic Multi-level Feature Tree Construction for Domain-Specific Reusable Artifacts Management](https://arxiv.org/abs/2506.03946)
*Dongming Jin,Zhi Jin,Nianyu Li,Kai Yang,Linyu Li,Suijing Guan*

Main category: cs.SE

TL;DR: FTBUILDER automates multi-level feature tree construction for software reuse, improving efficiency and accuracy in artifact selection.


<details>
  <summary>Details</summary>
Motivation: Manual construction of feature trees is time-consuming and relies on domain expertise, necessitating an automated solution.

Method: FTBUILDER crawls repositories, clusters artifacts, and uses LLMs to summarize features, building the tree recursively.

Result: The framework improves tree quality (9% silhouette coefficient, 11% GValue), saves 26% selection time, and boosts GPT-4 recommendation accuracy by 235%.

Conclusion: FTBUILDER is effective for open-source and domain-specific projects, offering scalability and efficiency.

Abstract: With the rapid growth of open-source ecosystems (e.g., Linux) and
domain-specific software projects (e.g., aerospace), efficient management of
reusable artifacts is becoming increasingly crucial for software reuse. The
multi-level feature tree enables semantic management based on functionality and
supports requirements-driven artifact selection. However, constructing such a
tree heavily relies on domain expertise, which is time-consuming and
labor-intensive. To address this issue, this paper proposes an automatic
multi-level feature tree construction framework named FTBUILDER, which consists
of three stages. It automatically crawls domain-specific software repositories
and merges their metadata to construct a structured artifact library. It
employs clustering algorithms to identify a set of artifacts with common
features. It constructs a prompt and uses LLMs to summarize their common
features. FTBUILDER recursively applies the identification and summarization
stages to construct a multi-level feature tree from the bottom up. To validate
FTBUILDER, we conduct experiments from multiple aspects (e.g., tree quality and
time cost) using the Linux distribution ecosystem. Specifically, we first
simultaneously develop and evaluate 24 alternative solutions in the FTBUILDER.
We then construct a three-level feature tree using the best solution among
them. Compared to the official feature tree, our tree exhibits higher quality,
with a 9% improvement in the silhouette coefficient and an 11% increase in
GValue. Furthermore, it can save developers more time in selecting artifacts by
26% and improve the accuracy of artifact recommendations with GPT-4 by 235%.
FTBUILDER can be extended to other open-source software communities and
domain-specific industrial enterprises.

</details>


### [18] [Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems](https://arxiv.org/abs/2506.04038)
*Sven Kirchner,Alois C. Knoll*

Main category: cs.SE

TL;DR: A novel framework integrates GenAI into automotive SDLC using LLMs for automated, safety-compliant code generation, validated via an ACC system.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in safety-critical automotive software development due to complexity and regulatory demands.

Method: Proposes a framework with LLMs for automated code generation, incorporating safety practices like static verification and test-driven development, validated through ACC system development.

Result: Demonstrates automatic code generation compliant with safety-critical requirements, optimizing LLM selection for accuracy.

Conclusion: Advances AI use in safety-critical domains, aligning generative models with real-world safety needs.

Abstract: Developing safety-critical automotive software presents significant
challenges due to increasing system complexity and strict regulatory demands.
This paper proposes a novel framework integrating Generative Artificial
Intelligence (GenAI) into the Software Development Lifecycle (SDLC). The
framework uses Large Language Models (LLMs) to automate code generation in
languages such as C++, incorporating safety-focused practices such as static
verification, test-driven development and iterative refinement. A
feedback-driven pipeline ensures the integration of test, simulation and
verification for compliance with safety standards. The framework is validated
through the development of an Adaptive Cruise Control (ACC) system. Comparative
benchmarking of LLMs ensures optimal model selection for accuracy and
reliability. Results demonstrate that the framework enables automatic code
generation while ensuring compliance with safety-critical requirements,
systematically integrating GenAI into automotive software engineering. This
work advances the use of AI in safety-critical domains, bridging the gap
between state-of-the-art generative models and real-world safety requirements.

</details>


### [19] [A Reference Architecture for Gamified Cultural Heritage Applications Leveraging Generative AI and Augmented Reality](https://arxiv.org/abs/2506.04090)
*Federico Martusciello,Henry Muccini,Antonio Bucchiarone*

Main category: cs.SE

TL;DR: A reference architecture for gamified cultural heritage apps using generative AI and AR to enhance interactivity, personalization, and engagement.


<details>
  <summary>Details</summary>
Motivation: Digital heritage apps often lack interactivity and personalization, limiting user engagement and educational impact.

Method: Proposes a modular architecture integrating gamification, generative AI, and augmented reality for adaptive storytelling and immersive experiences.

Result: The framework improves engagement through dynamic mechanics, personalized feedback, and behavior prediction, while ensuring scalability and adaptability.

Conclusion: The research offers a scalable, interoperable framework for designing intelligent cultural heritage apps, enhancing accessibility and user appreciation.

Abstract: The rapid advancement of Information and Communication Technologies is
transforming Cultural Heritage access, experience, and preservation. However,
many digital heritage applications lack interactivity, personalization, and
adaptability, limiting user engagement and educational impact. This short paper
presents a reference architecture for gamified cultural heritage applications
leveraging generative AI and augmented reality. Gamification enhances
motivation, artificial intelligence enables adaptive storytelling and
personalized content, and augmented reality fosters immersive, location-aware
experiences. Integrating AI with gamification supports dynamic mechanics,
personalized feedback, and user behavior prediction, improving engagement. The
modular design supports scalability, interoperability, and adaptability across
heritage contexts. This research provides a framework for designing interactive
and intelligent cultural heritage applications, promoting accessibility and
deeper appreciation among users and stakeholders.

</details>


### [20] [VISCA: Inferring Component Abstractions for Automated End-to-End Testing](https://arxiv.org/abs/2506.04161)
*Parsa Alian,Martin Tang,Ali Mesbah*

Main category: cs.SE

TL;DR: VISCA improves E2E test generation by transforming webpages into hierarchical, semantically rich UI components, outperforming state-of-the-art methods by 16% in feature coverage.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based E2E test generation lacks optimal contextual input, limiting accuracy and robustness.

Method: VISCA partitions webpages into segments, classifies them, and abstracts them into UI components using multimodal LLM analysis.

Result: VISCA achieves 92% average feature coverage, 16% higher than state-of-the-art methods.

Conclusion: VISCA's component-centric abstraction enhances contextual input, improving E2E test generation accuracy and robustness.

Abstract: Providing optimal contextual input presents a significant challenge for
automated end-to-end (E2E) test generation using large language models (LLMs),
a limitation that current approaches inadequately address. This paper
introduces Visual-Semantic Component Abstractor (VISCA), a novel method that
transforms webpages into a hierarchical, semantically rich component
abstraction. VISCA starts by partitioning webpages into candidate segments
utilizing a novel heuristic-based segmentation method. These candidate segments
subsequently undergo classification and contextual information extraction via
multimodal LLM-driven analysis, facilitating their abstraction into a
predefined vocabulary of user interface (UI) components. This component-centric
abstraction offers a more effective contextual basis than prior approaches,
enabling more accurate feature inference and robust E2E test case generation.
Our evaluations demonstrate that the test cases generated by VISCA achieve an
average feature coverage of 92%, exceeding the performance of the
state-of-the-art LLM-based E2E test generation method by 16%.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [21] [Mono: Is Your "Clean" Vulnerability Dataset Really Solvable? Exposing and Trapping Undecidable Patches and Beyond](https://arxiv.org/abs/2506.03651)
*Zeyu Gao,Junlin Zhou,Bolun Zhang,Yi He,Chao Zhang,Yuxin Cui,Hao Wang*

Main category: cs.CR

TL;DR: The paper introduces mono, an LLM-powered framework to improve vulnerability datasets by addressing labeling errors, contextual gaps, and undecidable patches, enhancing detection accuracy by 15%.


<details>
  <summary>Details</summary>
Motivation: Existing vulnerability datasets suffer from inaccurate labels, insufficient context, and undecidable patches, which mislead detection models.

Method: mono uses semantic-aware patch classification, iterative contextual analysis, and systematic root cause analysis to refine datasets.

Result: mono corrects 31.0% of labeling errors, recovers 89% of inter-procedural vulnerabilities, and identifies 16.7% of CVEs with undecidable patches, improving detection accuracy by 15%.

Conclusion: mono effectively constructs reliable vulnerability datasets and enhances model performance, with the framework and dataset MonoLens made publicly available.

Abstract: The quantity and quality of vulnerability datasets are essential for
developing deep learning solutions to vulnerability-related tasks. Due to the
limited availability of vulnerabilities, a common approach to building such
datasets is analyzing security patches in source code. However, existing
security patches often suffer from inaccurate labels, insufficient contextual
information, and undecidable patches that fail to clearly represent the root
causes of vulnerabilities or their fixes. These issues introduce noise into the
dataset, which can mislead detection models and undermine their effectiveness.
To address these issues, we present mono, a novel LLM-powered framework that
simulates human experts' reasoning process to construct reliable vulnerability
datasets. mono introduces three key components to improve security patch
datasets: (i) semantic-aware patch classification for precise vulnerability
labeling, (ii) iterative contextual analysis for comprehensive code
understanding, and (iii) systematic root cause analysis to identify and filter
undecidable patches. Our comprehensive evaluation on the MegaVul benchmark
demonstrates that mono can correct 31.0% of labeling errors, recover 89% of
inter-procedural vulnerabilities, and reveals that 16.7% of CVEs contain
undecidable patches. Furthermore, mono's enriched context representation
improves existing models' vulnerability detection accuracy by 15%. We open
source the framework mono and the dataset MonoLens in
https://github.com/vul337/mono.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [22] [Towards a Characterization of Two-way Bijections in a Reversible Computational Model](https://arxiv.org/abs/2506.03382)
*Matteo Palazzo,Luca Roversi*

Main category: cs.LO

TL;DR: A reversible, stack-based computational model for Two-way Bijections with zero-garbage and implicit computational complexity.


<details>
  <summary>Details</summary>
Motivation: To characterize Two-way Bijections in a reversible and efficient manner, addressing computational complexity and garbage-free execution.

Method: Introduces an imperative, stack-based, and reversible computational model.

Result: The model successfully characterizes Two-way Bijections with zero-garbage and implicit computational complexity.

Conclusion: The proposed model provides an efficient and reversible framework for Two-way Bijections, ensuring computational clarity and zero-garbage execution.

Abstract: We introduce an imperative, stack-based, and reversible computational model
that characterizes Two-way Bijections both implicitly, concerning their
computational complexity, and with zero-garbage.

</details>
